# News

---

### Software release: CEED v2.0

The CEED team released [CEED 2.0](http://ceed.exascaleproject.org/ceed-2.0) consisting of 12 integrated Spack packages for [libCEED](libceed.md), [mfem](mfem.md), [nek5000](nek.md), [nekcem](nek.md), [laghos](miniapps.md#laghos-new), [nekbone](miniapps.md#nekbone), [hpgmg](miniapps.md#hpgmg-fe), [occa](occa.md), [magma](magma.md), [gslib](gslib.md), [petsc](petsc.md) and [pumi](pumi.md) plus a new [CEED meta-package](https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/ceed/package.py).

With [Spack](https://spack.io), a user can install the whole CEED software stack simply with: `spack install ceed`.
Visit the [CEED 2.0 release page](http://ceed.exascaleproject.org/ceed-2.0) for further information about the release and installation help for various systems.

---

### Software release: libCEED v0.4

Version 0.4 of [libCEED](https://github.com/ceed/libceed) was released in March, 2019. The release includes

- new CPU and GPU backends
- CPU backend optimizations
- initial support for operator composition
- performance benchmarking
- a Navier-Stokes demo

---

### Laghos CTS-2 benchmark

The CEED team at LLNL developed a version of the [Laghos](https://github.com/ceed/laghos) miniapp to be
used in the second edition of the [Commodity Technology Systems](https://asc.llnl.gov/computers/commodity)
procurement process. These systems leverage industry advances and open source
software standards to build, field, and integrate Linux clusters of various
sizes into production service.

The CTS-2 version of Laghos features [RAJA](https://github.com/LLNL/RAJA)
backend, improved robustness and figure of merit computation, and is available
at https://github.com/CEED/Laghos/releases/tag/cts2.

---

### CEED researchers at NAHOMCon19

<img src="../img/NAHOMCon19.jpg" align="center" alt="NAHOMCon19 picture">

With the next International Conference on Spectral and High-Order Methods
(ICOSAHOM) meetings scheduled for 2020 in Vienna and 2022 in Seoul, it will be
at least 10 years between US-based settings of the principal high-order methods
conference, ICOSAHOM.

Because of the growing importance of high-order methods, several institutions
have joined together to organize the inaugural [North American High Order
Methods Conference](https://www.nahomcon19.sdsu.edu) (NAHOMCon19) to be held in
San Diego in summer of 2019, that will focus on the many developments in
high-order discretizations and applications that are taking place in North
America.

Several CEED researchers will participate in the conference, including Misun Min
and Tzanio Kolev, who will present two of the plenary talks, and Paul Fischer and
Tim Warburton, who are serving on the scientific committee.

---

### Software release: Laghos 2.0

Version 2.0 of the [Laghos](https://github.com/ceed/laghos) miniapp was released on November 19, 2018. The release includes

- CUDA, RAJA, OCCA and AMR versions of Laghos in the [cuda/](https://github.com/CEED/Laghos/tree/v2.0/cuda), [raja/](https://github.com/CEED/Laghos/tree/v2.0/raja), [occa/](https://github.com/CEED/Laghos/tree/v2.0/occa) and [amr/](https://github.com/CEED/Laghos/tree/v2.0/amr) directories.
- New example: Gresho vortex.
- Added a conservative time integrator (RK2Avg) and computation of total energy.
- Improved the computations of the matrix diagonal by contracting the squares of the B matrices.
- Added diagonal preconditioners for both partial and full assembly.
- Support the Bernstein positive basis When using partial assembly for the velocity.
- Travis CI regression testing on GitHub.

---

### CEED minisymposiums at SIAM CSE19

CEED researchers are organizing two minisymposiums at the [SIAM Conference on
Computational Science and Engineering](https://www.siam.org/Conferences/CM/Main/cse19) (CSE19)
in Spokane, Washington, Feb 25 - Mar 1, 2019:

- *Exascale Applications with High-Order Methods*: [Part 1](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=66158) and [Part 2](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=66159)
- *Exascale Software for High-order Methods*: [Part 1](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=66166) and [Part 2](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=66167)

The minisymposiums will focus on high-order applications, discretization
algorithms, and lightweight portable libraries targeting performance on
exascale hardware.

---

### MFEM part of E4S-0.1

MFEM was included in the first release of the Extreme-Scale Scientific Software
Stack (E4S) software collection developed by the Software Technologies focus
area of the ECP.

According to the [E4S website](http://e4s.io/), the primary purpose of this
initial 0.1 release was to demonstrate the release approach based on Spack
package maturity.

Future release will include additional software products, with the ultimate goal
of including all ECP ST software products.

---

### CEED researchers at ATPESC18

Several CEED researchers presented at the 2018 edition of the [Argonne Training Program
on Extreme-Scale Computing](https://extremecomputingtraining.anl.gov/agenda-2018/),
which is also part of the Exascale Computing Project.

The CEED presentations covered a wide variety of topics, from meshing, to GPU
programming, dense and sparse linear algebra, and high-order discretizations on
unstructured meshes.

![YouTube](uvVy3CqpVbM)
![YouTube](RKQ4xTztn-4)
![YouTube](Zh6pFjkmr0g)
![YouTube](K5Bx2LRPkyE)

Videos of all 2018 talks are available on
[YouTube](https://www.youtube.com/channel/UCfwgjtIQB3puojz_N9ly_Ag/playlists?sort=dd&shelf_id=4&view=50).

---

### Software release: libCEED 0.3

Version 0.3 of [libCEED](https://github.com/ceed/libceed) was released on September 30, 2018. The release includes

- New interface enabling active and passive fields
- Performance improvements and element vectorization
- Streamlined Fortran interface
- Improved testing and coverage

---

### Second CEED annual meeting in CU Boulder

<img src="../img/CEED2AM.jpg" align="center" alt="CEED2AM picture">

CEED held its second annual meeting August 8-10, 2018 at the University of
Colorado Boulder.

The goal of the meeting was to report on the progress in the center, deepen
existing and establish new connections with ECP hardware vendors, ECP software
technologies projects and other collaborators, plan project activities and
brainstorm/work as a group to make technical progress.

In addition to gathering together many of the CEED researchers, the meeting
included representatives of ECP applications, hardware vendors, software
technology and other interested projects.

See the [meeting page](http://ceed.exascaleproject.org/ceed2am/) for additional information.

---

### Software release: libParanumal v0.1.0

The inaugural version 0.1.0 of [libParanumal](https://github.com/paranumal/libParanumal) was released on July 31, 2018. The release includes

- Incompressible flow solver.
- Compressible flow solver.
- Galerkin-Boltzmann kinetic flow solver.
- Discontinuous Galerkin and continuous Galerkin spatial discretizations.
- A range of time steppers custom chosen for each flow solver.
- Support for meshes consisting of triangles, quadrilaterals, tetrahedra, or hexahedra up to at least degree 10.
- OCCA 1.0 based computational kernels.
- MPI based distributed computing

---

### Software release: MAGMA v2.4.0

Version 2.4.0 of [MAGMA](magma.md) was released on June 25, 2018. The release includes:

- Performance improvements across many batch routines, including batched TRSM, batched LU, batched LU-nopiv, and batched Cholesky
- Constrained least squares routines (magma_\[sdcz\]gglse) and dependencies
- Fixed some compilation issues with inf, nan, and nullptr.

For further details and download, see [MAGMA Download](http://icl.cs.utk.edu/magma/software/index.html).

---

### Software release: OCCA v1.0

Version 1.0 of [OCCA](occa.md) was released on June 13, 2018.
The main focus for the release included:

- Updating the API to expose backend-specific features in a generic way
- New OKL Parser better suited for language transforms and error handling

Useful links:

- [v0.2 -> v1.0](https://github.com/libocca/occa/releases/tag/v1.0.0-alpha.1#porting-from-v0.2-to-v1.0) porting guide
- [libocca.org](https://libocca.org) for more info about OCCA

---

### Software release: MFEM v3.4

Version 3.4 of [MFEM](mfem.md) was released on May 29, 2018. Some of the new
additions in this release are:

- Significantly improved non-conforming unstructured AMR scalability.
- Integration with [PUMI](pumi.md), the Parallel Unstructured Mesh Infrastructure from RPI.
- Block nonlinear  operators and variable order NURBS.
- Conduit [Mesh Blueprint](http://llnl-conduit.readthedocs.io/en/latest/blueprint.html)
  support.
- General "high-order"-to-"low-order refined" field transfer.
- New specialized time integrators (symplectic, generalized-alpha).
- Twelve new examples and miniapps.

For more details, see the [interactive documentation](http://mfem.org/examples) and the
full [CHANGELOG](https://raw.githubusercontent.com/mfem/mfem/master/CHANGELOG) at
[http://mfem.org](http://mfem.org/).

---

### 6th Nek5000 User Meeting held at U Florida

The 6th [Nek5000](nek.md) User/Developer Meeting was held in Tampa, FL, April 17-18, 2018.  The event was hosted by the DOE Center for Multiphase Turbulence, which is headed by Prof. S. Balachandar at the University of Florida. Thirty five researchers from industry, national labs, and American, Canadian, and European universities attended the event, which featured twenty three presentations and extensive discussions about current and new trends in Nek5000 development.  Next month will mark the 10th anniversary of Nek5000 going open source.

<img src="../img/nek5k_users_2018.jpg" align="center" alt="Nek5000 2018 meeting picture">

---

### Software release: CEED v1.0

The CEED team released its first software distribution, [CEED 1.0](http://ceed.exascaleproject.org/ceed-1.0) consisting of 12 integrated Spack packages for [libCEED](libceed.md), [mfem](mfem.md), [nek5000](nek.md), [nekcem](nek.md), [laghos](miniapps.md#laghos-new), [nekbone](miniapps.md#nekbone), [hpgmg](miniapps.md#hpgmg-fe), [occa](occa.md), [magma](magma.md), [gslib](gslib.md), [petsc](petsc.md) and [pumi](pumi.md) plus a new [CEED meta-package](https://github.com/spack/spack/blob/develop/var/spack/repos/builtin/packages/ceed/package.py).

With [Spack](https://spack.io), a user can install the whole CEED software stack simply with: `spack install ceed`.

As part of CEED 1.0, the team developed [comprehensive documentation](http://ceed.exascaleproject.org/ceed-1.0/) including software and compiler configurations for [ALCF](https://www.alcf.anl.gov), [OLCF](https://www.olcf.ornl.gov), [NERSC](http://www.nersc.gov) and [LLNL](https://hpc.llnl.gov):


Platform | Architecture | Spack Configuration |
-------- | ------------ | -------------------- |
**Mac**      | `darwin-x86_64` | [packages](spack/darwin-x86_64-packages.yaml) <i class="fa fa-cloud-download"></i> |
**Linux** (RHEL7)   | `linux-rhel7-x86_64` | [packages](spack/linux-rhel7-x86_64-packages.yaml) <i class="fa fa-cloud-download"></i> |
**Cori** (NERSC) | `cray-CNL-haswell` | [packages](spack/cori-packages.yaml) <i class="fa fa-cloud-download"></i> |
**Edison** (NERSC) | `cray-CNL-ivybridge` | [packages](spack/edison-packages.yaml) <i class="fa fa-cloud-download"></i> |
**Theta** (ALCF) | `cray-CNL-mic_knl` | [packages](spack/theta-packages.yaml) <i class="fa fa-cloud-download"></i> |
**Titan** (OLCF) | `cray-CNL-interlagos` | [packages](spack/titan-packages.yaml) <i class="fa fa-cloud-download"></i> |
**CORAL-EA** (LLNL) | `blueos_3_ppc64le_ib` | [packages](spack/blueos_3_ppc64le_ib-packages.yaml) <i class="fa fa-cloud-download"></i> &nbsp; [compilers](spack/blueos_3_ppc64le_ib-compilers.yaml) <i class="fa fa-cloud-download"></i> |
**TOSS3** (LLNL) | `toss_3_x86_64_ib` | [packages](spack/toss_3_x86_64_ib-packages.yaml) <i class="fa fa-cloud-download"></i> &nbsp; [compilers](spack/toss_3_x86_64_ib-compilers.yaml) <i class="fa fa-cloud-download"></i> |

For more information visit [http://ceed.exascaleproject.org/ceed-1.0](http://ceed.exascaleproject.org/ceed-1.0).

---

### Software release: libCEED v0.2

Version 0.2 of [libCEED](libceed.md), the CEED API library, was released in March 2018 with major improvements in the [OCCA](occa.md) backend.

libCEED is a lightweight portable library that allows, for the first time, a wide variety of applications (written in C, C++, Fortran) to share  a wide variety of discretization kernels (CPU, GPU, OpenMP, OpenCL), including high-performance GPU kernels.

libCEED comes with several examples of its usage, ranging from standalone C codes in the `/examples/ceed` directory to examples based on external packages, such as [MFEM](mfem.md), [PETSc](petsc.md) and [Nek5000](nek.md). Below is an illustration how libCEED enables these very different codes (C++, C, F77) to take advantage of a common set of GPU kernels (see also the [CEED 1.0 GPU demo](http://ceed.exascaleproject.org/ceed-1.0/#gpu-demo)):

```sh
# libCEED examples on CPU and GPU
cd examples/ceed
make
./ex1 -ceed /cpu/self
./ex1 -ceed /gpu/occa
cd ../..

# MFEM+libCEED examples on CPU and GPU
cd examples/mfem
make
./bp1 -ceed /cpu/self -no-vis
./bp1 -ceed /gpu/occa -no-vis
cd ../..

# PETSc+libCEED examples on CPU and GPU
cd examples/petsc
make
./bp1 -ceed /cpu/self
./bp1 -ceed /gpu/occa
cd ../..

# Nek+libCEED examples on CPU and GPU
cd examples/nek5000
./make-nek-examples.sh
./run-nek-example.sh -ceed /cpu/self -b 3
./run-nek-example.sh -ceed /gpu/occa -b 3
cd ../..
```

For more information visit [https://github.com/CEED/libCEED](https://github.com/CEED/libCEED).

---

### Panayot Vassilevski named SIAM fellow

[Panayot Vassilevski](https://people.llnl.gov/vassilevski1), who is part of the CEED team at LLNL, was named a [2018 SIAM fellow](https://sinews.siam.org/Details-Page/siam-announces-class-of-2018-fellows-1) for his work on *designing algebraic approaches for creating and analyzing multilevel algorithms*.

<img src="https://people.llnl.gov/include/getImage.php?person_id=529" align="center" alt="Panayot">

Panayot is the editor-in-chief for [Numerical Linear Algebra with Applications](https://onlinelibrary.wiley.com/journal/10991506) and has published [many papers](https://scholar.google.com/citations?user=GhpkHDAAAAAJ) and a monograph on [Multilevel Block Factorization Preconditioners](http://www.springer.com/us/book/9780387715636).

In CEED, Panayot is working on matrix-free preconditioners for high-order finite element discretizations.

*Congratulations Panayot!*

---

### Benchmark release by Paranumal team

The CEED group at Virginia Tech released [standalone implementations](https://github.com/kswirydo/CEED-A) of CEED's BP1.0, BP3.0, and BP3.5 [benchmark problems](bps.md). For results and discussion, see the ("CEED Code Competition: VT software release")[https://www.paranumal.com/single-post/2018/02/01/CEED-Code-Competition-bake-off-problems] entry in the [Paranumal blog](https://www.paranumal.com).

---

### Workshop on Batched, Reproducible, and Reduced Precision BLAS

CEED's [UTK team]](magma.md) organized a two-session minisymposium at the [SIAM Conference on Parallel Processing and Scientific Computing](https://www.siam.org/meetings/pp18/) in Tokyo, Japan from March 7-10, 2018, devoted on [Batched BLAS Standardization](http://meetings.siam.org/sess/dsp_programsess.cfm?SESSIONCODE=63546).

The minisymposium is part of our efforts on standardization and co-design of exascale discretization APIs with application developers, hardware vendors and ECP software technologies projects.  The goal is to extend the BLAS standard to include batched BLAS computational patterns/"application motifs" that are essential for representing and implementing tensor contractions.

Besides participation from the CEED project, stakeholders from ORNL, Sandia, NVidia, Intel, IBM, and Universities were invited.

---

### New website launched by the Virginia Tech CEED team

A new website was recently launched by the Parallel Numerical Algorithms (Paranumal)
research group at Virginia Tech [here](https://www.paranumal.com). The
site includes a blog that gives some practical computing tips related to
high performance implementations of finite element
methods developed as part of the CEED project [here](https://www.paranumal.com/blog).

---

### Initial release of libCEED: The CEED API library

The initial version of [libCEED](https://github.com/CEED/libCEED),
the CEED API library, was released in December 2017.

libCEED is a high-order API library, that for the first time provides a common
operator description on [algebraic level](https://github.com/CEED/libCEED/blob/master/doc/libCEEDapi.md),
that allows a wide variety of applications to take advantage of the efficient operator
evaluation algorithms in the different CEED packages (from a single source).

Our long-term vision for libCEED is to include a variety of back-end
implementations, ranging from simple reference kernels, to highly optimized
kernels targeting specific devices (e.g. GPUs) or specific polynomial orders.

For more information visit
[https://github.com/CEED/libCEED](https://github.com/CEED/libCEED).

---

### Software release: MFEM v3.3.2

Version 3.3.2 of [MFEM](mfem.md) was released on November 10, 2017. Some of the
new additions in this release are:

- Support for high-order mesh optimization based on the target-matrix
  optimization paradigm from the [ETHOS project](https://www.llnl.gov/casc/ethos).
- Implementation of the community policies in xSDK, the [Extreme-scale
  Scientific Software Development Kit](https://xsdk.info/).
- Integration with the
  [STRUMPACK](http://portal.nersc.gov/project/sparse/strumpack) parallel sparse
  direct solver and preconditioner.
- Several new [linear interpolators](http://mfem.org/lininterp/), five new
  examples and miniapps.
- Various memory, performance, discretization and solver improvements, including
  physical-to-reference space mapping capabilities.
- Continuous integration testing on Linux, Mac and Windows.

For more details, see the [interactive documentation](http://mfem.org/examples) and the
full [CHANGELOG](https://raw.githubusercontent.com/mfem/mfem/master/CHANGELOG) at
[http://mfem.org](http://mfem.org/).

---

### CEED participates in xSDK and FASTMath

MFEM joined xSDK, the [Extreme-scale Scientific Software Development
Kit](https://xsdk.info/) in ECP's software technologies focus area as of release
xSDK-0.3.0, see [https://xsdk.info/packages](https://xsdk.info/packages.).

[MFEM](mfem.md) and [PUMI](pumi.md) are also part of the [FASTMath](https://fastmath-scidac.llnl.gov/index.html)
institute in the SciDAC program, see [https://fastmath-scidac.llnl.gov/software-catalog.html](https://fastmath-scidac.llnl.gov/software-catalog.html).

---

### Software release: Nek5000 v17.0

[Nek5000](nek.md) version 17.0 was released as a major upgrade to Nek5000.
Major features improvements include:

- Refactored build system.
- New user-input parameter file format (`.par` replacing `.rea`).
- Characteristics (large time-step) support for moving mesh problems.
- Moving mesh support for the $PN-PN$ formulation.
- Improved stability for $PN-PN$ with variable viscosity.
- Support for mixed `Helmholtz/CVODE` solves.
- New fast `AMG setup` tool based on HYPRE.
- New `EXODUSII` mesh converter.
- New interface to `libxsmm` (fast MATMUL library).
- Extended `lowMach` solver for time varying thermodynamic pressure.
- Added DG for scalars.
- Reduced solver initialization time (parallel binary reader for all input files).
- Automatic general mesh-to-mesh transfer for restarts.
- Refactored support for overlapping domains (NekNek).
- Added high-pass filter relaxation (alternative to explicit filter).
- Refactored residual projection including support for coupled Helmholtz solves.

---

### Nekbone and Laghos join proxy app suites

The Nekbone and Laghos [miniapps](miniapps.md) developed in CEED were selected to be part of ECP's
initial [Proxy Applications Suite](http://proxyapps.exascaleproject.org/ecp-suite/).

Both miniapps were also picked to be [CORAL-2 benchmarks](https://asc.llnl.gov/coral-2-benchmarks).

Laghos was also selected as one of LLNL's [ASC co-design miniapps](https://codesign.llnl.gov/laghos.php).

---

### 6th Nek5000 User Meeting to be held at U Florida

The 6th Nek5000 User/Developer Meeting will be hosted by the DOE PSAAP-II
Compressible Multiphase Turbulence (CMT) center in Tampa, FL, March 17-18, 2018.

---

### Nek5000 hackathon at UIUC

The inaugural Nek5000 Hackathon was held at NCSA Building, University of
Illinois, Urbana-Champaign (UIUC), IL on Nov 12-14, 2017.

The event was attended by researchers and Nek5000 developers to promote the
application of Nek5000 to new problems from industry, national laboratories, and
academia.  Twenty-five participants spent three days working on setting up new
examples, developing new features, and helping one another to get maximum
performance on their applications. Some of the more prominent exchanges of ideas
included standardization of synthetic turbulent inflow techniques, use of CVODE
for pure advection-diffusion problems, and the use of the characteristics
methods for moving geometry applications.

For more details, see the [Nek5000 hackathon website](https://nek5000.mcs.anl.gov/2017/10/25/hackathon/).

---

### CEED organizing minisymposium at ICOSAHOM 2018

CEED is organizing a minisymposium, *Efficient High-Order Finite Element
Discretizations at Large Scale*, at the [International Conference on Spectral and
High-Order Methods](http://www.icosahom2018.org/) (ICOSAHOM 2018) in London UK,
Jul 9-13, 2018.

The goal of the minisymposium is to discuss the next-generation high-order
discretization algorithm and software, based on finite/spectral element
approaches that will enable a wide range of important scientific applications to
run efficiently on future architecture.

---

### Best Paper Award at NURETH-17

CEED researchers (P. Fischer, E. Merzari, A. Obabko) won a Best Paper Award at
the [17th International Topical Meeting on Nuclear Reactor Thermal
Hydraulics](http://www.nureth17.com) (NURETH-17), held in China in September
2017, with a paper entitled *High-Fidelity Simulation of Flow Induced
Vibrations in Helical Steam Generators for Small Modular Reactors*.

---

### CEED attending Cray, AMD and Intel Deep-Dives

CEED researchers and representatives of the [Nek](nek.md) and [MFEM](mfem.md)
teams will attend the October 2017, ECP vendor deep-dive meetings:

- Cray deep-dive in Bloomington, MN on Oct 18-19
- AMD deep-dive in Austin, TX on Oct 24-25
- Intel deep-dive in Hudson, MA on Oct 21-Nov 2

Topics of discussion include advanced technology and memory design, strong
scaling considerations and the porting and evaluation of CEED's [bake-off
problems](bps.md) and [miniapps](miniapps.md) (Nekbone, Laghos, NekCEM
CEEDling).

---

### New Nekbench repository

New [Nekbench repository](https://github.com/thilinarmtb/NekBench) has been
released to provide scripts that simplify the benchmarking of [Nek5000](nek.md).

The user provides ranges for important parameters ranges (e.g., processor counts
and local problem size ranges) and a test type (e.g., scaling or ping-pong
test). Nekbench will run the given test in the given parameter space using a
Nek5000 case file which is also given by the user (in the ping-pong tests, the
case file is optional).

Nekbench is written using bash scripting language and runs any Unix-like
operating system that supports bash. It has been successfully tested on Linux
laptops/desktops, ALCF Theta, NERSC Cori (KNL and Haswell), and NERSC Edison
machines for scaling tests.

Planned extensions for Nekbench include adding more machine types like ANL's
Cetus, additional support for the ping-pong test type, and automated plot
generation (e.g., scaling study graphs) for each test run.

---

### GPU ports of Nek and Laghos

GPU acceleration is a main focus of the [performance optimization](hw.md)
efforts in CEED. Recent progress in this direction include GPU ports of CEED's
[Nek5000](nek.md) application and the Nekbone and Laghos [miniapps](miniapps.md).

For Nek5000, an initial [GPU-enabled version](https://github.com/Nek5000/Nek5000/tree/openacc)
has been developed based on OpenACC. For Nekbone a pure OpenACC implementation as well as a hybrid
OpenACC/CUDA implementation with a CUDA kernel for matrix-vector multiplication
[has been developed](https://github.com/Nek5000/Nekbone/tree/cuda-openacc).

For Laghos, a [GPU-enabled version of](https://github.com/dmed256/laghos/tree/occa-dev) has been
released using the [OCCA](occa.md) interface. With this approach, the user is able to run Laghos
distributively using varying device types per MPI process, whether serial C++, OpenMP, or CUDA.


---

### First CEED annual meeting held at LLNL

<img src="../img/CEED1AM.jpg" align="center" alt="CEED1AM picture">

CEED held its first annual meeting in August, 2017 at the [HPC Innovation
Center](https://hpcinnovationcenter.llnl.gov/location-and-directions) of
Lawrence Livermore National Laboratory.

The goal of the meeting was to report on the progress in the center, deepen
existing and establish new connections with ECP hardware vendors, ECP software
technologies projects and other collaborators, plan project activities and
brainstorm/work as a group to make technical progress.

In addition to gathering together many of the CEED researchers, the meeting
included representatives of the ECP management, hardware vendors, software
technology and other interested projects.

---

### CEED researchers at ATPESC17

Six CEED researchers presented at the 2017 edition of the [Argonne Training Program
on Extreme-Scale Computing](https://extremecomputingtraining.anl.gov/agenda-2017/),
now part of the Exascale Computing Project.

The CEED presentations covered a wide variety of topics, from overview of Theta,
to GPU programming, dense and sparse linear algebra, and high-order discretizations
on unstructured meshes.

![YouTube](6eBNanfURWY)
![YouTube](lGmPy8xpT4E)
![YouTube](9ipyEz30ODM)
![YouTube](rVhzV1XcktE)
![YouTube](eJ6hRN7TeEU)
![YouTube](46AwtHqKFb8)

Videos of all 2017 talks are available on
[YouTube](https://www.youtube.com/channel/UCfwgjtIQB3puojz_N9ly_Ag/playlists?view=50&sort=dd&shelf_id=1).
CEED researchers have also participated in past editions of the meeting.

---

### CEED BPs and benchmarks repository released

CEED released an initial set of *bake-off (BP) problems*, which are simple
kernels designed to test and compare the performance of high-order codes, both
internally in CEED, as well as in the [broader high-order
community](https://github.com/kronbichler/ceed_benchmarks_dealii).

In addition to the benchmark descriptions on the [CEED BPs page](bps.md), a
[benchmarks repository](https://github.com/CEED/benchmarks) is publicly
available with several implementations of the CEED bake-off problems.
Currently, MFEM, Nek5000 and deal.ii are included, see directories
[tests/mfem_bps](https://github.com/CEED/benchmarks/tree/master/tests/mfem_bps),
[tests/nek5000_bps](https://github.com/CEED/benchmarks/tree/master/tests/nek5000_bps)
and
[tests/dealii_bps](https://github.com/CEED/benchmarks/tree/master/tests/dealii_bps)
respectively.

---

### New Laghos and NekCEM CEEDling miniapps released

Two new miniapps developed in CEED were released in June 2017: Laghos and NekCEM
CEEDling.

Laghos (LAGrangian High-Order Solver) is a new miniapp developed in CEED that
solves the time-dependent Euler equations of compressible gas dynamics in a
moving Lagrangian frame using unstructured high-order finite element spatial
discretization and explicit high-order time-stepping. In CEED, Laghos serves as
a proxy for a sub-component of the MARBL/LLNLApp application.

NekCEM CEEDling is a new NekCEM miniapp, solving the time-domain Maxwell
equation for electromagnetic systems.

For more details, see the CEED [miniapps page](miniapps.md) and the
[Laghos](https://github.com/ceed/Laghos) and [NekCEM
CEEDling](https://github.com/NekCEM/NekCEM_CEEDling) repositories on GitHub.

---

### Paper with MPICH at SC17

Joint paper with the MPICH group, *Why is MPI so Slow? Analyzing the fundamental limits in implementing MPI-3.1* [accepted in Supercomputing 2017](http://sc17.supercomputing.org/presentation/?id=pap554&sess=sess163).
The paper provides an in-depth analysis of the software overheads in the MPI
performance-critical path and exposes mandatory performance overheads that are
unavoidable based on the MPI-3.1 specification.

---

### STRUMPACK support in MFEM

Support for the sparse direct solver and preconditioner STRUMPACK [has been
integrated](https://github.com/mfem/mfem/pull/222) in MFEM.

[STRUMPACK](http://poratal.nersc.gov/project/sparse/strumpack) is being
developed at LBNL and is part of the ECP project *Factorization Based Sparse
Solvers and Preconditioners* (Xiaoye Sherry Li and Pieter Ghysels). The
STRUMPACK solver is based on multifrontal sparse Gaussian elimination and uses
hierarchically semi-separable matrices to compress fill-in. It can be used as an
exact direct solver or as an algebraic, robust and parallel preconditioner for a
range of discretized PDE problems.

---

### 2017 PETSc User Meeting

Over 75 participants from all over the world attended the PETSc User Meeting,
held June 14-16 in Boulder, CO. Hosted by the University of Colorado Boulder, the
event consisted of a one-day tutorial on the solver library PETSc and showcased
the latest research enabled by the functionality available in PETSc. The
[meeting agenda](https://www.mcs.anl.gov/petsc/meetings/2017/index.html) covered
a total of 15 talks, four posters, and two panels.

Thanks to generous support from Intel and Tech-X, 22 students received travel
grants and got to learn about the latest techniques on the large-scale numerical
solution of partial differential equations.

PETSc is a suite of data structures and routines for the scalable (parallel)
solution of scientific applications modeled by partial differential
equations. It has become one of the most widely used numerical software packages
of its kind and has users in application areas ranging from acoustics and
arterial flow to seismology and semiconductors.

---

### GPU Hackathon at BNL

Nek/CEED team participated the [GPU Hackathon 2017](https://www.bnl.gov/gpuhackathon/)
that was held in Brookhaven National Laboratory on June 5-9, 2017.
Our team focused on performing and tuning GPU-enabled [Nek5000/Nekbone/NekCEM](nek.md)
version on large-scale GPU systems for small modular reactor, thermal fluids,
and meta-materials modeling.

---

### Workshop on Batched, Reproducible, and Reduced Precision BLAS

The second [Workshop on Batched, Reproducible, and Reduced Precision BLAS](http://bit.ly/Batch-BLAS-2017)
was held in Atlanta, GA on February 23-25, 2017 including many members of the CEED [MAGMA](magma.md) team.

The goal of this workshop was to touch on extending the Basic Linear Algebra
Software Library (BLAS).  The existing BLAS have proven to be very effective in
assisting portable, efficient software for sequential and some of the current
class of high-performance computers. New computational needs in many
applications have motivated the need to investigate the possibility of extending
the currently accepted standards to provide greater parallelism for small size
operations, reproducibility, and reduced precision support.

Of particular interest to CEED is the use of batched BLAS for finite element
tensor contractions, and thus our team is interested in the establishment of a
batched BLAS standard, highly-optimized implementations, and support from
vendors on various architectures.

This is the second workshop of an open forum to discuss and formalize details
related to batched, reproducible, and reduced precision BLAS. The agenda and the
talks from the first workshop can be found [here](http://bit.ly/Batch-BLAS-2016).

---

### Software release: MFEM v3.3

Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element
methods and a main partner in CEED, was released on January 28, 2017 at [http://mfem.org](http://mfem.org)

The goal of MFEM is to enable high-performance scalable finite element
discretization research and application development on a wide variety of
platforms, ranging from laptops to exascale supercomputers.

It has many features, including:

- 2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.
- Parallel version scalable to hundreds of thousands of MPI cores.
- Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.
- Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.
- Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.
- Scalable algebraic multigrid, time integrators, and eigensolvers.
- Lightweight interactive OpenGL visualization with the MFEM-based [GLVis](http://glvis.org) tool.

Some of the [new additions in version 3.3](https://raw.githubusercontent.com/mfem/mfem/1569425857975d7c961b97d09c300da09aad9f82/CHANGELOG) are:

- Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features
  from the [PETSc](https://www.mcs.anl.gov/petsc) and [SUNDIALS](https://computation.llnl.gov/projects/sundials/sundials-software) suites.
- Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.
- General quadrature and nodal finite element basis types.
- Scalable parallel mesh format.
- Thirty six new integrators for common families of operators.
- Sixteen new serial and parallel example codes.
- Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the [Conduit](https://github.com/LLNL/conduit) mesh blueprint specification.

MFEM is being developed in [CASC](https://computation.llnl.gov/casc), [LLNL](https://www.llnl.gov) and is freely available under LGPL 2.1.
For more details, see the [interactive documentation](http://mfem.org/examples) and the full [CHANGELOG](https://raw.githubusercontent.com/mfem/mfem/master/CHANGELOG).

---

### CEED co-design center announced

The [Exascale Computing Project (ECP)](https://exascaleproject.org) announced on November 11, 2016 its selection
of four [co-design centers](https://exascaleproject.org/ecp_co-design_centers/), including CEED: the Center for
Efficient Exascale Discretizations, which is a [research partnership](about.md) between Lawrence Livermore National Laboratory;
Argonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,
Knoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).

Additional news coverage can be found in [LLNL Newsline](https://www.llnl.gov/news/lawrence-livermore-tapped-lead-%E2%80%98co-design%E2%80%99-center-exascale-computing-ecosystem)
and the [ANL press release](https://www.anl.gov/articles/co-design-centers-help-make-next-generation-exascale-computing-reality).

---

### R&D 100 Award for NekCEM / Nek5000

NekCEM/Nek5000: Scalable High-Order Simulation Codes received a 2016 R&D 100
Award, given by R&D Magazine to 100 top new technologies for the year.

The [R&D 100 citation](https://www.rdmag.com/article/2017/05/featured-r-d-100-award-winner-nekcem-nek5000-scalable-high-order-simulation-codes)
reads:
*"NekCEM/Nek5000: Release 4.0: Scalable High-Order Simulation Codes is an
open-source simulation-software package that delivers highly accurate solutions
for a wide range of scientific applications including electromagnetics, quantum
optics, fluid flow, thermal convection, combustion and magnetohydrodynamics. It
features state-of-the-art, scalable, high-order algorithms that are fast and
efficient on platforms ranging from laptops to the worldâ€™s fastest
computers. The size of the physical phenomena that can be simulated with this
package ranges from quantum dots for nanoscale devices to accretion disks
surrounding black holes. NekCEM provides simulation capabilities for the
analysis of electromagnetic and quantum optical devices, such as particle
accelerators and solar cells. Nek5000 provides turbulent flow simulation
capabilities for a variety of thermal-fluid problems including nuclear reactors,
internal combustion engines, vascular flows, and ocean currents."*

See the [ANL press release](http://www.anl.gov/articles/argonne-researchers-win-three-2016-rd-100-awards)
for more information.
