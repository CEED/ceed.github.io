{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Exascale Co-Design The Center for Efficient Exascale Discretizations (CEED) is a co-design center within the U.S. Department of Energy (DOE) Exascale Computing Project (ECP) with the following goals: Help applications leverage future architectures by providing them with state-of-the-art discretization algorithms that better exploit the hardware and deliver a significant performance gain over conventional low-order methods. Collaborate with hardware vendors and software technologies projects to utilize and impact the upcoming exascale hardware and its software stack through CEED-developed proxies and miniapps . Provide an efficient and user-friendly unstructured PDE discretization component for the upcoming exascale software ecosystem . CEED is a research partnership involving 30+ computational scientists from two DOE labs and five universities, including members of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects. You can reach us by emailing ceed-users@llnl.gov or by leaving a comment in the CEED user forum . The center's co-design efforts are organized in four interconnected R&D thrusts , focused on the following computational motifs and their performance on exascale hardware . See also our publications . PDE-based simulations on unstructured grids CEED is producing a range of software products supporting general finite element algorithms on triangular, quadrilateral, tetrahedral and hexahedral meshes in 3D, 2D and 1D. We target the whole de Rham complex: H 1 , H(curl), H(div) and L 2 /DG spaces and discretizations, including conforming and non-conforming unstructured adaptive mesh refinement (AMR). High-order/spectral finite elements Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation for any order space on any order mesh, including high-order curved meshes and all geometries in the de Rham complex. The CEED software will also include optimized assembly support for low-order methods.","title":"Home"},{"location":"#exascale-co-design","text":"The Center for Efficient Exascale Discretizations (CEED) is a co-design center within the U.S. Department of Energy (DOE) Exascale Computing Project (ECP) with the following goals: Help applications leverage future architectures by providing them with state-of-the-art discretization algorithms that better exploit the hardware and deliver a significant performance gain over conventional low-order methods. Collaborate with hardware vendors and software technologies projects to utilize and impact the upcoming exascale hardware and its software stack through CEED-developed proxies and miniapps . Provide an efficient and user-friendly unstructured PDE discretization component for the upcoming exascale software ecosystem . CEED is a research partnership involving 30+ computational scientists from two DOE labs and five universities, including members of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects. You can reach us by emailing ceed-users@llnl.gov or by leaving a comment in the CEED user forum . The center's co-design efforts are organized in four interconnected R&D thrusts , focused on the following computational motifs and their performance on exascale hardware . See also our publications .","title":"Exascale Co-Design"},{"location":"#pde-based-simulations-on-unstructured-grids","text":"CEED is producing a range of software products supporting general finite element algorithms on triangular, quadrilateral, tetrahedral and hexahedral meshes in 3D, 2D and 1D. We target the whole de Rham complex: H 1 , H(curl), H(div) and L 2 /DG spaces and discretizations, including conforming and non-conforming unstructured adaptive mesh refinement (AMR).","title":"PDE-based simulations on unstructured grids"},{"location":"#high-orderspectral-finite-elements","text":"Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation for any order space on any order mesh, including high-order curved meshes and all geometries in the de Rham complex. The CEED software will also include optimized assembly support for low-order methods.","title":"High-order/spectral finite elements"},{"location":"about/","text":"About CEED The Center for Efficient Exascale Discretizations is a research partnership between two U.S. Department of Energy laboratories and five universities: Argonne National Laboratory Lawrence Livermore National Laboratory Rensselaer Polytechnic Institute The University of Tennessee, Knoxville University of Colorado Boulder University of Illinois Urbana-Champaign Virginia Tech You can reach us by emailing ceed-users@llnl.gov or by leaving a comment in the CEED user forum . This research is supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations (Office of Science and the National Nuclear Security Administration) responsible for the planning and preparation of a capable exascale ecosystem , including software, applications, hardware, advanced system engineering and early testbed platforms, in support of the nation\u2019s exascale computing imperative . Our Team Ahmad Abdelfattah Aleks Obabko Ali Karakus Andrew Siegel Anthony Austin Barry Smith Cameron Smith David Beckingsale David Medina Ian Karlin Jack Dongarra \u2014 Lead for the Hardware thrust Jean-Sylvain Camier Jed Brown \u2014 Lead for the Software thrust Jeremy Thompson Katie Heisey Mark Shephard Matt Otten Misun Min \u2014 Lead for the Applications thrust Panayot Vassilevski Paul Fischer \u2014 Deputy Director of CEED Pedro Bello-Maldonado Robert Rieben Ron Rahaman Scott Parker Som Dutta Stanimire Tomov Stefan Kerkemeier Thilina Ratnayake Tim Warburton Tzanio Kolev \u2014 Director of CEED Valeria Barra Veselin Dobrev \u2014 Lead for the Finite Element thrust Vladimir Tomov Yohann Dudouit Alumni Azzam Haidar Kasia Swirydowicz Kazem Kamran Noel Chalmers Tim Moon Website built with MkDocs , Bootstrap and Bootswatch . Hosted on GitHub . LLNL-WEB-732668. Privacy & Legal Notice .","title":"About"},{"location":"about/#about-ceed","text":"The Center for Efficient Exascale Discretizations is a research partnership between two U.S. Department of Energy laboratories and five universities: Argonne National Laboratory Lawrence Livermore National Laboratory Rensselaer Polytechnic Institute The University of Tennessee, Knoxville University of Colorado Boulder University of Illinois Urbana-Champaign Virginia Tech You can reach us by emailing ceed-users@llnl.gov or by leaving a comment in the CEED user forum . This research is supported by the Exascale Computing Project (17-SC-20-SC), a collaborative effort of two U.S. Department of Energy organizations (Office of Science and the National Nuclear Security Administration) responsible for the planning and preparation of a capable exascale ecosystem , including software, applications, hardware, advanced system engineering and early testbed platforms, in support of the nation\u2019s exascale computing imperative .","title":"About CEED"},{"location":"about/#our-team","text":"Ahmad Abdelfattah Aleks Obabko Ali Karakus Andrew Siegel Anthony Austin Barry Smith Cameron Smith David Beckingsale David Medina Ian Karlin Jack Dongarra \u2014 Lead for the Hardware thrust Jean-Sylvain Camier Jed Brown \u2014 Lead for the Software thrust Jeremy Thompson Katie Heisey Mark Shephard Matt Otten Misun Min \u2014 Lead for the Applications thrust Panayot Vassilevski Paul Fischer \u2014 Deputy Director of CEED Pedro Bello-Maldonado Robert Rieben Ron Rahaman Scott Parker Som Dutta Stanimire Tomov Stefan Kerkemeier Thilina Ratnayake Tim Warburton Tzanio Kolev \u2014 Director of CEED Valeria Barra Veselin Dobrev \u2014 Lead for the Finite Element thrust Vladimir Tomov Yohann Dudouit","title":"Our Team"},{"location":"about/#alumni","text":"Azzam Haidar Kasia Swirydowicz Kazem Kamran Noel Chalmers Tim Moon Website built with MkDocs , Bootstrap and Bootswatch . Hosted on GitHub . LLNL-WEB-732668. Privacy & Legal Notice .","title":"Alumni"},{"location":"ap/","text":"Applications Thrust The goal of CEED's Applications (AP) thrust, led by Misun Min from Argonne National Laboratory , is to impact a wide range of ECP application teams through focused one-on-one interactions, facilitated by CEED application liaisons, as well as through one-to-many interactions, based on the development of easy-to-use discretization libraries for high-order finite element methods. Some of our ECP application targets are: Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL) Multi-physics Simulation Code (LLNL) Multiscale Coupled Urban System (ANL) Transforming Combustion Science and Technology with Exascale Simulations (SNL) Cloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL) In addition to maintaining a close connection with these high-order ECP applications, the AP thrust is also reaching out to low-order and non-ECP apparitions and using its interactions to derive requirements for CEED\u2019s miniapps and software technologies work. The AP thrust is actively involved with a variety of co-design activities both within ECP applications and CEED miniapps, such as: OpenACC- and CUDA-based GPU implementations, I/O performance improvement for large meshes, exploration of lightweight MPI and neighborhood collective MPI, and many more.","title":"Applications"},{"location":"ap/#applications-thrust","text":"The goal of CEED's Applications (AP) thrust, led by Misun Min from Argonne National Laboratory , is to impact a wide range of ECP application teams through focused one-on-one interactions, facilitated by CEED application liaisons, as well as through one-to-many interactions, based on the development of easy-to-use discretization libraries for high-order finite element methods. Some of our ECP application targets are: Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL) Multi-physics Simulation Code (LLNL) Multiscale Coupled Urban System (ANL) Transforming Combustion Science and Technology with Exascale Simulations (SNL) Cloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL) In addition to maintaining a close connection with these high-order ECP applications, the AP thrust is also reaching out to low-order and non-ECP apparitions and using its interactions to derive requirements for CEED\u2019s miniapps and software technologies work. The AP thrust is actively involved with a variety of co-design activities both within ECP applications and CEED miniapps, such as: OpenACC- and CUDA-based GPU implementations, I/O performance improvement for large meshes, exploration of lightweight MPI and neighborhood collective MPI, and many more.","title":"Applications Thrust "},{"location":"bps/","text":"CEED Bake-off Problems (Benchmarks) This page contains the specifications of CEED's bake-off problems : high-order kernels/benchmarks designed to test and compare the performance of high-order codes. Short Summary This section is a just a quick reference using the notation from the Terminology and Notation section below (cf. libCEED documentation ). For more details and software, see the follow-on sections. Bake-off Problems (BPs) BP1 : scalar PCG with mass matrix, $q = p+2$ BP2 : vector PCG with mass matrix, $q = p+2$ BP3 : scalar PCG with stiffness matrix, $q = p+2$ BP4 : vector PCG with stiffness matrix, $q = p+2$ BP5 : scalar PCG with stiffness matrix, $q = p+1$ BP6 : vector PCG with stiffness matrix, $q = p+1$ These are all T-vector -to- T-vector and include parallel scatter + element scatter + element evaluation kernel + element gather + parallel gather. The boundary conditions for BP1 and BP2 are homogeneous Neumann. For the rest of the BP s the boundary conditions are homogeneous Dirichlet. The nodal points, denoted by $p$, are GLL (Gauss-Legendre-Lobatto), while the quadrature points, denoted by $q$, are GL (Gauss-Legendre) for BP1 - BP4 and GLL for BP5 - BP6 . Bake-off Kernels (BKs) BK1 : scalar E-vector -to- E-vector evaluation of mass matrix, $q = p+2$ BK2 : vector E-vector -to- E-vector evaluation of mass matrix, $q = p+2$ BK3 : scalar E-vector -to- E-vector evaluation of stiffness matrix, $q = p+2$ BK4 : vector E-vector -to- E-vector evaluation of stiffness matrix, $q = p+2$ BK5 : scalar E-vector -to- E-vector evaluation of stiffness matrix, $q = p+1$ BK6 : vector E-vector -to- E-vector evaluation of stiffness matrix, $q = p+1$ The BKs are parallel to the BPs, except they do not include parallel and element scatter/gather (the actions of P and G and their transposes). CEED Benchmarks Repository The CEED benchmarks repository is available at https://github.com/CEED/benchmarks and contains implementations for the CEED bake-off problems with MFEM and Nek5000 in the directories tests/mfem_bps and tests/nek5000_bps , respectively. See the included README.md files for more details. VT CEED BP Software Release Checkout the new CEED BP Software Release by the Virginia Tech CEED team. Its main focus is on GPU performance with kernels written in the OCCA framework. Bake-off Problems Description Bake-off Problems . We define our first four bake-off problems, denoted as BP1, BP2, BP3, and BP4. BP1 Solve $B {\\underline u} = {\\underline f}$, where $B$ is the mass matrix. BP2 Solve the (block-diagonal) vector system, $B {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $B$ is as prescribed in BP1. BP3 Solve $A {\\underline u} = {\\underline f}$, where $A$ is the Poisson operator. BP4 Solve the (block-diagonal) vector system, $A {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $A$ is as prescribed in BP3. Bake-off Problem Details . The following items describe the details common to all BPs: Mesh: use a 3D box mesh with hexahedral elements. Boundary conditions (BCs): either no essential BCs, or essential BCs on the whole boundary. Solution space orders: $p=1,2,3,\\ldots,8$, and optionally higher $p$. Quadrature: tensor product Gauss-Legendre (GL) with $q=p+2$ points in each spatial dimension; the quadrature order is $2q-1=2p+3$. The cases of $q=2$, for $p=1$, and $q=3$, for $p=2$, are of interest as they provide a more favorable ratio of the total number of quadrature points to the total number of unknowns. Note that this ratio is smaller (and therefore, advantageous in terms of work per unknown) for larger $p$. Use nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial dimension. Consider mesh orders of $p_{\\rm mesh}=1$, and/or $p_{\\rm mesh}=p$. Elements are assumed to be deformed, meaning that the local element-by-element evaluation cannot exploit simplifications arising from the absence of cross-terms in the Laplacian, etc. Use the QA/PA operator representation, see Terminology and Notation . Meshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a given $s$, use a 3D Cartesian mesh with $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$. For example: if $s=15$, then $s_1=s_2=s_3=5$ if $s=16$, then $s_1=6$ and $s_2=s_3=5$ if $s=17$, then $s_1=s_2=6$ and $s_3=5$ Consider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$ are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$. Using a partitioning of this type allows us to consider cases with a small number of elements per MPI rank - down to one element/rank. Alternative mesh partitioning algorithms are also acceptable. Consider runs with \"large\" number of processors and vary the number of mesh elements per MPI rank starting from 1 and gradually increasing to a number where performance saturation is observed. This suite of benchmarks thus captures both the strong-scale and weak-scale performance limits under the assumption that the underlying code is scalable. Use the conjugate gradients (CG) iterative method to solve the linear system. Since we are interested in evaluating the performance of the QA/PA operator representation (see Terminology and Notation ), we assume no preconditioning, or simple diagonal preconditioning. Required output: Total number of MPI ranks and number of MPI ranks per compute node. Number of mesh elements, $E$. Polynomial degree, $p$. Total number of degrees of freedom, $n_T$ (size of a T-vector ), or approximately $n:=E p^3$. Time per iteration = total CG time $/$ number of CG iterations. Time is measured as maximum over all MPI ranks; using MPI_Wtime() or other similar function. [optional] Number of iterations to reach relative residual reduction of $10^{-6}$. [optional] Time for quadrature-point/partial assembly. Terminology and Notation Vector representation/storage categories: True degrees of freedom/unknowns, T-vector : each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$ this is a non-overlapping vector decomposition usually includes any essential (fixed) dofs. Local (w.r.t. processors) degrees of freedom/unknowns, L-vector : each unknown $i$ has exactly one copy on each processor that owns an element containing $i$ this is an overlapping vector decomposition with overlaps only across different processors - there is no duplication of unknowns on a single processor the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have more than one copy, on different processors. Per element decomposition, E-vector : each unknown $i$ has as many copies as the number of elements that contain $i$ usually, the copies of the unknowns are grouped by the element they belong to. In the case of AMR with hanging nodes (giving rise to hanging dofs): the L-vector is enhanced with the hanging/dependent dofs the additional hanging/dependent dofs are duplicated when they are shared by multiple processors this way, an E-vector can be derived from an L-vector without any communications and without additional computations to derive the dependend dofs in other words, an entry in an E-vector is obtained by copying an entry from the corresponding L-vector, optionally switching the sign of the entry (for $H(\\mathrm{div})$- and $H(\\mathrm{curl})$-conforming spaces). In the case of variable order spaces: the dependent dofs (usually on the higher-order side of a face/edge) can be treated just like the hanging/dependent dofs case. Quadrature point vector, Q-vector : this is similar to E-vector where instead of dofs, the vector represents values at qudrature points, grouped by element. In many cases it is useful to distinguish two types of vectors: X-vector, or primal X-vector, and X'-vector, or dual X-vector here X can be any of the T, L, E, or Q categories for example, the mass matrix operator maps a T-vector to a T'-vector the solutions vector is a T-vector, and the RHS vector is a T'-vector using the parallel prolongation operator, one can map the solution T-vector to a solution L-vector, etc. Operator representation/storage/action categories: Full true-dof parallel assembly, TA , or A : ParCSR or similar format the T in TA indicates that the data format represents an operator from a T-vector to a T'-vector. Full local assembly, LA : CSR matrix on each rank the parallel prolongation operator, $P$, (and its transpose) should use optimized matrix-free action note that $P$ is the operator mapping T-vectors to L-vectors. Element matrix assembly, EA : each element matrix is stored as a dense matrix optimized element and parallel prolongation operators note that the element prolongation operator is the mapping from an L-vector to an E-vector. Quadrature-point/partial assembly, QA or PA : precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix) at all quadrature points in all mesh elements the stored data can be viewed as a Q-vector. Unassembled option, UA or U : no assembly step the action uses directly the mesh node coordinates, and assumes specific form of the coefficient, e.g. constant, piecewise-constant, or given as a Q-vector (Q-coefficient). Notes and Remarks What are good partitioning algorithms for the strong scaling limit? The problem is to generate well balanced partitions when the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4 does not do well on this type of problems. What about METIS 5 and other graph partitioners? Maybe we need to develop specialized algorithms? MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"Benchmarks"},{"location":"bps/#ceed-bake-off-problems-benchmarks","text":"This page contains the specifications of CEED's bake-off problems : high-order kernels/benchmarks designed to test and compare the performance of high-order codes.","title":"CEED Bake-off Problems (Benchmarks)"},{"location":"bps/#short-summary","text":"This section is a just a quick reference using the notation from the Terminology and Notation section below (cf. libCEED documentation ). For more details and software, see the follow-on sections.","title":"Short Summary"},{"location":"bps/#bake-off-problems-bps","text":"BP1 : scalar PCG with mass matrix, $q = p+2$ BP2 : vector PCG with mass matrix, $q = p+2$ BP3 : scalar PCG with stiffness matrix, $q = p+2$ BP4 : vector PCG with stiffness matrix, $q = p+2$ BP5 : scalar PCG with stiffness matrix, $q = p+1$ BP6 : vector PCG with stiffness matrix, $q = p+1$ These are all T-vector -to- T-vector and include parallel scatter + element scatter + element evaluation kernel + element gather + parallel gather. The boundary conditions for BP1 and BP2 are homogeneous Neumann. For the rest of the BP s the boundary conditions are homogeneous Dirichlet. The nodal points, denoted by $p$, are GLL (Gauss-Legendre-Lobatto), while the quadrature points, denoted by $q$, are GL (Gauss-Legendre) for BP1 - BP4 and GLL for BP5 - BP6 .","title":"Bake-off Problems (BPs)"},{"location":"bps/#bake-off-kernels-bks","text":"BK1 : scalar E-vector -to- E-vector evaluation of mass matrix, $q = p+2$ BK2 : vector E-vector -to- E-vector evaluation of mass matrix, $q = p+2$ BK3 : scalar E-vector -to- E-vector evaluation of stiffness matrix, $q = p+2$ BK4 : vector E-vector -to- E-vector evaluation of stiffness matrix, $q = p+2$ BK5 : scalar E-vector -to- E-vector evaluation of stiffness matrix, $q = p+1$ BK6 : vector E-vector -to- E-vector evaluation of stiffness matrix, $q = p+1$ The BKs are parallel to the BPs, except they do not include parallel and element scatter/gather (the actions of P and G and their transposes).","title":"Bake-off Kernels (BKs)"},{"location":"bps/#ceed-benchmarks-repository","text":"The CEED benchmarks repository is available at https://github.com/CEED/benchmarks and contains implementations for the CEED bake-off problems with MFEM and Nek5000 in the directories tests/mfem_bps and tests/nek5000_bps , respectively. See the included README.md files for more details.","title":"CEED Benchmarks Repository"},{"location":"bps/#vt-ceed-bp-software-release","text":"Checkout the new CEED BP Software Release by the Virginia Tech CEED team. Its main focus is on GPU performance with kernels written in the OCCA framework.","title":"VT CEED BP Software Release"},{"location":"bps/#bake-off-problems-description","text":"Bake-off Problems . We define our first four bake-off problems, denoted as BP1, BP2, BP3, and BP4. BP1 Solve $B {\\underline u} = {\\underline f}$, where $B$ is the mass matrix. BP2 Solve the (block-diagonal) vector system, $B {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $B$ is as prescribed in BP1. BP3 Solve $A {\\underline u} = {\\underline f}$, where $A$ is the Poisson operator. BP4 Solve the (block-diagonal) vector system, $A {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $A$ is as prescribed in BP3. Bake-off Problem Details . The following items describe the details common to all BPs: Mesh: use a 3D box mesh with hexahedral elements. Boundary conditions (BCs): either no essential BCs, or essential BCs on the whole boundary. Solution space orders: $p=1,2,3,\\ldots,8$, and optionally higher $p$. Quadrature: tensor product Gauss-Legendre (GL) with $q=p+2$ points in each spatial dimension; the quadrature order is $2q-1=2p+3$. The cases of $q=2$, for $p=1$, and $q=3$, for $p=2$, are of interest as they provide a more favorable ratio of the total number of quadrature points to the total number of unknowns. Note that this ratio is smaller (and therefore, advantageous in terms of work per unknown) for larger $p$. Use nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial dimension. Consider mesh orders of $p_{\\rm mesh}=1$, and/or $p_{\\rm mesh}=p$. Elements are assumed to be deformed, meaning that the local element-by-element evaluation cannot exploit simplifications arising from the absence of cross-terms in the Laplacian, etc. Use the QA/PA operator representation, see Terminology and Notation . Meshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a given $s$, use a 3D Cartesian mesh with $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$. For example: if $s=15$, then $s_1=s_2=s_3=5$ if $s=16$, then $s_1=6$ and $s_2=s_3=5$ if $s=17$, then $s_1=s_2=6$ and $s_3=5$ Consider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$ are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$. Using a partitioning of this type allows us to consider cases with a small number of elements per MPI rank - down to one element/rank. Alternative mesh partitioning algorithms are also acceptable. Consider runs with \"large\" number of processors and vary the number of mesh elements per MPI rank starting from 1 and gradually increasing to a number where performance saturation is observed. This suite of benchmarks thus captures both the strong-scale and weak-scale performance limits under the assumption that the underlying code is scalable. Use the conjugate gradients (CG) iterative method to solve the linear system. Since we are interested in evaluating the performance of the QA/PA operator representation (see Terminology and Notation ), we assume no preconditioning, or simple diagonal preconditioning. Required output: Total number of MPI ranks and number of MPI ranks per compute node. Number of mesh elements, $E$. Polynomial degree, $p$. Total number of degrees of freedom, $n_T$ (size of a T-vector ), or approximately $n:=E p^3$. Time per iteration = total CG time $/$ number of CG iterations. Time is measured as maximum over all MPI ranks; using MPI_Wtime() or other similar function. [optional] Number of iterations to reach relative residual reduction of $10^{-6}$. [optional] Time for quadrature-point/partial assembly.","title":"Bake-off Problems Description"},{"location":"bps/#terminology-and-notation","text":"Vector representation/storage categories: True degrees of freedom/unknowns, T-vector : each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$ this is a non-overlapping vector decomposition usually includes any essential (fixed) dofs. Local (w.r.t. processors) degrees of freedom/unknowns, L-vector : each unknown $i$ has exactly one copy on each processor that owns an element containing $i$ this is an overlapping vector decomposition with overlaps only across different processors - there is no duplication of unknowns on a single processor the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have more than one copy, on different processors. Per element decomposition, E-vector : each unknown $i$ has as many copies as the number of elements that contain $i$ usually, the copies of the unknowns are grouped by the element they belong to. In the case of AMR with hanging nodes (giving rise to hanging dofs): the L-vector is enhanced with the hanging/dependent dofs the additional hanging/dependent dofs are duplicated when they are shared by multiple processors this way, an E-vector can be derived from an L-vector without any communications and without additional computations to derive the dependend dofs in other words, an entry in an E-vector is obtained by copying an entry from the corresponding L-vector, optionally switching the sign of the entry (for $H(\\mathrm{div})$- and $H(\\mathrm{curl})$-conforming spaces). In the case of variable order spaces: the dependent dofs (usually on the higher-order side of a face/edge) can be treated just like the hanging/dependent dofs case. Quadrature point vector, Q-vector : this is similar to E-vector where instead of dofs, the vector represents values at qudrature points, grouped by element. In many cases it is useful to distinguish two types of vectors: X-vector, or primal X-vector, and X'-vector, or dual X-vector here X can be any of the T, L, E, or Q categories for example, the mass matrix operator maps a T-vector to a T'-vector the solutions vector is a T-vector, and the RHS vector is a T'-vector using the parallel prolongation operator, one can map the solution T-vector to a solution L-vector, etc. Operator representation/storage/action categories: Full true-dof parallel assembly, TA , or A : ParCSR or similar format the T in TA indicates that the data format represents an operator from a T-vector to a T'-vector. Full local assembly, LA : CSR matrix on each rank the parallel prolongation operator, $P$, (and its transpose) should use optimized matrix-free action note that $P$ is the operator mapping T-vectors to L-vectors. Element matrix assembly, EA : each element matrix is stored as a dense matrix optimized element and parallel prolongation operators note that the element prolongation operator is the mapping from an L-vector to an E-vector. Quadrature-point/partial assembly, QA or PA : precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix) at all quadrature points in all mesh elements the stored data can be viewed as a Q-vector. Unassembled option, UA or U : no assembly step the action uses directly the mesh node coordinates, and assumes specific form of the coefficient, e.g. constant, piecewise-constant, or given as a Q-vector (Q-coefficient).","title":"Terminology and Notation"},{"location":"bps/#notes-and-remarks","text":"What are good partitioning algorithms for the strong scaling limit? The problem is to generate well balanced partitions when the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4 does not do well on this type of problems. What about METIS 5 and other graph partitioners? Maybe we need to develop specialized algorithms? MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"Notes and Remarks"},{"location":"ceed-1.0/","text":"CEED 1.0 Software Distribution The CEED distribution is a collection of software packages that can be integrated together to enable efficient discretizations in a variety of high-order applications on unstructured grids. CEED is using the Spack package manager for compatible building and installation of its software components. In this initial version, CEED 1.0, the CEED software suite consists of the following 12 packages, plus the CEED meta-package : GSLIB HPGMG Laghos libCEED MAGMA MFEM Nek5000 Nekbone NekCEM PETSc PUMI OCCA First-time users should read Simple Installation and Using the Installation below. (Quick summary: you can build and install all of the above packages with: spack install ceed ) If you are familiar with Spack, consider using the following machine-specific configurations for CEED (see also xSDK's config files ). Platform Architecture Spack Configuration Mac darwin-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Cori (NERSC) cray-CNL-haswell packages Edison (NERSC) cray-CNL-ivybridge packages Theta (ALCF) cray-CNL-mic_knl packages Titan (OLCF) cray-CNL-interlagos packages CORAL-EA (LLNL) blueos_3_ppc64le_ib packages compilers TOSS3 (LLNL) toss_3_x86_64_ib packages compilers For additional details, please consult the following sections: Simple Installation Using the Installation Building MFEM-based Applications Building libCEED-based Applications GPU demo Spack for Beginners Tips and Troubleshooting Building on Mac Building on Linux Building at LLNL TOSS3 Platforms CORAL Early Access Platforms Building at NERSC Cori Edison Building at ALCF Theta Building at OLCF Titan Installing CUDA The CEED team can be contacted by posting to our User Forum or via email at ceed-users@llnl.gov . For issues related to the CEED Spack packages, please start a discussion on the GitHub @spack/ceed page. Simple Installation If Spack is already available on your system and is visible in your PATH , you can install the CEED software simply with: spack install -v ceed To enable package testing during the build process, use instead: spack install -v --test=all ceed If you don't have Spack, you can download it and install CEED with the following commands: git clone https://github.com/spack/spack.git cd spack ./bin/spack install -v ceed To avoid long compile times, we strongly recommend that you add a packages.yaml file for your platform, see above and the Tips and Troubleshooting section. Using the Installation Spack will install the CEED packages (and the libraries they depend on) in a subtree of ./opt/spack/<architecture>/<compiler>/ that is specific to the architecture and compiler used (multiple compiler and/or architecture builds can coexist in a single Spack directory). Below are several examples of how the Spack installation can be linked with and used in user applications. Building MFEM-based Applications The simplest way to use the Spack installation is through the spack location command. For example, MFEM-based codes, such as the MFEM examples, can be simply built as follows: git clone git@github.com:mfem/mfem.git cd mfem; git checkout v3.3.2 cd examples make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk cd ../miniapps/electromagnetics make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk Alternatively, the Spack installation can be exported to a local directory: mkdir ceed spack view --verbose symlink ceed/mfem mfem The ceed/mfem directory now contains the Spack-built MFEM with all of its dependencies (technically, it contains links to all the build files inside the ./opt/spack/ subdirectory for MFEM). In particular, the MFEM library in ceed/mfem/lib and the MFEM build configuration file in ceed/mfem/share/mfem/config.mk . This directory can be used to build the MFEM examples as follows: git clone git@github.com:mfem/mfem.git cd mfem; git checkout v3.3.2 cd examples/petsc make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk cd .. make CONFIG_MK=../../ceed/mfem/share/mfem/config.mk The MFEM miniapps can further be built with: cd ../miniapps/electromagnetics make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk Building libCEED-based Applications Below we illustrate how to use the Spack installation to build libCEED-based applications, by building the examples in the current libCEED distribution. Using spack location , the libCEED examples can be built as follows: git clone git@github.com:CEED/libCEED.git cd libCEED/examples/ceed make CEED_DIR=`spack location -i libceed` ./ex1 -ceed /cpu/self If you have multiple builds of libceed or occa you need to be more specific in the above spack location command. To list all libceed and occa versions use spack find : spack find -lv libceed occa Then either use variants to choose a unique version, e.g. libceed~cuda , or specify the hashes printed in front of the libceed spec , e.g. libceed/yb3fvek or just /yb3fvek (and similarly for occa ). The serial, OpenMP, OpenCL and GPU OCCA backends can be used with: ./ex1 -ceed /cpu/occa ./ex1 -ceed /omp/occa ./ex1 -ceed /ocl/occa ./ex1 -ceed /gpu/occa In order to use the OCCA GPU backend, one needs to install CEED with the cuda variant enabled, i.e. using the spec ceed+cuda : spack install -v ceed+cuda For more details, see the section GPU demo below. With the MAGMA backend, the /cpu/magma and /gpu/magma resource descriptors can also be used. The MFEM/libCEED and PETSc/libCEED examples can be further built with: cd examples/mfem make CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem` ./bp1 -no-vis -o 2 -ceed /cpu/self ./bp3 -no-vis -o 2 -ceed /cpu/self cd ../petsc make CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc` ./bp1 -degree 2 -ceed /cpu/self Note that if PETSC_ARCH is set in your environment, you must either unset it or also pass PETSC_ARCH= in the above command. Depending on the available backends, additional CEED resource descriptors, e.g. petsc/bp1 -degree 2 -ceed /ocl/occa or mfem/bp1 -no-vis --order 2 -ceed /gpu/occa can be provided. Finally, the Nek5000/libCEED examples can be built as follows: cd ../nek5000 export CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000` ./make-nek-examples.sh Then you can run the Nek5000 examples as follows: export MPIEXEC=`spack location -i openmpi`/bin/mpiexec ./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3 In the above example, replace openmpi with wahtever the MPI implementation you have installed with spack. Also, you can do ./run-nek-example.sh -h to find out the options supported by the run script. options: -h|-help Print this usage information and exit -c|-ceed Ceed backend to be used for the run (optional, default: /cpu/self) -e|-example Example name (optional, default: bp1) -n|-np Specify number of MPI ranks for the run (optional, default: 4) -b|-box Specify the box geometry to be found in ./boxes/ directory (Mandatory) More information on running the Nek5000 examples can be found in the libCEED documentation . Alternatively, one can export the Spack install to a local directory: spack view --verbose symlink ceed/libceed libceed spack view --verbose symlink ceed/petsc petsc spack view --verbose symlink ceed/mfem mfem spack view --verbose symlink ceed/nek5000 nek5000 and use that to specify the CEED_DIR , MFEM_DIR and PETSC_DIR variables: cd libCEED/examples/ceed make CEED_DIR=../../ceed/libceed ./ex1 -ceed /cpu/self cd mfem make CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem ./bp1 -no-vis -o 2 -ceed /cpu/self ./bp3 -no-vis -o 2 -ceed /cpu/self cd ../petsc make CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc ./bp1 -degree 2 -ceed /cpu/self GPU demo Below is the full set of commands to install the CEED distribution on a GPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM, PETSc and Nek examples. Note that these are very different codes (C++, C, F77) which can nevertheless take advanatage through libCEED of a common set of GPU kernels. The setenv commands below assume csh / tcsh . We strongly recommend to add a packages.yaml file in order to avoid long compile times, see Tips and Troubleshooting . # Install CEED 1.0 distribution via Spack git clone git@github.com:spack/spack.git cd spack spack install ceed+cuda # Setup CEED component directories setenv CEED_DIR `spack location -i libceed` setenv MFEM_DIR `spack location -i mfem` setenv PETSC_DIR `spack location -i petsc` setenv NEK5K_DIR `spack location -i nek5000` # Clean OCCA cache # rm -rf ~/.occa # Clone libCEED examples directory as proxy for libCEED-based codes git clone git@github.com:CEED/libCEED.git mv libCEED/examples ceed-examples rm -rf libCEED # libCEED examples on CPU and GPU cd ceed-examples/ceed make ./ex1 -ceed /cpu/self ./ex1 -ceed /gpu/occa cd ../.. # MFEM+libCEED examples on CPU and GPU cd ceed-examples/mfem make ./bp1 -ceed /cpu/self -no-vis ./bp1 -ceed /gpu/occa -no-vis cd ../.. # PETSc+libCEED examples on CPU and GPU cd ceed-examples/petsc make ./bp1 -ceed /cpu/self ./bp1 -ceed /gpu/occa cd ../.. # Nek+libCEED examples on CPU and GPU cd ceed-examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self -b 3 ./run-nek-example.sh -ceed /gpu/occa -b 3 cd ../.. Spack for Beginners Spack is a package manager for scientific software that supports multiple versions, configurations, platforms, and compilers. While Spack does not change the build system that already exists in each CEED component, it coordinates the dependencies between these components and enables them to be built with the same compilers and options. If you are new to Spack, here are some Spack commands and options that you may find useful: Spack is a set of Python scripts so there is nothing to install! Just download with git clone https://github.com/spack/spack.git and add spack/bin to your path with the following commands: . share/spack/setup-env.sh for bash / zsh or setenv SPACK_ROOT pwd ; source $SPACK_ROOT/share/spack/setup-env.csh for csh / tcsh . Spack should automatically locate the standard compilers on your system. Use spack compilers to list the ones that have been found. If you need to configure additional compilers, you can do that through the config file, ~/.spack/compilers.yaml , or the platform-specific config file, ~/.spack/<platform>/compilers.yaml . Some examples of such files are provided below. Check the Spack documentation for additional details. Spack likes to build all of its packages. The file ~/.spack/packages.yaml , and similarly the platform-specific, ~/.spack/<platform>/packages.yaml , allow you to list the packages already installed on your system for Spack to use instead of compiling them itself. Some examples are provided below. Skip the -v option of spack install to see only a summary for the building of each package (as opposed to the compilation of individual files): spack install ceed . You can still turn the detailed build output on and off by pressing the v key in the Spack terminal. To troubleshoot the spack install process: spack --debug --verbose install ceed . To do a dry run of the spack install process: spack install --fake ceed . Note that you will have to run spack uninstall --all to clean up after this. To see the specific packages that will be installed for a particular package, e.g. ceed , use: spack spec -I ceed . To see the list of all installed packages: spack find . To list the location where all different versions of the ceed package were installed: spack find --long --paths ceed . Alternatively, for a specific version you can use spack location --install-dir ceed . To uninstall a package, e.g. mfem, including all packages that depend on it: spack uninstall --all --dependents mfem , or spack uninstall /qzn2u7t for a particular hash. To uninstall all packages that were ever installed by Spack: spack uninstall --all . In this case you may also want to clear the caches that Spack maintains with: spack clean -a . Tips and Troubleshooting Building on a Mac The file ceed1-darwin-x86_64-packages.yaml provides a sample packages.yaml file based on Homebrew , that should work on most Macs. (You can use MacPorts instead of Homebrew if you prefer.) packages: all: compiler: [clang] providers: blas: [veclibfort] lapack: [veclibfort] mpi: [openmpi] openmpi: paths: openmpi@3.0.0: ~/brew buildable: False cmake: paths: cmake@3.10.2: ~/brew buildable: False cuda: paths: cuda@9.1.85: /usr/local/cuda buildable: False libx11: paths: libx11@system: /opt/X11 version: [system] buildable: False libxt: paths: libxt@system: /opt/X11 version: [system] buildable: False python: paths: python@2.7.10: /usr buildable: False zlib: paths: zlib@1.2.11: /usr buildable: False The packages in ~/brew were installed with brew install package . If you don't have Homebrew, you can install it and the needed tools with: git clone https://github.com/Homebrew/brew.git cd brew bin/brew install openmpi cmake python zlib The packages in /usr are provided by Apple and come pre-built with Mac OS X. The cuda package is provided by NVIDIA and should be installed separately by downloading it from NVIDIA . We are using the Clang compiler, OpenMPI, and Apple's BLAS/LAPACK accelerator library. Building on a Linux Desktop The file ceed1-linux-rhel7-x86_64-packages.yaml provides a sample packages.yaml file that can be adapted to work on most Linux desktops (this particular file was tested on RHEL7). packages: all: compiler: [gcc] providers: mpi: [openmpi] blas: [netlib-lapack] lapack: [netlib-lapack] netlib-lapack: paths: netlib-lapack@system: /usr/lib64 buildable: False openmpi: paths: openmpi@3.0.0: ~/local buildable: False cmake: paths: cmake@3.10.2: ~/local buildable: False cuda: paths: cuda@9.1.85: ~/local/cuda buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib64 buildable: False The above file uses user-installed OpenMPI, CMake and CUDA packages, with the rest of the CEED prerequisites installed via the yum package manager . Building at LLNL's Computing Center TOSS3 Platforms The file ceed1-toss_3_x86_64_ib-packages.yaml is an example of a packages.yaml file for the TOSS3 system type at LLNL's Livermore Computing center. packages: all: compiler: [intel, gcc, clang, pgi] providers: mpi: [mvapich2, mpich, openmpi] blas: [intel-mkl, openblas] lapack: [intel-mkl, openblas] intel-mkl: paths: intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0 buildable: False mvapich2: paths: mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1 mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3 mvapich2@2.2%gcc@7.1.0: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-7.1.0 buildable: False cmake: paths: cmake@3.8.2: /usr/tce/packages/cmake/cmake-3.8.2 buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 buildable: False zlib: paths: zlib@1.2.7: /usr buildable: False The above file can be used to build CEED with different compilers (Intel being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for the TOSS3 platform can be found here: ceed1-toss_3_x86_64_ib-compilers.yaml . CORAL Early Access Platforms The file ceed1-blueos_3_ppc64le_ib-packages.yaml is an example of a packages.yaml file for the CORAL early access systems at LLNL's Livermore Computing center (this particular file is for the Ray machine). packages: all: compiler: [xl_r, xl, gcc, clang, pgi] providers: mpi: [spectrum-mpi] blas: [essl] lapack: [netlib-lapack] essl: paths: essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0 variants: threads=none version: [6.1.0] buildable: False veclibfort: buildable: False intel-parallel-studio: buildable: False intel-mkl: buildable: False atlas: buildable: False openblas: # OpenBLAS can be built only with gcc buildable: False netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant variants: +external-blas~lapacke spectrum-mpi: paths: spectrum-mpi@2017-04-03%xl_r@13.1.7-beta3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.03.21 spectrum-mpi@2017-04-03%xl_r@13.1.7-beta2: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.02.22 spectrum-mpi@2017-04-03%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-gcc-4.9.3 spectrum-mpi@2017-04-03%clang@3.8.0: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-clang-coral-2018.02.09 buildable: False cmake: paths: cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2 version: [3.9.2] buildable: False cuda: paths: cuda@9.0.176: /usr/tce/packages/cuda/cuda-9.0.176 cuda@9.1.85: /usr/tce/packages/cuda/cuda-9.1.85 version: [9.0.176, 9.1.85] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr/tcetmp/packages/python/python-2.7.14 version: [2.7.14] buildable: False A corresponding compilers.yaml file can be found here: ceed1-blueos_3_ppc64le_ib-compilers.yaml . Building at NERSC Cori The file ceed1-cori-packages.yaml is an example of a packages.yaml file for the Cori system at NERSC . packages: all: compiler: [gcc@5.2.0, intel/16.0.3.210] providers: mpi: [mpich] mpich: modules: mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-haswell: cray-mpich mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-haswell: cray-mpich buildable: False cmake: modules: cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-haswell: cmake cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-haswell: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14%gcc@5.2.0 arch=cray-CNL-haswell: /usr python@2.7.14%intel@16.0.3.210 arch=cray-CNL-haswell: /usr buildable: False Edison The file ceed1-edison-packages.yaml is an example of a packages.yaml file for the Edison system at NERSC . packages: all: compiler: [gcc@5.2.0, intel/16.0.3.210] providers: mpi: [mpich] mpich: modules: mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-ivybridge: cray-mpich mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-ivybridge: cray-mpich buildable: False cmake: modules: cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-ivybridge: cmake cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-ivybridge: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14%gcc@5.2.0 arch=cray-CNL-ivybridge: /usr python@2.7.14%intel@16.0.3.210 arch=cray-CNL-ivybridge: /usr buildable: False Building at ALCF Theta The file ceed1-theta-packages.yaml is an example of a packages.yaml file for the Theta system at ALCF . packages: all: compiler: [intel@16.0.3.210, gcc@5.3.0] providers: mpi: [mpich] intel-mkl: paths: intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel buildable: False mpich: modules: # requires 'module load cce' otherwise gives parsing error mpich@7.6.3%gcc@5.3.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 buildable: False cmake: paths: cmake@3.5.2%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.13%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False Building at OLCF Titan The file ceed1-titan-packages.yaml is an example of a packages.yaml file for the Titan system at OLCF . packages: all: compiler: [cce/8.6.4] providers: mpi: [mpich] mpich: modules: mpich@7.6.3%cce@8.6.4 arch=cray-CNL-interlagos: cray-mpich buildable: False cmake: paths: cmake@3.9.0%cce@8.6.4: /autofs/nccs-svm1_sw/titan/.swci/0-login/opt/spack/20170612/linux-suse_linux11-x86_64/gcc-4.3.4/cmake-3.9.0-owxiriblogovogl5zbrg45ulm3ln34cx/bin buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: modules: python@2.7.9%cce@8.6.4 arch=cray-CNL-interlagos: python buildable: False zlib: paths: zlib@1.2.17: /usr/lib64 buildable: False The default install of curl on Titan does not support ssl, so you need to add the path of a newer install to your PATH : module show curl # get the /path/to/curl/bin/dir export PATH=/path/to/curl/bin/dir:$PATH Additional issues on Titan: Spack does not support cray-libsci for BLAS/LAPACK (there is no 'dummy package' for cray-libsci yet ); the Cray compiler, cce , fails to build openblas (it does not e.g. recognize the -m64 flag, there may be other issues). With these caveats, the CEED metapackage can be installed with: ./bin/spack --debug --verbose install -v ceed%pgi@17.9.0 target=interlagos Note that spack will hang if you redirect std[err|out] to a file ( &> log ) and background the command (by appending an & ). Installing CUDA Several CEED packages depend on CUDA: OCCA, MAGMA and libCEED. To build these, add the cuda variant to the Spack build: sh spack install ceed+cuda You will need to have the NVIDIA CUDA SDK and driver installed on your system, see developer.nvidia.com , and specify it in the packages.yaml file. See, for example, the cuda section in Building on a Mac , or the ceed1-linux-rhel7-x86_64-packages.yaml file.","title":"_CEED 1.0"},{"location":"ceed-1.0/#ceed-10-software-distribution","text":"The CEED distribution is a collection of software packages that can be integrated together to enable efficient discretizations in a variety of high-order applications on unstructured grids. CEED is using the Spack package manager for compatible building and installation of its software components. In this initial version, CEED 1.0, the CEED software suite consists of the following 12 packages, plus the CEED meta-package : GSLIB HPGMG Laghos libCEED MAGMA MFEM Nek5000 Nekbone NekCEM PETSc PUMI OCCA First-time users should read Simple Installation and Using the Installation below. (Quick summary: you can build and install all of the above packages with: spack install ceed ) If you are familiar with Spack, consider using the following machine-specific configurations for CEED (see also xSDK's config files ). Platform Architecture Spack Configuration Mac darwin-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Cori (NERSC) cray-CNL-haswell packages Edison (NERSC) cray-CNL-ivybridge packages Theta (ALCF) cray-CNL-mic_knl packages Titan (OLCF) cray-CNL-interlagos packages CORAL-EA (LLNL) blueos_3_ppc64le_ib packages compilers TOSS3 (LLNL) toss_3_x86_64_ib packages compilers For additional details, please consult the following sections: Simple Installation Using the Installation Building MFEM-based Applications Building libCEED-based Applications GPU demo Spack for Beginners Tips and Troubleshooting Building on Mac Building on Linux Building at LLNL TOSS3 Platforms CORAL Early Access Platforms Building at NERSC Cori Edison Building at ALCF Theta Building at OLCF Titan Installing CUDA The CEED team can be contacted by posting to our User Forum or via email at ceed-users@llnl.gov . For issues related to the CEED Spack packages, please start a discussion on the GitHub @spack/ceed page.","title":"CEED 1.0 Software Distribution"},{"location":"ceed-1.0/#simple-installation","text":"If Spack is already available on your system and is visible in your PATH , you can install the CEED software simply with: spack install -v ceed To enable package testing during the build process, use instead: spack install -v --test=all ceed If you don't have Spack, you can download it and install CEED with the following commands: git clone https://github.com/spack/spack.git cd spack ./bin/spack install -v ceed To avoid long compile times, we strongly recommend that you add a packages.yaml file for your platform, see above and the Tips and Troubleshooting section.","title":"Simple Installation"},{"location":"ceed-1.0/#using-the-installation","text":"Spack will install the CEED packages (and the libraries they depend on) in a subtree of ./opt/spack/<architecture>/<compiler>/ that is specific to the architecture and compiler used (multiple compiler and/or architecture builds can coexist in a single Spack directory). Below are several examples of how the Spack installation can be linked with and used in user applications.","title":"Using the Installation"},{"location":"ceed-1.0/#building-mfem-based-applications","text":"The simplest way to use the Spack installation is through the spack location command. For example, MFEM-based codes, such as the MFEM examples, can be simply built as follows: git clone git@github.com:mfem/mfem.git cd mfem; git checkout v3.3.2 cd examples make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk cd ../miniapps/electromagnetics make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk Alternatively, the Spack installation can be exported to a local directory: mkdir ceed spack view --verbose symlink ceed/mfem mfem The ceed/mfem directory now contains the Spack-built MFEM with all of its dependencies (technically, it contains links to all the build files inside the ./opt/spack/ subdirectory for MFEM). In particular, the MFEM library in ceed/mfem/lib and the MFEM build configuration file in ceed/mfem/share/mfem/config.mk . This directory can be used to build the MFEM examples as follows: git clone git@github.com:mfem/mfem.git cd mfem; git checkout v3.3.2 cd examples/petsc make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk cd .. make CONFIG_MK=../../ceed/mfem/share/mfem/config.mk The MFEM miniapps can further be built with: cd ../miniapps/electromagnetics make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk","title":"Building MFEM-based Applications"},{"location":"ceed-1.0/#building-libceed-based-applications","text":"Below we illustrate how to use the Spack installation to build libCEED-based applications, by building the examples in the current libCEED distribution. Using spack location , the libCEED examples can be built as follows: git clone git@github.com:CEED/libCEED.git cd libCEED/examples/ceed make CEED_DIR=`spack location -i libceed` ./ex1 -ceed /cpu/self If you have multiple builds of libceed or occa you need to be more specific in the above spack location command. To list all libceed and occa versions use spack find : spack find -lv libceed occa Then either use variants to choose a unique version, e.g. libceed~cuda , or specify the hashes printed in front of the libceed spec , e.g. libceed/yb3fvek or just /yb3fvek (and similarly for occa ). The serial, OpenMP, OpenCL and GPU OCCA backends can be used with: ./ex1 -ceed /cpu/occa ./ex1 -ceed /omp/occa ./ex1 -ceed /ocl/occa ./ex1 -ceed /gpu/occa In order to use the OCCA GPU backend, one needs to install CEED with the cuda variant enabled, i.e. using the spec ceed+cuda : spack install -v ceed+cuda For more details, see the section GPU demo below. With the MAGMA backend, the /cpu/magma and /gpu/magma resource descriptors can also be used. The MFEM/libCEED and PETSc/libCEED examples can be further built with: cd examples/mfem make CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem` ./bp1 -no-vis -o 2 -ceed /cpu/self ./bp3 -no-vis -o 2 -ceed /cpu/self cd ../petsc make CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc` ./bp1 -degree 2 -ceed /cpu/self Note that if PETSC_ARCH is set in your environment, you must either unset it or also pass PETSC_ARCH= in the above command. Depending on the available backends, additional CEED resource descriptors, e.g. petsc/bp1 -degree 2 -ceed /ocl/occa or mfem/bp1 -no-vis --order 2 -ceed /gpu/occa can be provided. Finally, the Nek5000/libCEED examples can be built as follows: cd ../nek5000 export CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000` ./make-nek-examples.sh Then you can run the Nek5000 examples as follows: export MPIEXEC=`spack location -i openmpi`/bin/mpiexec ./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3 In the above example, replace openmpi with wahtever the MPI implementation you have installed with spack. Also, you can do ./run-nek-example.sh -h to find out the options supported by the run script. options: -h|-help Print this usage information and exit -c|-ceed Ceed backend to be used for the run (optional, default: /cpu/self) -e|-example Example name (optional, default: bp1) -n|-np Specify number of MPI ranks for the run (optional, default: 4) -b|-box Specify the box geometry to be found in ./boxes/ directory (Mandatory) More information on running the Nek5000 examples can be found in the libCEED documentation . Alternatively, one can export the Spack install to a local directory: spack view --verbose symlink ceed/libceed libceed spack view --verbose symlink ceed/petsc petsc spack view --verbose symlink ceed/mfem mfem spack view --verbose symlink ceed/nek5000 nek5000 and use that to specify the CEED_DIR , MFEM_DIR and PETSC_DIR variables: cd libCEED/examples/ceed make CEED_DIR=../../ceed/libceed ./ex1 -ceed /cpu/self cd mfem make CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem ./bp1 -no-vis -o 2 -ceed /cpu/self ./bp3 -no-vis -o 2 -ceed /cpu/self cd ../petsc make CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc ./bp1 -degree 2 -ceed /cpu/self","title":"Building libCEED-based Applications"},{"location":"ceed-1.0/#gpu-demo","text":"Below is the full set of commands to install the CEED distribution on a GPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM, PETSc and Nek examples. Note that these are very different codes (C++, C, F77) which can nevertheless take advanatage through libCEED of a common set of GPU kernels. The setenv commands below assume csh / tcsh . We strongly recommend to add a packages.yaml file in order to avoid long compile times, see Tips and Troubleshooting . # Install CEED 1.0 distribution via Spack git clone git@github.com:spack/spack.git cd spack spack install ceed+cuda # Setup CEED component directories setenv CEED_DIR `spack location -i libceed` setenv MFEM_DIR `spack location -i mfem` setenv PETSC_DIR `spack location -i petsc` setenv NEK5K_DIR `spack location -i nek5000` # Clean OCCA cache # rm -rf ~/.occa # Clone libCEED examples directory as proxy for libCEED-based codes git clone git@github.com:CEED/libCEED.git mv libCEED/examples ceed-examples rm -rf libCEED # libCEED examples on CPU and GPU cd ceed-examples/ceed make ./ex1 -ceed /cpu/self ./ex1 -ceed /gpu/occa cd ../.. # MFEM+libCEED examples on CPU and GPU cd ceed-examples/mfem make ./bp1 -ceed /cpu/self -no-vis ./bp1 -ceed /gpu/occa -no-vis cd ../.. # PETSc+libCEED examples on CPU and GPU cd ceed-examples/petsc make ./bp1 -ceed /cpu/self ./bp1 -ceed /gpu/occa cd ../.. # Nek+libCEED examples on CPU and GPU cd ceed-examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self -b 3 ./run-nek-example.sh -ceed /gpu/occa -b 3 cd ../..","title":"GPU demo"},{"location":"ceed-1.0/#spack-for-beginners","text":"Spack is a package manager for scientific software that supports multiple versions, configurations, platforms, and compilers. While Spack does not change the build system that already exists in each CEED component, it coordinates the dependencies between these components and enables them to be built with the same compilers and options. If you are new to Spack, here are some Spack commands and options that you may find useful: Spack is a set of Python scripts so there is nothing to install! Just download with git clone https://github.com/spack/spack.git and add spack/bin to your path with the following commands: . share/spack/setup-env.sh for bash / zsh or setenv SPACK_ROOT pwd ; source $SPACK_ROOT/share/spack/setup-env.csh for csh / tcsh . Spack should automatically locate the standard compilers on your system. Use spack compilers to list the ones that have been found. If you need to configure additional compilers, you can do that through the config file, ~/.spack/compilers.yaml , or the platform-specific config file, ~/.spack/<platform>/compilers.yaml . Some examples of such files are provided below. Check the Spack documentation for additional details. Spack likes to build all of its packages. The file ~/.spack/packages.yaml , and similarly the platform-specific, ~/.spack/<platform>/packages.yaml , allow you to list the packages already installed on your system for Spack to use instead of compiling them itself. Some examples are provided below. Skip the -v option of spack install to see only a summary for the building of each package (as opposed to the compilation of individual files): spack install ceed . You can still turn the detailed build output on and off by pressing the v key in the Spack terminal. To troubleshoot the spack install process: spack --debug --verbose install ceed . To do a dry run of the spack install process: spack install --fake ceed . Note that you will have to run spack uninstall --all to clean up after this. To see the specific packages that will be installed for a particular package, e.g. ceed , use: spack spec -I ceed . To see the list of all installed packages: spack find . To list the location where all different versions of the ceed package were installed: spack find --long --paths ceed . Alternatively, for a specific version you can use spack location --install-dir ceed . To uninstall a package, e.g. mfem, including all packages that depend on it: spack uninstall --all --dependents mfem , or spack uninstall /qzn2u7t for a particular hash. To uninstall all packages that were ever installed by Spack: spack uninstall --all . In this case you may also want to clear the caches that Spack maintains with: spack clean -a .","title":"Spack for Beginners"},{"location":"ceed-1.0/#tips-and-troubleshooting","text":"","title":"Tips and Troubleshooting"},{"location":"ceed-1.0/#building-on-a-mac","text":"The file ceed1-darwin-x86_64-packages.yaml provides a sample packages.yaml file based on Homebrew , that should work on most Macs. (You can use MacPorts instead of Homebrew if you prefer.) packages: all: compiler: [clang] providers: blas: [veclibfort] lapack: [veclibfort] mpi: [openmpi] openmpi: paths: openmpi@3.0.0: ~/brew buildable: False cmake: paths: cmake@3.10.2: ~/brew buildable: False cuda: paths: cuda@9.1.85: /usr/local/cuda buildable: False libx11: paths: libx11@system: /opt/X11 version: [system] buildable: False libxt: paths: libxt@system: /opt/X11 version: [system] buildable: False python: paths: python@2.7.10: /usr buildable: False zlib: paths: zlib@1.2.11: /usr buildable: False The packages in ~/brew were installed with brew install package . If you don't have Homebrew, you can install it and the needed tools with: git clone https://github.com/Homebrew/brew.git cd brew bin/brew install openmpi cmake python zlib The packages in /usr are provided by Apple and come pre-built with Mac OS X. The cuda package is provided by NVIDIA and should be installed separately by downloading it from NVIDIA . We are using the Clang compiler, OpenMPI, and Apple's BLAS/LAPACK accelerator library.","title":"Building on a Mac"},{"location":"ceed-1.0/#building-on-a-linux-desktop","text":"The file ceed1-linux-rhel7-x86_64-packages.yaml provides a sample packages.yaml file that can be adapted to work on most Linux desktops (this particular file was tested on RHEL7). packages: all: compiler: [gcc] providers: mpi: [openmpi] blas: [netlib-lapack] lapack: [netlib-lapack] netlib-lapack: paths: netlib-lapack@system: /usr/lib64 buildable: False openmpi: paths: openmpi@3.0.0: ~/local buildable: False cmake: paths: cmake@3.10.2: ~/local buildable: False cuda: paths: cuda@9.1.85: ~/local/cuda buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib64 buildable: False The above file uses user-installed OpenMPI, CMake and CUDA packages, with the rest of the CEED prerequisites installed via the yum package manager .","title":"Building on a Linux Desktop"},{"location":"ceed-1.0/#building-at-llnls-computing-center","text":"","title":"Building at LLNL's Computing Center"},{"location":"ceed-1.0/#toss3-platforms","text":"The file ceed1-toss_3_x86_64_ib-packages.yaml is an example of a packages.yaml file for the TOSS3 system type at LLNL's Livermore Computing center. packages: all: compiler: [intel, gcc, clang, pgi] providers: mpi: [mvapich2, mpich, openmpi] blas: [intel-mkl, openblas] lapack: [intel-mkl, openblas] intel-mkl: paths: intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0 buildable: False mvapich2: paths: mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1 mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3 mvapich2@2.2%gcc@7.1.0: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-7.1.0 buildable: False cmake: paths: cmake@3.8.2: /usr/tce/packages/cmake/cmake-3.8.2 buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 buildable: False zlib: paths: zlib@1.2.7: /usr buildable: False The above file can be used to build CEED with different compilers (Intel being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for the TOSS3 platform can be found here: ceed1-toss_3_x86_64_ib-compilers.yaml .","title":"TOSS3 Platforms"},{"location":"ceed-1.0/#coral-early-access-platforms","text":"The file ceed1-blueos_3_ppc64le_ib-packages.yaml is an example of a packages.yaml file for the CORAL early access systems at LLNL's Livermore Computing center (this particular file is for the Ray machine). packages: all: compiler: [xl_r, xl, gcc, clang, pgi] providers: mpi: [spectrum-mpi] blas: [essl] lapack: [netlib-lapack] essl: paths: essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0 variants: threads=none version: [6.1.0] buildable: False veclibfort: buildable: False intel-parallel-studio: buildable: False intel-mkl: buildable: False atlas: buildable: False openblas: # OpenBLAS can be built only with gcc buildable: False netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant variants: +external-blas~lapacke spectrum-mpi: paths: spectrum-mpi@2017-04-03%xl_r@13.1.7-beta3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.03.21 spectrum-mpi@2017-04-03%xl_r@13.1.7-beta2: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.02.22 spectrum-mpi@2017-04-03%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-gcc-4.9.3 spectrum-mpi@2017-04-03%clang@3.8.0: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-clang-coral-2018.02.09 buildable: False cmake: paths: cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2 version: [3.9.2] buildable: False cuda: paths: cuda@9.0.176: /usr/tce/packages/cuda/cuda-9.0.176 cuda@9.1.85: /usr/tce/packages/cuda/cuda-9.1.85 version: [9.0.176, 9.1.85] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14: /usr/tcetmp/packages/python/python-2.7.14 version: [2.7.14] buildable: False A corresponding compilers.yaml file can be found here: ceed1-blueos_3_ppc64le_ib-compilers.yaml .","title":"CORAL Early Access Platforms"},{"location":"ceed-1.0/#building-at-nersc","text":"","title":"Building at NERSC"},{"location":"ceed-1.0/#cori","text":"The file ceed1-cori-packages.yaml is an example of a packages.yaml file for the Cori system at NERSC . packages: all: compiler: [gcc@5.2.0, intel/16.0.3.210] providers: mpi: [mpich] mpich: modules: mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-haswell: cray-mpich mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-haswell: cray-mpich buildable: False cmake: modules: cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-haswell: cmake cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-haswell: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14%gcc@5.2.0 arch=cray-CNL-haswell: /usr python@2.7.14%intel@16.0.3.210 arch=cray-CNL-haswell: /usr buildable: False","title":"Cori"},{"location":"ceed-1.0/#edison","text":"The file ceed1-edison-packages.yaml is an example of a packages.yaml file for the Edison system at NERSC . packages: all: compiler: [gcc@5.2.0, intel/16.0.3.210] providers: mpi: [mpich] mpich: modules: mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-ivybridge: cray-mpich mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-ivybridge: cray-mpich buildable: False cmake: modules: cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-ivybridge: cmake cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-ivybridge: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.14%gcc@5.2.0 arch=cray-CNL-ivybridge: /usr python@2.7.14%intel@16.0.3.210 arch=cray-CNL-ivybridge: /usr buildable: False","title":"Edison"},{"location":"ceed-1.0/#building-at-alcf","text":"","title":"Building at ALCF"},{"location":"ceed-1.0/#theta","text":"The file ceed1-theta-packages.yaml is an example of a packages.yaml file for the Theta system at ALCF . packages: all: compiler: [intel@16.0.3.210, gcc@5.3.0] providers: mpi: [mpich] intel-mkl: paths: intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel buildable: False mpich: modules: # requires 'module load cce' otherwise gives parsing error mpich@7.6.3%gcc@5.3.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 buildable: False cmake: paths: cmake@3.5.2%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: paths: python@2.7.13%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False","title":"Theta"},{"location":"ceed-1.0/#building-at-olcf","text":"","title":"Building at OLCF"},{"location":"ceed-1.0/#titan","text":"The file ceed1-titan-packages.yaml is an example of a packages.yaml file for the Titan system at OLCF . packages: all: compiler: [cce/8.6.4] providers: mpi: [mpich] mpich: modules: mpich@7.6.3%cce@8.6.4 arch=cray-CNL-interlagos: cray-mpich buildable: False cmake: paths: cmake@3.9.0%cce@8.6.4: /autofs/nccs-svm1_sw/titan/.swci/0-login/opt/spack/20170612/linux-suse_linux11-x86_64/gcc-4.3.4/cmake-3.9.0-owxiriblogovogl5zbrg45ulm3ln34cx/bin buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False python: modules: python@2.7.9%cce@8.6.4 arch=cray-CNL-interlagos: python buildable: False zlib: paths: zlib@1.2.17: /usr/lib64 buildable: False The default install of curl on Titan does not support ssl, so you need to add the path of a newer install to your PATH : module show curl # get the /path/to/curl/bin/dir export PATH=/path/to/curl/bin/dir:$PATH Additional issues on Titan: Spack does not support cray-libsci for BLAS/LAPACK (there is no 'dummy package' for cray-libsci yet ); the Cray compiler, cce , fails to build openblas (it does not e.g. recognize the -m64 flag, there may be other issues). With these caveats, the CEED metapackage can be installed with: ./bin/spack --debug --verbose install -v ceed%pgi@17.9.0 target=interlagos Note that spack will hang if you redirect std[err|out] to a file ( &> log ) and background the command (by appending an & ).","title":"Titan"},{"location":"ceed-1.0/#installing-cuda","text":"Several CEED packages depend on CUDA: OCCA, MAGMA and libCEED. To build these, add the cuda variant to the Spack build: sh spack install ceed+cuda You will need to have the NVIDIA CUDA SDK and driver installed on your system, see developer.nvidia.com , and specify it in the packages.yaml file. See, for example, the cuda section in Building on a Mac , or the ceed1-linux-rhel7-x86_64-packages.yaml file.","title":"Installing CUDA"},{"location":"ceed-2.0/","text":"CEED 2.0 Software Distribution The CEED distribution is a collection of software packages that can be integrated together to enable efficient discretizations in a variety of high-order applications on unstructured grids. CEED is using the Spack package manager for compatible building and installation of its software components. In this version, CEED 2.0, the CEED software suite consists of the following 12 packages, plus the CEED meta-package : GSLIB -1.0.2 HPGMG -0.4 Laghos -2.0 libCEED -0.4 MAGMA -2.5.0 MFEM -3.4 Nek5000 -17.0 Nekbone -17.0 NekCEM -7332619 PETSc -3.11.1 PUMI -2.2.0 OCCA -1.0.8 If you are interested in the previous release, see the CEED-1.0 page. First-time users should read Simple Installation and Using the Installation below. (Quick summary: you can build and install all of the above packages with: spack install ceed ) If you are familiar with Spack, consider using the following machine-specific configurations for CEED (see also the spack-configs repository and the xSDK's config files ). Platform Architecture Spack Configuration Mac darwin-highsierra-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Linux (Ubuntu) ubuntu18.10-x86_64 packages Cori (NERSC) cray-cnl9-haswell packages Theta (ALCF) cray-CNL-mic_knl packages Pascal (LLNL) toss_3_x86_64_ib packages compilers Lassen (LLNL) linux-rhel7-ppc64le packages compilers Summit (ORNL) linux-rhel7-ppc64le packages For additional details, please consult the following sections: Simple Installation Using the Installation Building MFEM-based Applications Building libCEED-based Applications Using Containers NERSC's Shifter Singularity GPU demo Spack for Beginners Tips and Troubleshooting Building on Mac Building on Linux Building at LLNL Pascal (TOSS3) Lassen Building at NERSC Cori Building at ALCF Theta Installing CUDA The CEED team can be contacted by posting to our User Forum or via email at ceed-users@llnl.gov . For issues related to the CEED Spack packages, please start a discussion on the GitHub @spack/ceed page. Simple Installation If Spack is already available on your system and is visible in your PATH , you can install the CEED software simply with: spack install -v ceed To enable package testing during the build process, use instead: spack install -v --test=all ceed If you don't have Spack, you can download it and install CEED with the following commands: git clone https://github.com/spack/spack.git cd spack ./bin/spack install -v ceed To avoid long compile times, we strongly recommend that you add a packages.yaml file for your platform, see above and the Tips and Troubleshooting section. Using the Installation Spack will install the CEED packages (and the libraries they depend on) in a subtree of ./opt/spack/<architecture>/<compiler>/ that is specific to the architecture and compiler used (multiple compiler and/or architecture builds can coexist in a single Spack directory). Below are several examples of how the Spack installation can be linked with and used in user applications. Building MFEM-based Applications The simplest way to use the Spack installation is through the spack location command. For example, MFEM-based codes, such as the MFEM examples, can be simply built as follows: git clone https://github.com/mfem/mfem.git cd mfem; git checkout v3.4 cd examples make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk cd ../miniapps/electromagnetics make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk Alternatively, the Spack installation can be exported to a local directory: mkdir ceed spack view --verbose symlink ceed/mfem mfem The ceed/mfem directory now contains the Spack-built MFEM with all of its dependencies (technically, it contains links to all the build files inside the ./opt/spack/ subdirectory for MFEM). In particular, the MFEM library in ceed/mfem/lib and the MFEM build configuration file in ceed/mfem/share/mfem/config.mk . This directory can be used to build the MFEM examples as follows: git clone https://github.com/mfem/mfem.git cd mfem; git checkout v3.4 cd examples/petsc make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk cd .. make CONFIG_MK=../../ceed/mfem/share/mfem/config.mk The MFEM miniapps can further be built with: cd ../miniapps/electromagnetics make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk Building libCEED-based Applications Below we illustrate how to use the Spack installation to build libCEED-based applications, by building the examples in the current libCEED distribution. Using spack location , the libCEED examples can be built as follows: git clone https://github.com/CEED/libCEED.git cd libCEED/examples/ceed make CEED_DIR=`spack location -i libceed` ./ex1 -ceed /cpu/self If you have multiple builds of libceed or occa you need to be more specific in the above spack location command. To list all libceed and occa versions use spack find : spack find -lv libceed occa Then either use variants to choose a unique version, e.g. libceed~cuda , or specify the hashes printed in front of the libceed spec , e.g. libceed/yb3fvek or just /yb3fvek (and similarly for occa ). The serial, OpenMP, OpenCL and GPU OCCA backends can be used with: ./ex1 -ceed /cpu/occa ./ex1 -ceed /omp/occa ./ex1 -ceed /ocl/occa ./ex1 -ceed /gpu/occa In order to use the OCCA GPU backend, one needs to install CEED with the cuda variant enabled, i.e. using the spec ceed+cuda : spack install -v ceed+cuda For more details, see the section GPU demo below. With the MAGMA backend, the /cpu/magma and /gpu/magma resource descriptors can also be used. The MFEM/libCEED and PETSc/libCEED examples can be further built with: cd examples/mfem make CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem` ./bp1 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp3 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../petsc make CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc` ./bp1 -degree 2 -ceed /cpu/self Note that if PETSC_ARCH is set in your environment, you must either unset it or also pass PETSC_ARCH= in the above command. Depending on the available backends, additional CEED resource descriptors, e.g. petsc/bp1 -degree 2 -ceed /ocl/occa or mfem/bp1 -no-vis --order 2 -ceed /gpu/occa can be provided. Finally, the Nek5000/libCEED examples can be built as follows: cd ../nek5000 export CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000` ./make-nek-examples.sh Then you can run the Nek5000 examples as follows: export MPIEXEC=`spack location -i openmpi`/bin/mpiexec ./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3 In the above example, replace openmpi with whatever the MPI implementation you have installed with spack. Also, you can do ./run-nek-example.sh -h to find out the options supported by the run script. options: -h|-help Print this usage information and exit -c|-ceed Ceed backend to be used for the run (optional, default: /cpu/self) -e|-example Example name (optional, default: bp1) -n|-np Specify number of MPI ranks for the run (optional, default: 4) -b|-box Specify the box geometry to be found in ./boxes/ directory (Mandatory) More information on running the Nek5000 examples can be found in the libCEED documentation . Alternatively, one can export the Spack install to a local directory: spack view --verbose symlink ceed/libceed libceed spack view --verbose symlink ceed/petsc petsc spack view --verbose symlink ceed/mfem mfem spack view --verbose symlink ceed/nek5000 nek5000 and use that to specify the CEED_DIR , MFEM_DIR and PETSC_DIR variables: cd libCEED/examples/ceed make CEED_DIR=../../ceed/libceed ./ex1 -ceed /cpu/self cd mfem make CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem ./bp1 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp3 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../petsc make CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc ./bp1 -degree 2 -ceed /cpu/self Using Containers Docker is a popular container system available on Linux, Mac, and Windows. After installing Docker, running one command docker run -it --rm -v `pwd`:/ceed jedbrown/ceed bash gives you a development environment with CEED installed via Spack and the host's current working directory mounted at /ceed (the current working directory in the container). For example, host$ git clone https://github.com/ceed/libceed host$ cd libceed/examples/petsc host$ docker run -it --rm -v `pwd`:/ceed jedbrown/ceed bash container$ make PETSC_DIR=`spack location -i petsc` CEED_DIR=`spack location -i libceed` container$ mpiexec -n 2 ./bp1 Global dofs: 2541 Process decomposition: 2 1 1 Local elements: 1000 = 10 10 10 Owned dofs: 1210 = 10 11 11 KSP cg CONVERGED_RTOL iterations 34 rnorm 3.992091e-09 Pointwise error (max) 1.267540e-02 See the Dockerfile to understand how this image was prepared and/or create your own images. NERSC's Shifter Containers also work at NERSC using Shifter , a container system designed for HPC. To pull the latest CEED image, use shifterimg pull docker:jedbrown/ceed:2.0 then build code using shifter commands in place of the docker commands above, e.g., host$ shifter --image=docker:jedbrown/ceed:2.0 bash container$ make PETSC_DIR=`spack location -i petsc` CEED_DIR=`spack location -i libceed` whire we see that shifter defaults behave similarly to the options we had to give manually for docker . Batch jobs can be submitted via sbatch --image docker:jedbrown/ceed:latest ... with the following in your submission script: srun -n 64 shifter ./your-petsc-app Singularity Singularity as another HPC container system with usage similar to Shifter above; consult the documentation for details. GPU demo Below is the full set of commands to install the CEED distribution on a GPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM, PETSc and Nek examples. Note that these are very different codes (C++, C, F77) which can nevertheless take advantage through libCEED of a common set of GPU kernels. The setenv commands below assume csh / tcsh . We strongly recommend to add a packages.yaml file in order to avoid long compile times, see Tips and Troubleshooting . # Install CEED 2.0 distribution via Spack git clone https://github.com/spack/spack.git cd spack spack install ceed+cuda # Setup CEED component directories setenv CEED_DIR `spack location -i libceed` setenv MFEM_DIR `spack location -i mfem` setenv PETSC_DIR `spack location -i petsc` setenv NEK5K_DIR `spack location -i nek5000` # Clean OCCA cache # rm -rf ~/.occa # Clone libCEED examples directory as proxy for libCEED-based codes git clone https://github.com/CEED/libCEED.git mv libCEED/examples ceed-examples rm -rf libCEED # libCEED examples on CPU and GPU cd ceed-examples/ceed make ./ex1 -ceed /cpu/self/ref/blocked ./ex1 -ceed /gpu/cuda/ref cd ../.. # MFEM+libCEED examples on CPU and GPU cd ceed-examples/mfem make ./bp1 -ceed /cpu/self/ref/blocked -no-vis -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp1 -ceed /gpu/cuda/ref -no-vis -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../.. # PETSc+libCEED examples on CPU and GPU cd ceed-examples/petsc make ./bp1 -ceed /cpu/self/ref/blocked ./bp1 -ceed /gpu/cuda/ref cd ../.. # Nek+libCEED examples on CPU and GPU cd ceed-examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self/ref/blocked -b 3 ./run-nek-example.sh -ceed /gpu/cuda/ref -b 3 cd ../.. Spack for Beginners Spack is a package manager for scientific software that supports multiple versions, configurations, platforms, and compilers. While Spack does not change the build system that already exists in each CEED component, it coordinates the dependencies between these components and enables them to be built with the same compilers and options. If you are new to Spack, here are some Spack commands and options that you may find useful: Spack is a set of Python scripts so there is nothing to install! Just download with git clone https://github.com/spack/spack.git and add spack/bin to your path with the following commands: . share/spack/setup-env.sh for bash / zsh or setenv SPACK_ROOT `pwd`; source $SPACK_ROOT/share/spack/setup-env.csh for csh / tcsh . Spack should automatically locate the standard compilers on your system. Use spack compilers to list the ones that have been found. If you need to configure additional compilers, you can do that through the config file, ~/.spack/compilers.yaml , or the platform-specific config file, ~/.spack/<platform>/compilers.yaml . Some examples of such files are provided below. Check the Spack documentation for additional details. Spack likes to build all of its packages. The file ~/.spack/packages.yaml , and similarly the platform-specific, ~/.spack/<platform>/packages.yaml , allow you to list the packages already installed on your system for Spack to use instead of compiling them itself. Some examples are provided below. Skip the -v option of spack install to see only a summary for the building of each package (as opposed to the compilation of individual files): spack install ceed . You can still turn the detailed build output on and off by pressing the v key in the Spack terminal. To troubleshoot the spack install process: spack --debug --verbose install ceed . To do a dry run of the spack install process: spack install --fake ceed . Note that you will have to run spack uninstall --all to clean up after this. To see the specific packages that will be installed for a particular package, e.g. ceed , use: spack spec -I ceed . To see the list of all installed packages: spack find . To list the location where all different versions of the ceed package were installed: spack find --long --paths ceed . Alternatively, for a specific version you can use spack location --install-dir ceed . To uninstall a package, e.g. mfem, including all packages that depend on it: spack uninstall --all --dependents mfem , or spack uninstall /qzn2u7t for a particular hash. To uninstall all packages that were ever installed by Spack: spack uninstall --all . In this case you may also want to clear the caches that Spack maintains with: spack clean -a . Tips and Troubleshooting Building on a Mac The file ceed2-darwin-highsierra-x86_64-packages.yaml provides a sample packages.yaml file based on Homebrew , that should work on most Macs. (You can use MacPorts instead of Homebrew if you prefer.) packages: all: compiler: [clang] providers: blas: [veclibfort] lapack: [veclibfort] mpi: [openmpi] openmpi: paths: openmpi@3.0.0: ~/brew buildable: False cmake: paths: cmake@3.10.2: ~/brew buildable: False cuda: paths: cuda@9.1.85: /usr/local/cuda buildable: False libx11: paths: libx11@system: /opt/X11 version: [system] buildable: False libxt: paths: libxt@system: /opt/X11 version: [system] buildable: False xproto: paths: # see /opt/X11/lib/pkgconfig/xproto.pc xproto@7.0.31: /opt/X11 version: [7.0.31] buildable: False python: paths: python@2.7.10: /usr buildable: False zlib: paths: zlib@1.2.11: /usr buildable: False The packages in ~/brew were installed with brew install package . If you don't have Homebrew, you can install it and the needed tools with: git clone https://github.com/Homebrew/brew.git cd brew bin/brew install openmpi cmake python zlib The packages in /usr are provided by Apple and come pre-built with Mac OS X. The cuda package is provided by NVIDIA and should be installed separately by downloading it from NVIDIA . We are using the Clang compiler, OpenMPI, and Apple's BLAS/LAPACK accelerator library. Building on a Linux Desktop The file ceed2-linux-rhel7-x86_64-packages.yaml provides a sample packages.yaml file that can be adapted to work on a RHEL7 Linux desktop packages: all: compiler: [gcc] providers: mpi: [openmpi] blas: [netlib-lapack] lapack: [netlib-lapack] netlib-lapack: paths: netlib-lapack@system: /usr/lib64 buildable: False openmpi: paths: openmpi@3.0.0: ~/local buildable: False cmake: paths: cmake@3.10.2: ~/local buildable: False cuda: paths: cuda@9.1.85: ~/local/cuda buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: xproto@7.0.32: /usr version: [7.0.32] buildable: False python: paths: python@2.7.14: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib64 buildable: False The above file uses user-installed OpenMPI, CMake and CUDA packages, with the rest of the CEED prerequisites installed via the yum package manager . A very similar file, ceed2-ubuntu18.10-packages.yaml provides Spack configuration for the Ubuntu distribution: packages: all: compiler: [gcc] providers: mpi: [mpich] blas: [openblas] lapack: [openblas] openblas: paths: openblas@system: /usr/lib buildable: False mpich: paths: mpich@3.3: /usr/local buildable: False cmake: paths: cmake@3.12.1: /usr buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # See /usr/share/pkgconfig/xproto.pc for version xproto@7.0.32: /usr buildable: False python: paths: python@3.6.7: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib buildable: False In this case we use GCC and other development packages via apt install and with MPICH installed separately (as needed to use containerized HPC environments like Shifter and Singularity). You can use docker pull jedbrown/ceed-base to get a build environment that is ready for spack install ceed . Building at LLNL's Computing Center Pascal (TOSS3 Platforms) The file ceed2-pascal-packages.yaml is an example of a packages.yaml file for the TOSS3 system type at LLNL's Livermore Computing center. packages: cmake: paths: cmake@3.13.4: /usr/tce/packages/cmake/cmake-3.13.4 version: [3.13.4] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 version: [2.7.14] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # see /usr/share/pkgconfig/xproto.pc xproto@7.0.32: /usr version: [7.0.32] buildable: False mvapich2: paths: mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1 mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3 intel-mkl: paths: intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0 version: [2018.0.128] buildable: False cuda: paths: cuda@10.0.130: /usr/tce/packages/cuda/cuda-10.0.130 version: [10.0.130] buildable: False all: compiler: [intel, gcc] providers: mpi: [mvapich2] blas: [intel-mkl, openblas] lapack: [intel-mkl, openblas] The above file can be used to build CEED with different compilers (Intel being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for the TOSS3 platform can be found here: ceed2-pascal-compilers.yaml . Lassen The file ceed2-lassen-packages.yaml is an example of a packages.yaml file for the Lassen system at LLNL's Livermore Computing center, which is similar to the Sierra supercomputer. packages: all: compiler: [xl_r, xl, gcc] providers: mpi: [spectrum-mpi] blas: [essl] lapack: [netlib-lapack] essl: paths: essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0 variants: threads=none version: [6.1.0] buildable: False veclibfort: buildable: False intel-parallel-studio: buildable: False intel-mkl: buildable: False atlas: buildable: False openblas: # OpenBLAS can be built only with gcc buildable: False netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant variants: +external-blas~lapacke spectrum-mpi: paths: spectrum-mpi@2019-01-30%xl_r@16.1.1: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-xl-2019.02.07 spectrum-mpi@2019-01-30%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-gcc-4.9.3 spectrum-mpi@2019-01-30%gcc@7.3.1: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-gcc-7.3.1 buildable: False cmake: paths: cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2 version: [3.9.2] buildable: False cuda: paths: cuda@9.2.148: /usr/tce/packages/cuda/cuda-9.2.148 version: [9.2.148] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # see /usr/share/pkgconfig/xproto.pc xproto@7.0.31: /usr version: [7.0.31] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 version: [2.7.14] buildable: False The above file can be used to build CEED with different compilers (xl being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for Lassen can be found here: ceed2-lassen-compilers.yaml . Building at NERSC Cori The file ceed2-cori-packages.yaml is an example of a packages.yaml file for the Cori system at NERSC . packages: all: compiler: [gcc@7.3.0, intel@18.0.5.274] providers: mpi: [mpich] mkl: [intel-mkl] blas: [intel-mkl, cray-libsci] scalapack: [intel-mkl, cray-libsci] pkgconfig: [pkg-config] mpich: modules: mpich@3.2%gcc@7.3.0 arch=cray-cnl9-haswell: cray-mpich mpich@3.2%intel@18.0.5.274 arch=cray-cnl9-haswell: cray-mpich buildable: False intel-mkl: buildable: false paths: intel-mkl@2018.3.222%intel: /opt/intel intel-mkl@2018.3.222%gcc: /opt/intel pkg-config: buildable: false paths: pkg-config@0.28: /usr cmake: modules: cmake@3.14.0%gcc@7.3.0 arch=cray-cnl9-haswell: cmake cmake@3.14.0%intel@18.0.5.274 arch=cray-cnl9-haswell: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # See /usr/lib64/pkgconfig/xproto.pc for version xproto@7.0.28: /usr buildable: False python: paths: python@2.7.13: /usr buildable: False boost: modules: boost@1.69.0%gcc@7.3.0 arch=cray-cnl9-haswell: boost boost@1.69.0%intel@18.0.5.274 arch=cray-cnl9-haswell: boost buildable: False m4: modules: m4@1.4.17%gcc@7.3.0 arch=cray-cnl9-haswell: m4 m4@1.4.17%intel@18.0.5.274 arch=cray-cnl9-haswell: m4 buildable: False openssl: modules: openssl@1.1.0a%gcc@7.3.0 arch=cray-cnl9-haswell: openssl openssl@1.1.0a%intel@18.0.5.274 arch=cray-cnl9-haswell: openssl buildable: False perl: paths: perl@5.18.2%gcc@7.3.0 arch=cray-cnl9-haswell: /usr perl@5.18.2%intel@18.0.5.274 arch=cray-cnl9-haswell: /usr buildable: False autoconf: modules: autoconf@2.69%gcc@7.3.0 arch=cray-cnl9-haswell: autoconf autoconf@2.69%intel@18.0.5.274 arch=cray-cnl9-haswell: autoconf buildable: False automake: modules: automake@1.15%gcc@7.3.0 arch=cray-cnl9-haswell: automake automake@1.15%intel@18.0.5.274 arch=cray-cnl9-haswell: automake buildable: False Building at ALCF Theta The file ceed2-theta-packages.yaml is an example of a packages.yaml file for the Theta system at ALCF . Note: You have to unload the xalt module on Theta with module unload xalt . Otherwise suite-sparse fails to build. packages: cmake: paths: cmake@3.5.2%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False python: paths: python@2.7.13%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False pkg-config: paths: pkg-config@0.28%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr pkg-config@0.28%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False autoconf: paths: autoconf@2.69%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr autoconf@2.69%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False automake: paths: automake@1.13.4%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr automake@1.13.4%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False libtool: paths: libtool@2.4.2%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr libtool@2.4.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False m4: paths: m4@1.4.16%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr m4@1.4.16%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False intel-mkl: paths: intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel buildable: False mpich: modules: # requires 'module load cce' otherwise gives parsing error mpich@7.6.3%gcc@8.2.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 buildable: False boost: paths: boost@1.64.0%gcc@8.2.0 arch=cray-CNL-mic_knl: /soft/libraries/boost/1.64.0/gnu boost@1.64.0%intel@16.0.3.210 arch=cray-CNL-mic_knl: /soft/libraries/boost/1.64.0/intel buildable: False all: providers: mpi: [mpich] compiler: [gcc@8.2.0] Building at OLCF Summit The file ceed2-summit-packages.yaml is an example of a packages.yaml file for the Summit system at OLCF . The packages.yaml file gives updated locations for spectrum-mpi and cuda. Then one has to make sure other modules like xalt are not loaded because xalt provides a conflicting version of \"ld\" which breaks the build. One may have to first install gcc version 6.5.0 and configure it in spack as a compiler. Then one can compile ceed and dependencies using netlib-lapack as the blas and lapack provider. Here are the commands to do the full compile: git clone https://github.com/spack/spack source spack/share/spack/setup-env.sh cp packages.yaml spack/etc/spack/ module purge spack install gcc@6.5.0 %gcc spack compiler find spack install ceed %gcc ^netlib-lapack Installing CUDA Several CEED packages depend on CUDA: OCCA, MAGMA and libCEED. To build these, add the cuda variant to the Spack build: sh spack install ceed+cuda You will need to have the NVIDIA CUDA SDK and driver installed on your system, see developer.nvidia.com , and specify it in the packages.yaml file. See, for example, the cuda section in Building on a Mac , or the ceed2-linux-rhel7-x86_64-packages.yaml file.","title":"CEED 2.0"},{"location":"ceed-2.0/#ceed-20-software-distribution","text":"The CEED distribution is a collection of software packages that can be integrated together to enable efficient discretizations in a variety of high-order applications on unstructured grids. CEED is using the Spack package manager for compatible building and installation of its software components. In this version, CEED 2.0, the CEED software suite consists of the following 12 packages, plus the CEED meta-package : GSLIB -1.0.2 HPGMG -0.4 Laghos -2.0 libCEED -0.4 MAGMA -2.5.0 MFEM -3.4 Nek5000 -17.0 Nekbone -17.0 NekCEM -7332619 PETSc -3.11.1 PUMI -2.2.0 OCCA -1.0.8 If you are interested in the previous release, see the CEED-1.0 page. First-time users should read Simple Installation and Using the Installation below. (Quick summary: you can build and install all of the above packages with: spack install ceed ) If you are familiar with Spack, consider using the following machine-specific configurations for CEED (see also the spack-configs repository and the xSDK's config files ). Platform Architecture Spack Configuration Mac darwin-highsierra-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Linux (Ubuntu) ubuntu18.10-x86_64 packages Cori (NERSC) cray-cnl9-haswell packages Theta (ALCF) cray-CNL-mic_knl packages Pascal (LLNL) toss_3_x86_64_ib packages compilers Lassen (LLNL) linux-rhel7-ppc64le packages compilers Summit (ORNL) linux-rhel7-ppc64le packages For additional details, please consult the following sections: Simple Installation Using the Installation Building MFEM-based Applications Building libCEED-based Applications Using Containers NERSC's Shifter Singularity GPU demo Spack for Beginners Tips and Troubleshooting Building on Mac Building on Linux Building at LLNL Pascal (TOSS3) Lassen Building at NERSC Cori Building at ALCF Theta Installing CUDA The CEED team can be contacted by posting to our User Forum or via email at ceed-users@llnl.gov . For issues related to the CEED Spack packages, please start a discussion on the GitHub @spack/ceed page.","title":"CEED 2.0 Software Distribution"},{"location":"ceed-2.0/#simple-installation","text":"If Spack is already available on your system and is visible in your PATH , you can install the CEED software simply with: spack install -v ceed To enable package testing during the build process, use instead: spack install -v --test=all ceed If you don't have Spack, you can download it and install CEED with the following commands: git clone https://github.com/spack/spack.git cd spack ./bin/spack install -v ceed To avoid long compile times, we strongly recommend that you add a packages.yaml file for your platform, see above and the Tips and Troubleshooting section.","title":"Simple Installation"},{"location":"ceed-2.0/#using-the-installation","text":"Spack will install the CEED packages (and the libraries they depend on) in a subtree of ./opt/spack/<architecture>/<compiler>/ that is specific to the architecture and compiler used (multiple compiler and/or architecture builds can coexist in a single Spack directory). Below are several examples of how the Spack installation can be linked with and used in user applications.","title":"Using the Installation"},{"location":"ceed-2.0/#building-mfem-based-applications","text":"The simplest way to use the Spack installation is through the spack location command. For example, MFEM-based codes, such as the MFEM examples, can be simply built as follows: git clone https://github.com/mfem/mfem.git cd mfem; git checkout v3.4 cd examples make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk cd ../miniapps/electromagnetics make CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk Alternatively, the Spack installation can be exported to a local directory: mkdir ceed spack view --verbose symlink ceed/mfem mfem The ceed/mfem directory now contains the Spack-built MFEM with all of its dependencies (technically, it contains links to all the build files inside the ./opt/spack/ subdirectory for MFEM). In particular, the MFEM library in ceed/mfem/lib and the MFEM build configuration file in ceed/mfem/share/mfem/config.mk . This directory can be used to build the MFEM examples as follows: git clone https://github.com/mfem/mfem.git cd mfem; git checkout v3.4 cd examples/petsc make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk cd .. make CONFIG_MK=../../ceed/mfem/share/mfem/config.mk The MFEM miniapps can further be built with: cd ../miniapps/electromagnetics make CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk","title":"Building MFEM-based Applications"},{"location":"ceed-2.0/#building-libceed-based-applications","text":"Below we illustrate how to use the Spack installation to build libCEED-based applications, by building the examples in the current libCEED distribution. Using spack location , the libCEED examples can be built as follows: git clone https://github.com/CEED/libCEED.git cd libCEED/examples/ceed make CEED_DIR=`spack location -i libceed` ./ex1 -ceed /cpu/self If you have multiple builds of libceed or occa you need to be more specific in the above spack location command. To list all libceed and occa versions use spack find : spack find -lv libceed occa Then either use variants to choose a unique version, e.g. libceed~cuda , or specify the hashes printed in front of the libceed spec , e.g. libceed/yb3fvek or just /yb3fvek (and similarly for occa ). The serial, OpenMP, OpenCL and GPU OCCA backends can be used with: ./ex1 -ceed /cpu/occa ./ex1 -ceed /omp/occa ./ex1 -ceed /ocl/occa ./ex1 -ceed /gpu/occa In order to use the OCCA GPU backend, one needs to install CEED with the cuda variant enabled, i.e. using the spec ceed+cuda : spack install -v ceed+cuda For more details, see the section GPU demo below. With the MAGMA backend, the /cpu/magma and /gpu/magma resource descriptors can also be used. The MFEM/libCEED and PETSc/libCEED examples can be further built with: cd examples/mfem make CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem` ./bp1 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp3 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../petsc make CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc` ./bp1 -degree 2 -ceed /cpu/self Note that if PETSC_ARCH is set in your environment, you must either unset it or also pass PETSC_ARCH= in the above command. Depending on the available backends, additional CEED resource descriptors, e.g. petsc/bp1 -degree 2 -ceed /ocl/occa or mfem/bp1 -no-vis --order 2 -ceed /gpu/occa can be provided. Finally, the Nek5000/libCEED examples can be built as follows: cd ../nek5000 export CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000` ./make-nek-examples.sh Then you can run the Nek5000 examples as follows: export MPIEXEC=`spack location -i openmpi`/bin/mpiexec ./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3 In the above example, replace openmpi with whatever the MPI implementation you have installed with spack. Also, you can do ./run-nek-example.sh -h to find out the options supported by the run script. options: -h|-help Print this usage information and exit -c|-ceed Ceed backend to be used for the run (optional, default: /cpu/self) -e|-example Example name (optional, default: bp1) -n|-np Specify number of MPI ranks for the run (optional, default: 4) -b|-box Specify the box geometry to be found in ./boxes/ directory (Mandatory) More information on running the Nek5000 examples can be found in the libCEED documentation . Alternatively, one can export the Spack install to a local directory: spack view --verbose symlink ceed/libceed libceed spack view --verbose symlink ceed/petsc petsc spack view --verbose symlink ceed/mfem mfem spack view --verbose symlink ceed/nek5000 nek5000 and use that to specify the CEED_DIR , MFEM_DIR and PETSC_DIR variables: cd libCEED/examples/ceed make CEED_DIR=../../ceed/libceed ./ex1 -ceed /cpu/self cd mfem make CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem ./bp1 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp3 -no-vis -o 2 -ceed /cpu/self -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../petsc make CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc ./bp1 -degree 2 -ceed /cpu/self","title":"Building libCEED-based Applications"},{"location":"ceed-2.0/#using-containers","text":"Docker is a popular container system available on Linux, Mac, and Windows. After installing Docker, running one command docker run -it --rm -v `pwd`:/ceed jedbrown/ceed bash gives you a development environment with CEED installed via Spack and the host's current working directory mounted at /ceed (the current working directory in the container). For example, host$ git clone https://github.com/ceed/libceed host$ cd libceed/examples/petsc host$ docker run -it --rm -v `pwd`:/ceed jedbrown/ceed bash container$ make PETSC_DIR=`spack location -i petsc` CEED_DIR=`spack location -i libceed` container$ mpiexec -n 2 ./bp1 Global dofs: 2541 Process decomposition: 2 1 1 Local elements: 1000 = 10 10 10 Owned dofs: 1210 = 10 11 11 KSP cg CONVERGED_RTOL iterations 34 rnorm 3.992091e-09 Pointwise error (max) 1.267540e-02 See the Dockerfile to understand how this image was prepared and/or create your own images.","title":"Using Containers"},{"location":"ceed-2.0/#nerscs-shifter","text":"Containers also work at NERSC using Shifter , a container system designed for HPC. To pull the latest CEED image, use shifterimg pull docker:jedbrown/ceed:2.0 then build code using shifter commands in place of the docker commands above, e.g., host$ shifter --image=docker:jedbrown/ceed:2.0 bash container$ make PETSC_DIR=`spack location -i petsc` CEED_DIR=`spack location -i libceed` whire we see that shifter defaults behave similarly to the options we had to give manually for docker . Batch jobs can be submitted via sbatch --image docker:jedbrown/ceed:latest ... with the following in your submission script: srun -n 64 shifter ./your-petsc-app","title":"NERSC's Shifter"},{"location":"ceed-2.0/#singularity","text":"Singularity as another HPC container system with usage similar to Shifter above; consult the documentation for details.","title":"Singularity"},{"location":"ceed-2.0/#gpu-demo","text":"Below is the full set of commands to install the CEED distribution on a GPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM, PETSc and Nek examples. Note that these are very different codes (C++, C, F77) which can nevertheless take advantage through libCEED of a common set of GPU kernels. The setenv commands below assume csh / tcsh . We strongly recommend to add a packages.yaml file in order to avoid long compile times, see Tips and Troubleshooting . # Install CEED 2.0 distribution via Spack git clone https://github.com/spack/spack.git cd spack spack install ceed+cuda # Setup CEED component directories setenv CEED_DIR `spack location -i libceed` setenv MFEM_DIR `spack location -i mfem` setenv PETSC_DIR `spack location -i petsc` setenv NEK5K_DIR `spack location -i nek5000` # Clean OCCA cache # rm -rf ~/.occa # Clone libCEED examples directory as proxy for libCEED-based codes git clone https://github.com/CEED/libCEED.git mv libCEED/examples ceed-examples rm -rf libCEED # libCEED examples on CPU and GPU cd ceed-examples/ceed make ./ex1 -ceed /cpu/self/ref/blocked ./ex1 -ceed /gpu/cuda/ref cd ../.. # MFEM+libCEED examples on CPU and GPU cd ceed-examples/mfem make ./bp1 -ceed /cpu/self/ref/blocked -no-vis -m `spack location -i mfem`/share/mfem/data/star.mesh ./bp1 -ceed /gpu/cuda/ref -no-vis -m `spack location -i mfem`/share/mfem/data/star.mesh cd ../.. # PETSc+libCEED examples on CPU and GPU cd ceed-examples/petsc make ./bp1 -ceed /cpu/self/ref/blocked ./bp1 -ceed /gpu/cuda/ref cd ../.. # Nek+libCEED examples on CPU and GPU cd ceed-examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self/ref/blocked -b 3 ./run-nek-example.sh -ceed /gpu/cuda/ref -b 3 cd ../..","title":"GPU demo"},{"location":"ceed-2.0/#spack-for-beginners","text":"Spack is a package manager for scientific software that supports multiple versions, configurations, platforms, and compilers. While Spack does not change the build system that already exists in each CEED component, it coordinates the dependencies between these components and enables them to be built with the same compilers and options. If you are new to Spack, here are some Spack commands and options that you may find useful: Spack is a set of Python scripts so there is nothing to install! Just download with git clone https://github.com/spack/spack.git and add spack/bin to your path with the following commands: . share/spack/setup-env.sh for bash / zsh or setenv SPACK_ROOT `pwd`; source $SPACK_ROOT/share/spack/setup-env.csh for csh / tcsh . Spack should automatically locate the standard compilers on your system. Use spack compilers to list the ones that have been found. If you need to configure additional compilers, you can do that through the config file, ~/.spack/compilers.yaml , or the platform-specific config file, ~/.spack/<platform>/compilers.yaml . Some examples of such files are provided below. Check the Spack documentation for additional details. Spack likes to build all of its packages. The file ~/.spack/packages.yaml , and similarly the platform-specific, ~/.spack/<platform>/packages.yaml , allow you to list the packages already installed on your system for Spack to use instead of compiling them itself. Some examples are provided below. Skip the -v option of spack install to see only a summary for the building of each package (as opposed to the compilation of individual files): spack install ceed . You can still turn the detailed build output on and off by pressing the v key in the Spack terminal. To troubleshoot the spack install process: spack --debug --verbose install ceed . To do a dry run of the spack install process: spack install --fake ceed . Note that you will have to run spack uninstall --all to clean up after this. To see the specific packages that will be installed for a particular package, e.g. ceed , use: spack spec -I ceed . To see the list of all installed packages: spack find . To list the location where all different versions of the ceed package were installed: spack find --long --paths ceed . Alternatively, for a specific version you can use spack location --install-dir ceed . To uninstall a package, e.g. mfem, including all packages that depend on it: spack uninstall --all --dependents mfem , or spack uninstall /qzn2u7t for a particular hash. To uninstall all packages that were ever installed by Spack: spack uninstall --all . In this case you may also want to clear the caches that Spack maintains with: spack clean -a .","title":"Spack for Beginners"},{"location":"ceed-2.0/#tips-and-troubleshooting","text":"","title":"Tips and Troubleshooting"},{"location":"ceed-2.0/#building-on-a-mac","text":"The file ceed2-darwin-highsierra-x86_64-packages.yaml provides a sample packages.yaml file based on Homebrew , that should work on most Macs. (You can use MacPorts instead of Homebrew if you prefer.) packages: all: compiler: [clang] providers: blas: [veclibfort] lapack: [veclibfort] mpi: [openmpi] openmpi: paths: openmpi@3.0.0: ~/brew buildable: False cmake: paths: cmake@3.10.2: ~/brew buildable: False cuda: paths: cuda@9.1.85: /usr/local/cuda buildable: False libx11: paths: libx11@system: /opt/X11 version: [system] buildable: False libxt: paths: libxt@system: /opt/X11 version: [system] buildable: False xproto: paths: # see /opt/X11/lib/pkgconfig/xproto.pc xproto@7.0.31: /opt/X11 version: [7.0.31] buildable: False python: paths: python@2.7.10: /usr buildable: False zlib: paths: zlib@1.2.11: /usr buildable: False The packages in ~/brew were installed with brew install package . If you don't have Homebrew, you can install it and the needed tools with: git clone https://github.com/Homebrew/brew.git cd brew bin/brew install openmpi cmake python zlib The packages in /usr are provided by Apple and come pre-built with Mac OS X. The cuda package is provided by NVIDIA and should be installed separately by downloading it from NVIDIA . We are using the Clang compiler, OpenMPI, and Apple's BLAS/LAPACK accelerator library.","title":"Building on a Mac"},{"location":"ceed-2.0/#building-on-a-linux-desktop","text":"The file ceed2-linux-rhel7-x86_64-packages.yaml provides a sample packages.yaml file that can be adapted to work on a RHEL7 Linux desktop packages: all: compiler: [gcc] providers: mpi: [openmpi] blas: [netlib-lapack] lapack: [netlib-lapack] netlib-lapack: paths: netlib-lapack@system: /usr/lib64 buildable: False openmpi: paths: openmpi@3.0.0: ~/local buildable: False cmake: paths: cmake@3.10.2: ~/local buildable: False cuda: paths: cuda@9.1.85: ~/local/cuda buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: xproto@7.0.32: /usr version: [7.0.32] buildable: False python: paths: python@2.7.14: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib64 buildable: False The above file uses user-installed OpenMPI, CMake and CUDA packages, with the rest of the CEED prerequisites installed via the yum package manager . A very similar file, ceed2-ubuntu18.10-packages.yaml provides Spack configuration for the Ubuntu distribution: packages: all: compiler: [gcc] providers: mpi: [mpich] blas: [openblas] lapack: [openblas] openblas: paths: openblas@system: /usr/lib buildable: False mpich: paths: mpich@3.3: /usr/local buildable: False cmake: paths: cmake@3.12.1: /usr buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # See /usr/share/pkgconfig/xproto.pc for version xproto@7.0.32: /usr buildable: False python: paths: python@3.6.7: /usr buildable: False zlib: paths: zlib@1.2.11: /usr/lib buildable: False In this case we use GCC and other development packages via apt install and with MPICH installed separately (as needed to use containerized HPC environments like Shifter and Singularity). You can use docker pull jedbrown/ceed-base to get a build environment that is ready for spack install ceed .","title":"Building on a Linux Desktop"},{"location":"ceed-2.0/#building-at-llnls-computing-center","text":"","title":"Building at LLNL's Computing Center"},{"location":"ceed-2.0/#pascal-toss3-platforms","text":"The file ceed2-pascal-packages.yaml is an example of a packages.yaml file for the TOSS3 system type at LLNL's Livermore Computing center. packages: cmake: paths: cmake@3.13.4: /usr/tce/packages/cmake/cmake-3.13.4 version: [3.13.4] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 version: [2.7.14] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # see /usr/share/pkgconfig/xproto.pc xproto@7.0.32: /usr version: [7.0.32] buildable: False mvapich2: paths: mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1 mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3 intel-mkl: paths: intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0 version: [2018.0.128] buildable: False cuda: paths: cuda@10.0.130: /usr/tce/packages/cuda/cuda-10.0.130 version: [10.0.130] buildable: False all: compiler: [intel, gcc] providers: mpi: [mvapich2] blas: [intel-mkl, openblas] lapack: [intel-mkl, openblas] The above file can be used to build CEED with different compilers (Intel being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for the TOSS3 platform can be found here: ceed2-pascal-compilers.yaml .","title":"Pascal (TOSS3 Platforms)"},{"location":"ceed-2.0/#lassen","text":"The file ceed2-lassen-packages.yaml is an example of a packages.yaml file for the Lassen system at LLNL's Livermore Computing center, which is similar to the Sierra supercomputer. packages: all: compiler: [xl_r, xl, gcc] providers: mpi: [spectrum-mpi] blas: [essl] lapack: [netlib-lapack] essl: paths: essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0 variants: threads=none version: [6.1.0] buildable: False veclibfort: buildable: False intel-parallel-studio: buildable: False intel-mkl: buildable: False atlas: buildable: False openblas: # OpenBLAS can be built only with gcc buildable: False netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant variants: +external-blas~lapacke spectrum-mpi: paths: spectrum-mpi@2019-01-30%xl_r@16.1.1: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-xl-2019.02.07 spectrum-mpi@2019-01-30%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-gcc-4.9.3 spectrum-mpi@2019-01-30%gcc@7.3.1: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2019.01.30-gcc-7.3.1 buildable: False cmake: paths: cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2 version: [3.9.2] buildable: False cuda: paths: cuda@9.2.148: /usr/tce/packages/cuda/cuda-9.2.148 version: [9.2.148] buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # see /usr/share/pkgconfig/xproto.pc xproto@7.0.31: /usr version: [7.0.31] buildable: False python: paths: python@2.7.14: /usr/tce/packages/python/python-2.7.14 version: [2.7.14] buildable: False The above file can be used to build CEED with different compilers (xl being the default), for example: spack install ceed%gcc~petsc A corresponding compilers.yaml file for Lassen can be found here: ceed2-lassen-compilers.yaml .","title":"Lassen"},{"location":"ceed-2.0/#building-at-nersc","text":"","title":"Building at NERSC"},{"location":"ceed-2.0/#cori","text":"The file ceed2-cori-packages.yaml is an example of a packages.yaml file for the Cori system at NERSC . packages: all: compiler: [gcc@7.3.0, intel@18.0.5.274] providers: mpi: [mpich] mkl: [intel-mkl] blas: [intel-mkl, cray-libsci] scalapack: [intel-mkl, cray-libsci] pkgconfig: [pkg-config] mpich: modules: mpich@3.2%gcc@7.3.0 arch=cray-cnl9-haswell: cray-mpich mpich@3.2%intel@18.0.5.274 arch=cray-cnl9-haswell: cray-mpich buildable: False intel-mkl: buildable: false paths: intel-mkl@2018.3.222%intel: /opt/intel intel-mkl@2018.3.222%gcc: /opt/intel pkg-config: buildable: false paths: pkg-config@0.28: /usr cmake: modules: cmake@3.14.0%gcc@7.3.0 arch=cray-cnl9-haswell: cmake cmake@3.14.0%intel@18.0.5.274 arch=cray-cnl9-haswell: cmake buildable: False libx11: paths: libx11@system: /usr version: [system] buildable: False libxt: paths: libxt@system: /usr version: [system] buildable: False xproto: paths: # See /usr/lib64/pkgconfig/xproto.pc for version xproto@7.0.28: /usr buildable: False python: paths: python@2.7.13: /usr buildable: False boost: modules: boost@1.69.0%gcc@7.3.0 arch=cray-cnl9-haswell: boost boost@1.69.0%intel@18.0.5.274 arch=cray-cnl9-haswell: boost buildable: False m4: modules: m4@1.4.17%gcc@7.3.0 arch=cray-cnl9-haswell: m4 m4@1.4.17%intel@18.0.5.274 arch=cray-cnl9-haswell: m4 buildable: False openssl: modules: openssl@1.1.0a%gcc@7.3.0 arch=cray-cnl9-haswell: openssl openssl@1.1.0a%intel@18.0.5.274 arch=cray-cnl9-haswell: openssl buildable: False perl: paths: perl@5.18.2%gcc@7.3.0 arch=cray-cnl9-haswell: /usr perl@5.18.2%intel@18.0.5.274 arch=cray-cnl9-haswell: /usr buildable: False autoconf: modules: autoconf@2.69%gcc@7.3.0 arch=cray-cnl9-haswell: autoconf autoconf@2.69%intel@18.0.5.274 arch=cray-cnl9-haswell: autoconf buildable: False automake: modules: automake@1.15%gcc@7.3.0 arch=cray-cnl9-haswell: automake automake@1.15%intel@18.0.5.274 arch=cray-cnl9-haswell: automake buildable: False","title":"Cori"},{"location":"ceed-2.0/#building-at-alcf","text":"","title":"Building at ALCF"},{"location":"ceed-2.0/#theta","text":"The file ceed2-theta-packages.yaml is an example of a packages.yaml file for the Theta system at ALCF . Note: You have to unload the xalt module on Theta with module unload xalt . Otherwise suite-sparse fails to build. packages: cmake: paths: cmake@3.5.2%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False python: paths: python@2.7.13%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False pkg-config: paths: pkg-config@0.28%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr pkg-config@0.28%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False autoconf: paths: autoconf@2.69%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr autoconf@2.69%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False automake: paths: automake@1.13.4%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr automake@1.13.4%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False libtool: paths: libtool@2.4.2%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr libtool@2.4.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False m4: paths: m4@1.4.16%gcc@8.2.0 arch=cray-CNL-mic_knl: /usr m4@1.4.16%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr buildable: False intel-mkl: paths: intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel buildable: False mpich: modules: # requires 'module load cce' otherwise gives parsing error mpich@7.6.3%gcc@8.2.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3 buildable: False boost: paths: boost@1.64.0%gcc@8.2.0 arch=cray-CNL-mic_knl: /soft/libraries/boost/1.64.0/gnu boost@1.64.0%intel@16.0.3.210 arch=cray-CNL-mic_knl: /soft/libraries/boost/1.64.0/intel buildable: False all: providers: mpi: [mpich] compiler: [gcc@8.2.0]","title":"Theta"},{"location":"ceed-2.0/#building-at-olcf","text":"","title":"Building at OLCF"},{"location":"ceed-2.0/#summit","text":"The file ceed2-summit-packages.yaml is an example of a packages.yaml file for the Summit system at OLCF . The packages.yaml file gives updated locations for spectrum-mpi and cuda. Then one has to make sure other modules like xalt are not loaded because xalt provides a conflicting version of \"ld\" which breaks the build. One may have to first install gcc version 6.5.0 and configure it in spack as a compiler. Then one can compile ceed and dependencies using netlib-lapack as the blas and lapack provider. Here are the commands to do the full compile: git clone https://github.com/spack/spack source spack/share/spack/setup-env.sh cp packages.yaml spack/etc/spack/ module purge spack install gcc@6.5.0 %gcc spack compiler find spack install ceed %gcc ^netlib-lapack","title":"Summit"},{"location":"ceed-2.0/#installing-cuda","text":"Several CEED packages depend on CUDA: OCCA, MAGMA and libCEED. To build these, add the cuda variant to the Spack build: sh spack install ceed+cuda You will need to have the NVIDIA CUDA SDK and driver installed on your system, see developer.nvidia.com , and specify it in the packages.yaml file. See, for example, the cuda section in Building on a Mac , or the ceed2-linux-rhel7-x86_64-packages.yaml file.","title":"Installing CUDA"},{"location":"ceed-code/","text":"CEED APIs CEED is building on the efforts of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects to develop application program interfaces (APIs), both at high-level and at low-level. Multiple APIs are necessary to enable application to take advantage of CEED-developed high-order technologies at the level they are comfortable with. In addition, our high-level API will call internally low-level API functionality. Low level API The CEED low-level API, libCEED operates with the foundational components of finite element operators, described by the following decomposition: To achieve high-performance, it is critical to take advantage of the tensor-product structure of both the finite element basis and the quadrature rule to efficiently apply the action of $B$ without necessarily computing its entries. This is generally know as sum factorization . In the case where we precompute and store the $D$ matrix, we call the algorithm partial assembly . Since libCEED is based on a common operator description at algebraic level, it can also be used as the foundation for an efficient high-order operator format . For more information of the CEED low level API see the libCEED page. High level API The CEED high-level API operates with global discretization concepts, specifying a global mesh, finite element spaces and PDE operators to be discretized with the point-wise physics representing the coefficients in these operators. Given such inputs, CEED provides efficient discretization and evaluation of the requested operators, without the need for the application to be concerned with element-level operations. Internally, the high-level API relies on CEED's low-level API described below. The global perspective also allows CEED to provide general unstructured adaptive mesh refinement support, with minimal impact in the application code. This API is currently under development. Stay tuned for more details... MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"CEED APIs"},{"location":"ceed-code/#ceed-apis","text":"CEED is building on the efforts of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects to develop application program interfaces (APIs), both at high-level and at low-level. Multiple APIs are necessary to enable application to take advantage of CEED-developed high-order technologies at the level they are comfortable with. In addition, our high-level API will call internally low-level API functionality.","title":"CEED APIs"},{"location":"ceed-code/#low-level-api","text":"The CEED low-level API, libCEED operates with the foundational components of finite element operators, described by the following decomposition: To achieve high-performance, it is critical to take advantage of the tensor-product structure of both the finite element basis and the quadrature rule to efficiently apply the action of $B$ without necessarily computing its entries. This is generally know as sum factorization . In the case where we precompute and store the $D$ matrix, we call the algorithm partial assembly . Since libCEED is based on a common operator description at algebraic level, it can also be used as the foundation for an efficient high-order operator format . For more information of the CEED low level API see the libCEED page.","title":"Low level API"},{"location":"ceed-code/#high-level-api","text":"The CEED high-level API operates with global discretization concepts, specifying a global mesh, finite element spaces and PDE operators to be discretized with the point-wise physics representing the coefficients in these operators. Given such inputs, CEED provides efficient discretization and evaluation of the requested operators, without the need for the application to be concerned with element-level operations. Internally, the high-level API relies on CEED's low-level API described below. The global perspective also allows CEED to provide general unstructured adaptive mesh refinement support, with minimal impact in the application code. This API is currently under development. Stay tuned for more details... MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"High level API"},{"location":"ceed1am/","text":"CEED First Annual Meeting August 15-17, 2017 Lawrence Livermore National Laboratory Overview CEED will hold its first annual meeting in August 15-17, 2017 at the HPC Innovation Center of Lawrence Livermore National Laboratory in Livermore, California. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects. Meeting Agenda Latest agenda: Go to the CVENT website Login with: CEED2017 Click on the Agenda tab Catering and Dinner Breakfast and Lunch will be included with a separate fee. Dinner on 8/15 will be provided with that fee as well. Lodging Options There are many hotels in Livermore. A few from the about 10 hotels in Livermore are below. Other hotels are available in Pleasanton and other nearby cities. If you stay outside of Livermore we recommend staying west of the city to have a reverse commute to the lab. Best Western - 2.5 miles from LLNL Double Tree - 5 miles from LLNL Courtyard by Marriot - 9 miles from LLNL Holiday Inn Express - 9 miles from LLNL About Livermore and LLNL Founded in 1869, Livermore is California\u2019s oldest wine region, framed by award-winning wineries, farm lands and ranches that mirror the valley\u2019s western heritage. As home to renowned science and technology centers, Lawrence Livermore National Laboratory and Sandia National Laboratory, Livermore is a technological hub and an academically engaged community. It has become an integral part of the Bay Area, successfully competing in the global market powered by its wealth of research, technology and innovation. For more than 60 years, the Lawrence Livermore National Laboratory has applied science and technology to make the world a safer place. World class facilities include the National Ignition Facility, and the Terascale facility hosting Sequoia, currently the world's fifth fastest supercomputer. Questions? Please contact the meeting organizers at: llnl-ceed-meeting@listserv.llnl.gov .","title":"_CEED1AM"},{"location":"ceed1am/#ceed-first-annual-meeting","text":"","title":"CEED First Annual Meeting"},{"location":"ceed1am/#august-15-17-2017","text":"","title":"August 15-17, 2017"},{"location":"ceed1am/#lawrence-livermore-national-laboratory","text":"","title":"Lawrence Livermore National Laboratory"},{"location":"ceed1am/#overview","text":"CEED will hold its first annual meeting in August 15-17, 2017 at the HPC Innovation Center of Lawrence Livermore National Laboratory in Livermore, California. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.","title":"Overview"},{"location":"ceed1am/#meeting-agenda","text":"Latest agenda: Go to the CVENT website Login with: CEED2017 Click on the Agenda tab","title":"Meeting Agenda"},{"location":"ceed1am/#catering-and-dinner","text":"Breakfast and Lunch will be included with a separate fee. Dinner on 8/15 will be provided with that fee as well.","title":"Catering and Dinner"},{"location":"ceed1am/#lodging-options","text":"There are many hotels in Livermore. A few from the about 10 hotels in Livermore are below. Other hotels are available in Pleasanton and other nearby cities. If you stay outside of Livermore we recommend staying west of the city to have a reverse commute to the lab. Best Western - 2.5 miles from LLNL Double Tree - 5 miles from LLNL Courtyard by Marriot - 9 miles from LLNL Holiday Inn Express - 9 miles from LLNL","title":"Lodging Options"},{"location":"ceed1am/#about-livermore-and-llnl","text":"Founded in 1869, Livermore is California\u2019s oldest wine region, framed by award-winning wineries, farm lands and ranches that mirror the valley\u2019s western heritage. As home to renowned science and technology centers, Lawrence Livermore National Laboratory and Sandia National Laboratory, Livermore is a technological hub and an academically engaged community. It has become an integral part of the Bay Area, successfully competing in the global market powered by its wealth of research, technology and innovation. For more than 60 years, the Lawrence Livermore National Laboratory has applied science and technology to make the world a safer place. World class facilities include the National Ignition Facility, and the Terascale facility hosting Sequoia, currently the world's fifth fastest supercomputer.","title":"About Livermore and LLNL"},{"location":"ceed1am/#questions","text":"Please contact the meeting organizers at: llnl-ceed-meeting@listserv.llnl.gov .","title":"Questions?"},{"location":"ceed2am/","text":"CEED Second Annual Meeting August 8-10, 2018 University of Colorado Boulder Overview CEED will hold its second annual meeting August 8-10, 2018 in Koelbel S127 ( adjacent to the Engineering Center) at the University of Colorado Boulder. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects. Registration If you plan to attend, please register no later than June 30 . Meeting Agenda Wednesday, August 8th Time Activity 8:00-8:30 Coffee & Refreshments 8:30-8:45 Welcome Jed Brown, CU Boulder 8:45-9:30 Overview of CEED Tzanio Kolev, LLNL http://ceed.exascaleproject.org 9:30-10:00 CEED Finite Element Thrust Activities Veselin Dobrev, LLNL Dr.Dobrev will present work performed by the CEED team related to the Finite Element thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/fe 10:00-10:30 Coffee Break 10:30-11:00 Nek5000, fast solvers, and scalability Paul Fischer, UIUC/ANL 11:00-11:30 libParanumal status update Tim Warburton, Virginia Tech 11:30-12:00 Developments in Support of Complex Geometry Conforming Mesh Adaptation Mark Shephard, RPI 12:00-1:30 Lunch 1:30-2:00 CEED Application Thrust Activities Misun Min, ANL Dr.Min will present work performed by the CEED team related to the Application thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/ap 2:00-2:30 ExaSMR: recent nuclear engineering petascale simulations and performance improvements Elia Merzari, ANL 2:30-3:00 Developments related to the MARBL application Vladimir Tomov, LLNL 3:00-3:30 Coffee Break & Group Photo 3:30-4:00 CEED Software Thrust Activities Jed Brown, CU Boulder Dr.Brown will present work performed by the CEED team related to the Software thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/sw 4:00-4:30 OCCA 1.0: Updates and Roadmap David Medina, Occalytics 4:30-4:50 CEED Hardware Thrust Activities Stan Tomov, UTK Dr.Tomov will present work performed by the CEED team related to the Finite Element thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/hw 4:50-5:10 LLNL Sierra Progress and Testbeds of Interest to CEED Ian Karlin, LLNL Walk to dinner 6:30-8:00 Conference Dinner FATE Brewing Company Thursday, August 9th Time Activity 8:00-8:30 Coffee & Refreshments 8:30-8:50 Performance of the E3SM spectral element atmosphere dynamical core Mark Taylor, SNL The E3SM atmosphere component model's dynamical core is based on the spectral element method. I'll present performance results from our Fortran and C++/kokkos implementations of this dynamical core on a range of processors (Ivy Bridge, Haswell, KNL, and P100 and V100 GPUs). With a careful implementation, the C++ code is competitive with the Fortran code on all processors and also supports GPUs. With sufficient work per node, the KNL and V100 processors can obtain significant speedups over conventional Xeons. But in the strong scaling limit, we see little or no improvement over the circa-2012 Ivy Bridge processor. 8:50-9:10 ExaWind Mike Sprague, NREL 9:10-9:30 Transforming Additive Manufacturing Through Exascale Computing (ExaAM) Christopher Newman, LANL 9:30-9:50 Modeling of RF actuator for fusion plasmas Syun'ichi Shiraiwa, MIT 10:00-10:30 Coffee Break 10:30-10:50 Tracer particles on high order curved meshes Kenny Weiss, LLNL 10:50-11:10 Urban Modeling in ECP Aleks Obabko, ANL 11:10-11:30 Use of the SUNDIALS suite of time integrators and nonlinear solvers from MFEM Carol Woodward, LLNL 11:30-11:40 Exascale MPI Project Update Ken Raffeneti, ANL 11:50-1:00 Lunch 1:00-3:00 Hackathon 3:00-6:00 Hike from Chautauqua Dinner on your own Friday, August 10th Time Activity 8:00-8:30 Coffee & Refreshments 8:30-10:10 Internal planning/future milestones 10:10-10:30 Coffee Break 10:30-12:00 Discussion and hacking 12:00-1:00 Lunch 1:00-3:00 Discussion and hacking 3:00-3:30 Coffee Break 3:30-5:00 Discussion and hacking Dinner on your own Friday discussion topics next round of BPs libCEED discussion FMS discussion matrix-free solvers working groups: AP, SW, HW, FE internal hackathon Lodging Options There are many hotels close to the university in Boulder. Millennium Harvest - 5 minute walk Best Western - 10 minute walk Marriott - 10 minute walk Embassy Suites (Hilton) - 10 minute walk Garden Inn (Hilton) - 10 minute walk Transportation from the Denver airport RTD AB bus runs hourly for $9. [It is also possible to take the A Line (light rail) from the airport to Denver's Union station (leaving every 15 minutes) and transfer to the FF1 / FF2 to Boulder -- this is an option if you just miss the AB. Other options include Lyft/Uber and Green Ride Boulder . Parking If you need to park on campus, you can get a temporary permit at the parking station on the east side of the Engineering Center . Questions? Please contact the meeting organizers at: ceed-meeting@llnl.gov .","title":"_CEED2AM"},{"location":"ceed2am/#ceed-second-annual-meeting","text":"","title":"CEED Second Annual Meeting"},{"location":"ceed2am/#august-8-10-2018","text":"","title":"August 8-10, 2018"},{"location":"ceed2am/#university-of-colorado-boulder","text":"","title":"University of Colorado Boulder"},{"location":"ceed2am/#overview","text":"CEED will hold its second annual meeting August 8-10, 2018 in Koelbel S127 ( adjacent to the Engineering Center) at the University of Colorado Boulder. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.","title":"Overview"},{"location":"ceed2am/#registration","text":"If you plan to attend, please register no later than June 30 .","title":"Registration"},{"location":"ceed2am/#meeting-agenda","text":"","title":"Meeting Agenda"},{"location":"ceed2am/#wednesday-august-8th","text":"Time Activity 8:00-8:30 Coffee & Refreshments 8:30-8:45 Welcome Jed Brown, CU Boulder 8:45-9:30 Overview of CEED Tzanio Kolev, LLNL http://ceed.exascaleproject.org 9:30-10:00 CEED Finite Element Thrust Activities Veselin Dobrev, LLNL Dr.Dobrev will present work performed by the CEED team related to the Finite Element thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/fe 10:00-10:30 Coffee Break 10:30-11:00 Nek5000, fast solvers, and scalability Paul Fischer, UIUC/ANL 11:00-11:30 libParanumal status update Tim Warburton, Virginia Tech 11:30-12:00 Developments in Support of Complex Geometry Conforming Mesh Adaptation Mark Shephard, RPI 12:00-1:30 Lunch 1:30-2:00 CEED Application Thrust Activities Misun Min, ANL Dr.Min will present work performed by the CEED team related to the Application thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/ap 2:00-2:30 ExaSMR: recent nuclear engineering petascale simulations and performance improvements Elia Merzari, ANL 2:30-3:00 Developments related to the MARBL application Vladimir Tomov, LLNL 3:00-3:30 Coffee Break & Group Photo 3:30-4:00 CEED Software Thrust Activities Jed Brown, CU Boulder Dr.Brown will present work performed by the CEED team related to the Software thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/sw 4:00-4:30 OCCA 1.0: Updates and Roadmap David Medina, Occalytics 4:30-4:50 CEED Hardware Thrust Activities Stan Tomov, UTK Dr.Tomov will present work performed by the CEED team related to the Finite Element thrust milestones and outline future plans and ideas for collaborations. http://ceed.exascaleproject.org/hw 4:50-5:10 LLNL Sierra Progress and Testbeds of Interest to CEED Ian Karlin, LLNL Walk to dinner 6:30-8:00 Conference Dinner FATE Brewing Company","title":"Wednesday, August 8th"},{"location":"ceed2am/#thursday-august-9th","text":"Time Activity 8:00-8:30 Coffee & Refreshments 8:30-8:50 Performance of the E3SM spectral element atmosphere dynamical core Mark Taylor, SNL The E3SM atmosphere component model's dynamical core is based on the spectral element method. I'll present performance results from our Fortran and C++/kokkos implementations of this dynamical core on a range of processors (Ivy Bridge, Haswell, KNL, and P100 and V100 GPUs). With a careful implementation, the C++ code is competitive with the Fortran code on all processors and also supports GPUs. With sufficient work per node, the KNL and V100 processors can obtain significant speedups over conventional Xeons. But in the strong scaling limit, we see little or no improvement over the circa-2012 Ivy Bridge processor. 8:50-9:10 ExaWind Mike Sprague, NREL 9:10-9:30 Transforming Additive Manufacturing Through Exascale Computing (ExaAM) Christopher Newman, LANL 9:30-9:50 Modeling of RF actuator for fusion plasmas Syun'ichi Shiraiwa, MIT 10:00-10:30 Coffee Break 10:30-10:50 Tracer particles on high order curved meshes Kenny Weiss, LLNL 10:50-11:10 Urban Modeling in ECP Aleks Obabko, ANL 11:10-11:30 Use of the SUNDIALS suite of time integrators and nonlinear solvers from MFEM Carol Woodward, LLNL 11:30-11:40 Exascale MPI Project Update Ken Raffeneti, ANL 11:50-1:00 Lunch 1:00-3:00 Hackathon 3:00-6:00 Hike from Chautauqua Dinner on your own","title":"Thursday, August 9th"},{"location":"ceed2am/#friday-august-10th","text":"Time Activity 8:00-8:30 Coffee & Refreshments 8:30-10:10 Internal planning/future milestones 10:10-10:30 Coffee Break 10:30-12:00 Discussion and hacking 12:00-1:00 Lunch 1:00-3:00 Discussion and hacking 3:00-3:30 Coffee Break 3:30-5:00 Discussion and hacking Dinner on your own","title":"Friday, August 10th"},{"location":"ceed2am/#friday-discussion-topics","text":"next round of BPs libCEED discussion FMS discussion matrix-free solvers working groups: AP, SW, HW, FE internal hackathon","title":"Friday discussion topics"},{"location":"ceed2am/#lodging-options","text":"There are many hotels close to the university in Boulder. Millennium Harvest - 5 minute walk Best Western - 10 minute walk Marriott - 10 minute walk Embassy Suites (Hilton) - 10 minute walk Garden Inn (Hilton) - 10 minute walk","title":"Lodging Options"},{"location":"ceed2am/#transportation-from-the-denver-airport","text":"RTD AB bus runs hourly for $9. [It is also possible to take the A Line (light rail) from the airport to Denver's Union station (leaving every 15 minutes) and transfer to the FF1 / FF2 to Boulder -- this is an option if you just miss the AB. Other options include Lyft/Uber and Green Ride Boulder .","title":"Transportation from the Denver airport"},{"location":"ceed2am/#parking","text":"If you need to park on campus, you can get a temporary permit at the parking station on the east side of the Engineering Center .","title":"Parking"},{"location":"ceed2am/#questions","text":"Please contact the meeting organizers at: ceed-meeting@llnl.gov .","title":"Questions?"},{"location":"ceed3am/","text":"CEED Third Annual Meeting August 6-8, 2019 Virginia Tech Overview CEED will hold its third annual meeting August 6-8, 2019 at Virginia Tech in Goodwin Hall 155 . The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects. Registration If you plan to attend, please register no later than June 30 . The registration fee for the meeting is $60, payable at the door in cash or check upon arrival at the meeting. The fee covers snacks and refreshments each day as well as a CEED T-shirt. Meeting Agenda Tuesday, August 6 Time Activity 8:00-8:30 Coffee & Welcome Tim Warburton (VT) and Tim Germann (LANL) 8:30-9:00 CEED Overview Tzanio Kolev (LLNL) 9:00-9:30 Finite Element Thrust Overview Veselin Dobrev (LLNL) 9:30-10:00 High-Order Solver Developments at UIUC Paul Fischer (UIUC) 10:00-10:30 Coffee Break 10:30-11:00 libParanumal Progress Update Tim Warburton (VT) libParanumal has proved capable of running SEM incompressible calculations in weak scaling mode on at least half of the GPUs on Summit at 90% efficiency. In this talk we will discuss ongoing efforts to reduce the number of degrees of freedom required to stay in the weak scaling regime. 11:00-11:30 OCCA Updates David Medina (Occalytics) 11:30-12:00 Supporting Complex Geometry RF Adaptive Simulations Mark Shephard (RPI) 12:00-12:30 Applications Thrust Overview Misun Min (ANL) 12:30-1:30 Lunch 1:30-2:00 Accelerating Numerical Methods for a Next Generation Multi-Physics Code Arturo Vargas (LLNL) MARBL is LLNL\u2019s next generation multi-physics code for simulating high energy density physics. A distinguishing feature of this code is the modular CS infrastructure (Axom), and choice of high order numerical methods. The choice of high order schemes leads to higher arithmetic intensity per data access, a trait favored by modern computing processors. In this talk, we provide an overview of recent developments within the Arbitrary Lagrangian-Eulerian package, Blast, and focus on our adoption of programming models and algorithmic tailoring to leverage next gen super computers. This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-ABS-774160. 2:00-2:30 Urban Aleks Obabko (ANL) 2:30-3:00 ExaConstit: Towards an Exascale Crystal Plasticity FEM Code Robert Carson (LLNL) An overview of ExaConstit, a new crystal plasticity code built upon the MFEM framework, will be given. It\u2019s being designed as a component of the Exascale Additive Manufacturing Project (ExaAM) to couple microstructure development and local property analysis with process simulation. Initial results of ExaConstit being applied to a generated microstructure from AM processing condition will be discussed. A strong scaling study of ExaConstit on Sierra/Summit like machines will also be presented. 3:00-3:30 ExaWind Robert Knaus (SNL) 3:30-4:00 Coffee Break & Group Photo 4:00-4:30 Software Thrust & libCEED Overview Jed Brown (CU Boulder) 4:30-5:00 libCEED under PETSc: Current and Future Efforts Oana Marin (ANL) We describe benefits of libCEED to PETSc and aspects to be pursued, from integration under complex unstructured meshes, to machine learning coupling and efficient support for research and implementation of matrix free preconditioners. 5:00-5:30 Hardware Thrust & MAGMA Stanmire Tomov (UTK) 5:30-6:00 Aurora: Argonne's Exascale System Scott Parker (ANL) Presentation and discussion of the publicly available information on the Aurora system and approaches to preparing applications. 7:00-9:00 Conference Dinner Location TBD Wednesday, August 7 Time Activity 8:00-8:30 Coffee 8:30-9:00 VTK-m Plans for Higher-Order Elements Hank Childs (Oregon) 9:00-9:30 Devil Ray: Ray Tracing High Order Meshes for Visualization Matt Larsen (LLNL) This talk will outline our efforts to develop a portable ray tracing library for high-order meshes. Devil Ray is focused on visualization tasks such as iso-surface and volume rendering. 9:30-10:00 Parallel I/O in MFEM Using ADIOS2 William Godoy (ORNL) This talk provides a brief overview of the integration efforts and available features of ADIOS2 as the parallel I/O solution to MFEM for code coupling, in-situ visualization, and scalable file I/O. 10:00-10:30 Coffee Break 10:30-11:00 Low-Synch Gram-Schmidt Projection Schemes for GMRES-AMG and for Moving Mesh Solvers Stephen Thomas (NREL) In collaboration with Tim Warburton and Anthony Austin, we apply the low-synch classical Gram-Schmidt algorithms recently proposed by Swirydowicz et al (2018) to the least-squares polynomial projection schemes introduced by Fischer (1998). We modify the original projection scheme to instead compute an improved initial guess for a preconditioned GMRES-AMG pressure solver for the incompressible Navier-Stokes equations. For the Nalu Wind model, the GMRES iteration count drops to zero with a relatively small window of previous solutions. 11:00-11:30 Fast Direct Solvers in MFEM Pieter Ghysels (LBNL) We present recent improvements in the SuperLU and STRUMPACK sparse direct solvers and preconditioners, which are available from MFEM. Support for GPUs has been improved or added. The preconditioners in STRUMPACK have been improved and tested on an indefinite Maxwell problem using MFEM. 11:30-12:00 Algebraic Multigrid Preconditioners for Matrix-Free Discretizations Bruno Turcksin (ORNL) Multigrid preconditioners are very popular preconditioners because of their convergence properties and scalability. Multigrid methods can be divided into geometric multigrid (GMG) and algebraic multigrid (AMG). GMG are based on building different coarser grids and therefore, are excellent as matrix-free preconditioners. The challenge is that GMG cannot be easily applied when the mesh is unstructured. AMG, however, use the entries in the matrix of the system to build coarser grids. This makes AMG more flexible when the matrix is available but it limits their use as matrix-free preconditioners. We surpass the limitation of knowing the matrix entries by basing our AMG on the spectral AMGe method. Unlike other AMG, the spectral AMGe method does not use the matrix of the system. Instead it requires the eigenvectors of the operator evaluated on parts of the domain (agglomerates). This means that, similar to GMG, a mesh is required. However the constraints on how the agglomerates are built are minimal since we do not need to rediscretize the operator on the agglomerates. While at the fine level no matrix associated to the operator is assembled, we still assemble a matrix at the coarsest level in order to use a direct solver. 12:00-12:30 Recent Progress on Accelerating Algebraic Multigrid Solvers with GPUs Ruipeng Li (LLNL) 12:30-1:30 Lunch 1:30-6:00 Hike to the Cascades Dinner on your own. Thursday, August 8 Time Activity 8:00-8:30 Coffee 8:30-10:00 Internal planning 10:00-10:30 Coffee Break 10:30-12:00 Meetings and discussions 12:00-1:30 Lunch 1:30-2:30 AMD HIP Tutorial Noel Chalmers and Damon McDougall (AMD) 2:30-3:30 Hackathon 3:30-4:00 Coffee Break 4:00-6:00 Hackathon Dinner on your own. Lodging Options The Inn at Virginia Tech is a 10-minute walk from the meeting. CEED attendees are eligible for a group rate of $139.00/night for the nights of August 5-8 while rooms are available. To reserve a room at that rate, click here . Other lodging options adjacent to campus: Residence Inn Blacksburg-University by Marriott - 10-minute walk Hyatt Place Blacksburg/University - 10-minute walk Main Street Inn - 15-20-minute walk The following hotels are further away (30-35-minute walk) but are still relatively close to campus. Attendees can drive (5 minutes; see below for information about parking) or take Blacksburg Transit (BT) to get to the meeting. The HWA and HWB bus routes ( schedule PDF ) connect the stop nearest these hotels ( Plantation/Prices Fork with the one nearest the meeting ( Old Security Building/Stanger/Old Turner ). Note that BT will be operating on reduced service schedules during the summer break. Holiday Inn Express & Suites Blacksburg Hilton Garden Inn Blacksburg Getting to Blacksburg The nearest airport to Blacksburg served by major commercial airlines is the regional airport in Roanoke (IATA symbol ROA), which is about 45 minutes away by car. Other nearby airports include Lynchburg (LYH, 1 hour and 30 minutes), Greensboro (GSO, 2 hours and 30 minutes), and Charlotte (CLT, 3 hours). Driving Directions from Roanoke Airport: From the airport's main entrance, turn right onto Aviation Dr. and exit immediately to westbound VA-101 / Hershberger Rd. Then, exit to I-581 N and follow signs to I-81 S toward Bristol. Proceed on I-81 S for about 25 mi. and then take exit 118-B to US-460 W toward Christiansburg and Blacksburg. After about 9 mi., exit to Prices Fork Rd., following signs to downtown. Campus is to the right following the intersection at University City Blvd. Buses and Shuttles from Roanoke Airport: The Smart Way Bus runs between Roanoke Airport and the Squires Student Center on campus. The fare is $4.00 one way, payable to the driver in exact cash. The Roanoke Airport Transportation Service operates shuttles between Roanoke Airport and Blacksburg that can drop passengers off at the Inn at Virginia Tech, the Holiday Inn, or the Squires Student Center. Reservations (required) can be made by calling 1-800-288-1958. Parking Parking is available on campus at the North End Center Garage , from which it is about a 5-minute walk to the meeting venue. There is a daily fee. Alternatively, one can acquire (also for a fee) a visitor's campus parking permit at the Visitor Center . Goodwin Hall is immediately adjacent to Perry Street Lot 3 . Further information about parking on campus may be found here . Questions? For questions, please contact the meeting organizers at ceed-meeting@llnl.gov . Acknowledgements We gratefully acknowledge support for this meeting from the Virginia Tech College of Science and Department of Mathematics.","title":"_CEED3AM"},{"location":"ceed3am/#ceed-third-annual-meeting","text":"","title":"CEED Third Annual Meeting"},{"location":"ceed3am/#august-6-8-2019","text":"","title":"August 6-8, 2019"},{"location":"ceed3am/#virginia-tech","text":"","title":"Virginia Tech"},{"location":"ceed3am/#overview","text":"CEED will hold its third annual meeting August 6-8, 2019 at Virginia Tech in Goodwin Hall 155 . The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.","title":"Overview"},{"location":"ceed3am/#registration","text":"If you plan to attend, please register no later than June 30 . The registration fee for the meeting is $60, payable at the door in cash or check upon arrival at the meeting. The fee covers snacks and refreshments each day as well as a CEED T-shirt.","title":"Registration"},{"location":"ceed3am/#meeting-agenda","text":"","title":"Meeting Agenda"},{"location":"ceed3am/#tuesday-august-6","text":"Time Activity 8:00-8:30 Coffee & Welcome Tim Warburton (VT) and Tim Germann (LANL) 8:30-9:00 CEED Overview Tzanio Kolev (LLNL) 9:00-9:30 Finite Element Thrust Overview Veselin Dobrev (LLNL) 9:30-10:00 High-Order Solver Developments at UIUC Paul Fischer (UIUC) 10:00-10:30 Coffee Break 10:30-11:00 libParanumal Progress Update Tim Warburton (VT) libParanumal has proved capable of running SEM incompressible calculations in weak scaling mode on at least half of the GPUs on Summit at 90% efficiency. In this talk we will discuss ongoing efforts to reduce the number of degrees of freedom required to stay in the weak scaling regime. 11:00-11:30 OCCA Updates David Medina (Occalytics) 11:30-12:00 Supporting Complex Geometry RF Adaptive Simulations Mark Shephard (RPI) 12:00-12:30 Applications Thrust Overview Misun Min (ANL) 12:30-1:30 Lunch 1:30-2:00 Accelerating Numerical Methods for a Next Generation Multi-Physics Code Arturo Vargas (LLNL) MARBL is LLNL\u2019s next generation multi-physics code for simulating high energy density physics. A distinguishing feature of this code is the modular CS infrastructure (Axom), and choice of high order numerical methods. The choice of high order schemes leads to higher arithmetic intensity per data access, a trait favored by modern computing processors. In this talk, we provide an overview of recent developments within the Arbitrary Lagrangian-Eulerian package, Blast, and focus on our adoption of programming models and algorithmic tailoring to leverage next gen super computers. This work was performed under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-ABS-774160. 2:00-2:30 Urban Aleks Obabko (ANL) 2:30-3:00 ExaConstit: Towards an Exascale Crystal Plasticity FEM Code Robert Carson (LLNL) An overview of ExaConstit, a new crystal plasticity code built upon the MFEM framework, will be given. It\u2019s being designed as a component of the Exascale Additive Manufacturing Project (ExaAM) to couple microstructure development and local property analysis with process simulation. Initial results of ExaConstit being applied to a generated microstructure from AM processing condition will be discussed. A strong scaling study of ExaConstit on Sierra/Summit like machines will also be presented. 3:00-3:30 ExaWind Robert Knaus (SNL) 3:30-4:00 Coffee Break & Group Photo 4:00-4:30 Software Thrust & libCEED Overview Jed Brown (CU Boulder) 4:30-5:00 libCEED under PETSc: Current and Future Efforts Oana Marin (ANL) We describe benefits of libCEED to PETSc and aspects to be pursued, from integration under complex unstructured meshes, to machine learning coupling and efficient support for research and implementation of matrix free preconditioners. 5:00-5:30 Hardware Thrust & MAGMA Stanmire Tomov (UTK) 5:30-6:00 Aurora: Argonne's Exascale System Scott Parker (ANL) Presentation and discussion of the publicly available information on the Aurora system and approaches to preparing applications. 7:00-9:00 Conference Dinner Location TBD","title":"Tuesday, August 6"},{"location":"ceed3am/#wednesday-august-7","text":"Time Activity 8:00-8:30 Coffee 8:30-9:00 VTK-m Plans for Higher-Order Elements Hank Childs (Oregon) 9:00-9:30 Devil Ray: Ray Tracing High Order Meshes for Visualization Matt Larsen (LLNL) This talk will outline our efforts to develop a portable ray tracing library for high-order meshes. Devil Ray is focused on visualization tasks such as iso-surface and volume rendering. 9:30-10:00 Parallel I/O in MFEM Using ADIOS2 William Godoy (ORNL) This talk provides a brief overview of the integration efforts and available features of ADIOS2 as the parallel I/O solution to MFEM for code coupling, in-situ visualization, and scalable file I/O. 10:00-10:30 Coffee Break 10:30-11:00 Low-Synch Gram-Schmidt Projection Schemes for GMRES-AMG and for Moving Mesh Solvers Stephen Thomas (NREL) In collaboration with Tim Warburton and Anthony Austin, we apply the low-synch classical Gram-Schmidt algorithms recently proposed by Swirydowicz et al (2018) to the least-squares polynomial projection schemes introduced by Fischer (1998). We modify the original projection scheme to instead compute an improved initial guess for a preconditioned GMRES-AMG pressure solver for the incompressible Navier-Stokes equations. For the Nalu Wind model, the GMRES iteration count drops to zero with a relatively small window of previous solutions. 11:00-11:30 Fast Direct Solvers in MFEM Pieter Ghysels (LBNL) We present recent improvements in the SuperLU and STRUMPACK sparse direct solvers and preconditioners, which are available from MFEM. Support for GPUs has been improved or added. The preconditioners in STRUMPACK have been improved and tested on an indefinite Maxwell problem using MFEM. 11:30-12:00 Algebraic Multigrid Preconditioners for Matrix-Free Discretizations Bruno Turcksin (ORNL) Multigrid preconditioners are very popular preconditioners because of their convergence properties and scalability. Multigrid methods can be divided into geometric multigrid (GMG) and algebraic multigrid (AMG). GMG are based on building different coarser grids and therefore, are excellent as matrix-free preconditioners. The challenge is that GMG cannot be easily applied when the mesh is unstructured. AMG, however, use the entries in the matrix of the system to build coarser grids. This makes AMG more flexible when the matrix is available but it limits their use as matrix-free preconditioners. We surpass the limitation of knowing the matrix entries by basing our AMG on the spectral AMGe method. Unlike other AMG, the spectral AMGe method does not use the matrix of the system. Instead it requires the eigenvectors of the operator evaluated on parts of the domain (agglomerates). This means that, similar to GMG, a mesh is required. However the constraints on how the agglomerates are built are minimal since we do not need to rediscretize the operator on the agglomerates. While at the fine level no matrix associated to the operator is assembled, we still assemble a matrix at the coarsest level in order to use a direct solver. 12:00-12:30 Recent Progress on Accelerating Algebraic Multigrid Solvers with GPUs Ruipeng Li (LLNL) 12:30-1:30 Lunch 1:30-6:00 Hike to the Cascades Dinner on your own.","title":"Wednesday, August 7"},{"location":"ceed3am/#thursday-august-8","text":"Time Activity 8:00-8:30 Coffee 8:30-10:00 Internal planning 10:00-10:30 Coffee Break 10:30-12:00 Meetings and discussions 12:00-1:30 Lunch 1:30-2:30 AMD HIP Tutorial Noel Chalmers and Damon McDougall (AMD) 2:30-3:30 Hackathon 3:30-4:00 Coffee Break 4:00-6:00 Hackathon Dinner on your own.","title":"Thursday, August 8"},{"location":"ceed3am/#lodging-options","text":"The Inn at Virginia Tech is a 10-minute walk from the meeting. CEED attendees are eligible for a group rate of $139.00/night for the nights of August 5-8 while rooms are available. To reserve a room at that rate, click here . Other lodging options adjacent to campus: Residence Inn Blacksburg-University by Marriott - 10-minute walk Hyatt Place Blacksburg/University - 10-minute walk Main Street Inn - 15-20-minute walk The following hotels are further away (30-35-minute walk) but are still relatively close to campus. Attendees can drive (5 minutes; see below for information about parking) or take Blacksburg Transit (BT) to get to the meeting. The HWA and HWB bus routes ( schedule PDF ) connect the stop nearest these hotels ( Plantation/Prices Fork with the one nearest the meeting ( Old Security Building/Stanger/Old Turner ). Note that BT will be operating on reduced service schedules during the summer break. Holiday Inn Express & Suites Blacksburg Hilton Garden Inn Blacksburg","title":"Lodging Options"},{"location":"ceed3am/#getting-to-blacksburg","text":"The nearest airport to Blacksburg served by major commercial airlines is the regional airport in Roanoke (IATA symbol ROA), which is about 45 minutes away by car. Other nearby airports include Lynchburg (LYH, 1 hour and 30 minutes), Greensboro (GSO, 2 hours and 30 minutes), and Charlotte (CLT, 3 hours). Driving Directions from Roanoke Airport: From the airport's main entrance, turn right onto Aviation Dr. and exit immediately to westbound VA-101 / Hershberger Rd. Then, exit to I-581 N and follow signs to I-81 S toward Bristol. Proceed on I-81 S for about 25 mi. and then take exit 118-B to US-460 W toward Christiansburg and Blacksburg. After about 9 mi., exit to Prices Fork Rd., following signs to downtown. Campus is to the right following the intersection at University City Blvd. Buses and Shuttles from Roanoke Airport: The Smart Way Bus runs between Roanoke Airport and the Squires Student Center on campus. The fare is $4.00 one way, payable to the driver in exact cash. The Roanoke Airport Transportation Service operates shuttles between Roanoke Airport and Blacksburg that can drop passengers off at the Inn at Virginia Tech, the Holiday Inn, or the Squires Student Center. Reservations (required) can be made by calling 1-800-288-1958.","title":"Getting to Blacksburg"},{"location":"ceed3am/#parking","text":"Parking is available on campus at the North End Center Garage , from which it is about a 5-minute walk to the meeting venue. There is a daily fee. Alternatively, one can acquire (also for a fee) a visitor's campus parking permit at the Visitor Center . Goodwin Hall is immediately adjacent to Perry Street Lot 3 . Further information about parking on campus may be found here .","title":"Parking"},{"location":"ceed3am/#questions","text":"For questions, please contact the meeting organizers at ceed-meeting@llnl.gov .","title":"Questions?"},{"location":"ceed3am/#acknowledgements","text":"We gratefully acknowledge support for this meeting from the Virginia Tech College of Science and Department of Mathematics.","title":"Acknowledgements"},{"location":"codesign/","text":"Discretization Co-Design Approach CEED's co-design approach is based on close collaboration between its Applications , Hardware , and Software thrusts, each of which has a two-way, push-and-pull relation with the external application, hardware and software technologies teams. CEED's Finite Elements thrust serves as a central hub that ties together, coordinates and contributes to the efforts in all thrusts. For example, the development of discretization libraries in CEED is led by the Finite Elements thrust but involves working closely with vendors ( Hardware thrust) and software technology efforts ( Software thrust) to take full advantage of exascale hardware. Making sure that these libraries meet the needs of, and are successfully incorporated in, ECP applications is based on collaboration between the Applications and Finite Elements thrusts. In addition to libraries of highly performant kernels, a key product of the CEED project will be a set of miniapps that will serve multiple roles: Provide a mechanism to test and optimize across the breadth of implementations already developed by team members for a variety of platforms. Serve as stand-alone unit-test drivers for the library kernels. Provide well-documented (implementation, usage, and performance) benchmarks to work with vendors, now and in the future (e.g. in system procurement). Provide test and demonstration cases for application scientists who are considering new formulations. The miniapps developed in CEED, which we also refer to as CEEDlings , will range from local, element-level kernels , which can be run in a simulator, to bake-off problems , which combine local and global kernels into model problem benchmarks for high-order computations, to proxy apps , which will include application-relevant physics. These encapsulated CEEDlings will be used in interactions with vendors on emergent HPC technologies and ECP software technologies projects to highlight performance critical paths (e.g. size of on package memory, internode latency, hardware collectives) and provide simple examples of meaningful high-order computations. One of the goals in these interactions will be to impact the design of exascale architectures , and system and application software, for improved portability and performance of the high-order algorithms.","title":"Approach"},{"location":"codesign/#discretization-co-design-approach","text":"CEED's co-design approach is based on close collaboration between its Applications , Hardware , and Software thrusts, each of which has a two-way, push-and-pull relation with the external application, hardware and software technologies teams. CEED's Finite Elements thrust serves as a central hub that ties together, coordinates and contributes to the efforts in all thrusts. For example, the development of discretization libraries in CEED is led by the Finite Elements thrust but involves working closely with vendors ( Hardware thrust) and software technology efforts ( Software thrust) to take full advantage of exascale hardware. Making sure that these libraries meet the needs of, and are successfully incorporated in, ECP applications is based on collaboration between the Applications and Finite Elements thrusts. In addition to libraries of highly performant kernels, a key product of the CEED project will be a set of miniapps that will serve multiple roles: Provide a mechanism to test and optimize across the breadth of implementations already developed by team members for a variety of platforms. Serve as stand-alone unit-test drivers for the library kernels. Provide well-documented (implementation, usage, and performance) benchmarks to work with vendors, now and in the future (e.g. in system procurement). Provide test and demonstration cases for application scientists who are considering new formulations. The miniapps developed in CEED, which we also refer to as CEEDlings , will range from local, element-level kernels , which can be run in a simulator, to bake-off problems , which combine local and global kernels into model problem benchmarks for high-order computations, to proxy apps , which will include application-relevant physics. These encapsulated CEEDlings will be used in interactions with vendors on emergent HPC technologies and ECP software technologies projects to highlight performance critical paths (e.g. size of on package memory, internode latency, hardware collectives) and provide simple examples of meaningful high-order computations. One of the goals in these interactions will be to impact the design of exascale architectures , and system and application software, for improved portability and performance of the high-order algorithms.","title":"Discretization Co-Design Approach"},{"location":"discr/","text":"Efficient High-Order Operator Format While a global (parallel) sparse matrix is a good representation of a PDE operator discretized with low-order elements, a global parallel matrix is a poor choice when discretizing with high-order elements, due to the large cost of both the memory transfer and floating point operations. CEED is developing an alternative operator format, based on the CEED low-level API , that allows efficient operator evaluation that is optimal in memory and nearly-optimal in FLOPs cost. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details... General Interpolation of Solution Field Values Particle tracking, grid-to-grid transfer and data analysis are typical operations that require off-grid function evaluation. CEED is developing a scalable interpolation routine for arbitrary-order hexahedral elements that uses a hash table to rapidly identify candidate elements/processors that might contain the point in question, followed by a Newton iteration to find the point in the reference domain. The iteration is based on minimization, rather than root-finding, which is advantageous when the interpolation point is on or near an element boundary where high-order interpolants tend to exhibit rapid variation. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Discretization"},{"location":"discr/#efficient-high-order-operator-format","text":"While a global (parallel) sparse matrix is a good representation of a PDE operator discretized with low-order elements, a global parallel matrix is a poor choice when discretizing with high-order elements, due to the large cost of both the memory transfer and floating point operations. CEED is developing an alternative operator format, based on the CEED low-level API , that allows efficient operator evaluation that is optimal in memory and nearly-optimal in FLOPs cost. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Efficient High-Order Operator Format "},{"location":"discr/#general-interpolation-of-solution-field-values","text":"Particle tracking, grid-to-grid transfer and data analysis are typical operations that require off-grid function evaluation. CEED is developing a scalable interpolation routine for arbitrary-order hexahedral elements that uses a hash table to rapidly identify candidate elements/processors that might contain the point in question, followed by a Newton iteration to find the point in the reference domain. The iteration is based on minimization, rather than root-finding, which is advantageous when the interpolation point is on or near an element boundary where high-order interpolants tend to exhibit rapid variation. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"General Interpolation of Solution Field Values"},{"location":"fe/","text":"Finite Element Thrust The goal of CEED's Finite Element (FE) thrust, led by Veselin Dobrev from Lawrence Livermore National Laboratory , is to continue to improve the state-of-the-art spectral element/high-order finite element algorithms and kernels in the CEED software targeting exascale architectures. Why High-Order Finite Elements? Efficient exploitation of exascale architectures requires a rethink of the numerical algorithms used in large-scale scientific applications. These architectures favor algorithms that expose ultra-fine-grain parallelism and maximize the ratio of floating-point operations to energy-intensive data movement. Many large-scale applications employ unstructured finite element discretization methods, where practical efficiency is measured by the accuracy achieved per unit computational time. One of the few viable approaches to achieve high performance in this case is to use matrix-free high-order finite element methods, since these methods can both increase the accuracy and/or lower the computational time due to reduced data motion. To achieve this efficiency, high-order methods use mesh elements that are mapped from canonical reference elements (hexes, wedges, pyramids, tetrahedra) and exploit, where possible, the tensor-product structure of the canonical mesh elements and finite element spaces. Through matrix-free partial assembly, the use of canonical reference elements enables substantial cache efficiency and minimizes extraneous data movement in comparison to traditional low-order approaches. High-Order Benchmarks and Miniapps The FE thrust works closely with the Applications and Hardware thrusts on the development of CEED benchmarks and miniapps , and coordinates the project software activities with the Software thrust. Ecosystem for High-Order Applications In addition to performance improvements, the FE thrust is focusing on community-wide challenges that are necessary for establishing a full-fledged high-order application software ecosystem. These efforts include research and development in the following areas: High-Order Meshes Unstructured Adaptive Mesh Refinement Efficient High-Order Operator Format / Representation Batched Dense Tensor Contractions Scalable Matrix-Free Solvers General Interpolation Visualization of High-Order Meshes and Functions","title":"Finite Elements"},{"location":"fe/#finite-element-thrust","text":"The goal of CEED's Finite Element (FE) thrust, led by Veselin Dobrev from Lawrence Livermore National Laboratory , is to continue to improve the state-of-the-art spectral element/high-order finite element algorithms and kernels in the CEED software targeting exascale architectures.","title":"Finite Element Thrust "},{"location":"fe/#why-high-order-finite-elements","text":"Efficient exploitation of exascale architectures requires a rethink of the numerical algorithms used in large-scale scientific applications. These architectures favor algorithms that expose ultra-fine-grain parallelism and maximize the ratio of floating-point operations to energy-intensive data movement. Many large-scale applications employ unstructured finite element discretization methods, where practical efficiency is measured by the accuracy achieved per unit computational time. One of the few viable approaches to achieve high performance in this case is to use matrix-free high-order finite element methods, since these methods can both increase the accuracy and/or lower the computational time due to reduced data motion. To achieve this efficiency, high-order methods use mesh elements that are mapped from canonical reference elements (hexes, wedges, pyramids, tetrahedra) and exploit, where possible, the tensor-product structure of the canonical mesh elements and finite element spaces. Through matrix-free partial assembly, the use of canonical reference elements enables substantial cache efficiency and minimizes extraneous data movement in comparison to traditional low-order approaches.","title":"Why High-Order Finite Elements?"},{"location":"fe/#high-order-benchmarks-and-miniapps","text":"The FE thrust works closely with the Applications and Hardware thrusts on the development of CEED benchmarks and miniapps , and coordinates the project software activities with the Software thrust.","title":"High-Order Benchmarks and Miniapps"},{"location":"fe/#ecosystem-for-high-order-applications","text":"In addition to performance improvements, the FE thrust is focusing on community-wide challenges that are necessary for establishing a full-fledged high-order application software ecosystem. These efforts include research and development in the following areas: High-Order Meshes Unstructured Adaptive Mesh Refinement Efficient High-Order Operator Format / Representation Batched Dense Tensor Contractions Scalable Matrix-Free Solvers General Interpolation Visualization of High-Order Meshes and Functions","title":"Ecosystem for High-Order Applications"},{"location":"fms/","text":"FMS FMS (Field and Mesh Specification) is a high-order interface proposed by CEED, that allows a wide variety of applications and visualization tools to represent unstructured high-order meshes with general high-order finite element fields defined on them. FMS is intended as a lightweight format and API that can represent general finite elements within a common, easy to use framework. This includes high-order solutions and meshes as well as non-standard finite elements, such as Nedelec and Raviart-Thomas elements. See the FMS documentation for more details. This is an active area of research for our team and we are interested in feedback and collaboration. For more information, visit https://github.com/CEED/FMS .","title":"FMS"},{"location":"fms/#fms","text":"FMS (Field and Mesh Specification) is a high-order interface proposed by CEED, that allows a wide variety of applications and visualization tools to represent unstructured high-order meshes with general high-order finite element fields defined on them. FMS is intended as a lightweight format and API that can represent general finite elements within a common, easy to use framework. This includes high-order solutions and meshes as well as non-standard finite elements, such as Nedelec and Raviart-Thomas elements. See the FMS documentation for more details. This is an active area of research for our team and we are interested in feedback and collaboration. For more information, visit https://github.com/CEED/FMS .","title":"FMS "},{"location":"gslib/","text":"GSLIB GSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications. It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers. Any global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix. GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$. Users simply provide the local-to-global map $j(i)$ on each processor. GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms. The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks. GSLIB supports Stencils of arbitrary width 64-bit indexing for global addressing short / long ints, floats, doubles arbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$ GPUDirect communication (beta-version) gather-scatter on arbitrary m-tuples (vector fields) Additional features include: XXT solver (parallel direct solver for coarse-grid problems) AMG solver (design for solving coarse-grid problems in parallel) Robust and efficient spectral/finite element interpolation in parallel. Applications and libraries using GSLIB include Nek5000 , NekCEM , Nektar++ , and MOAB . In CEED, GSLIB is primarily involved in the efforts of the Software thrust. For more information, see the GSLIB GitHub repository: https://github.com/gslib/gslib . MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"GSLIB"},{"location":"gslib/#gslib","text":"GSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications. It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers. Any global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix. GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$. Users simply provide the local-to-global map $j(i)$ on each processor. GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms. The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks. GSLIB supports Stencils of arbitrary width 64-bit indexing for global addressing short / long ints, floats, doubles arbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$ GPUDirect communication (beta-version) gather-scatter on arbitrary m-tuples (vector fields) Additional features include: XXT solver (parallel direct solver for coarse-grid problems) AMG solver (design for solving coarse-grid problems in parallel) Robust and efficient spectral/finite element interpolation in parallel. Applications and libraries using GSLIB include Nek5000 , NekCEM , Nektar++ , and MOAB . In CEED, GSLIB is primarily involved in the efforts of the Software thrust. For more information, see the GSLIB GitHub repository: https://github.com/gslib/gslib . MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});","title":"GSLIB"},{"location":"hw/","text":"Hardware Thrust The goal of CEED's Hardware (HW) thrust, led by Jack Dongarra from The University of Tennessee, Knoxville , is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope. In addition to maintaining a close connection with ECP vendors, the HW thrust is a connection point for the MPI, OpenMP, and compiler-related work in the ECP. Members of the HW team are also actively involved with the batched BLAS standardization efforts. The HW thrust participates in a variety of co-design activities, both with ECP hardware vendors and within CEED miniapps, to explore optimal data locality and motion and to enhance the scalability and parallelism of high-order algorithms.","title":"Hardware"},{"location":"hw/#hardware-thrust","text":"The goal of CEED's Hardware (HW) thrust, led by Jack Dongarra from The University of Tennessee, Knoxville , is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope. In addition to maintaining a close connection with ECP vendors, the HW thrust is a connection point for the MPI, OpenMP, and compiler-related work in the ECP. Members of the HW team are also actively involved with the batched BLAS standardization efforts. The HW thrust participates in a variety of co-design activities, both with ECP hardware vendors and within CEED miniapps, to explore optimal data locality and motion and to enhance the scalability and parallelism of high-order algorithms.","title":"Hardware Thrust "},{"location":"libceed/","text":"libCEED libCEED is a high-order API library, that provides a common algebraic low-level operator description, allowing a wide variety of applications to take advantage of the efficient operator evaluation algorithms in the different CEED packages (from a single source). One of the challenges with high-order methods is that a global sparse matrix is no longer a good representation of a high-order linear operator, both with respect to the FLOPs needed for its evaluation, as well as the memory transfer needed for a matvec. Thus, high-order methods require a new \"format\" that still represents a linear (or more generally non-linear) operator, but not through a sparse matrix. The goal of libCEED is to propose such a format, as well as supporting implementations and data structures, that enable efficient operator evaluation on a variety of computational device types (CPUs, GPUs, etc.). This new operator description is based on algebraically factored form, which is easy to incorporate in a wide variety of applications, without significant refactoring of their own discretization infrastructure. This is an active area of research for our team and we are interested in feedback and collaboration. For more information, visit https://github.com/CEED/libCEED .","title":"libCEED"},{"location":"libceed/#libceed","text":"libCEED is a high-order API library, that provides a common algebraic low-level operator description, allowing a wide variety of applications to take advantage of the efficient operator evaluation algorithms in the different CEED packages (from a single source). One of the challenges with high-order methods is that a global sparse matrix is no longer a good representation of a high-order linear operator, both with respect to the FLOPs needed for its evaluation, as well as the memory transfer needed for a matvec. Thus, high-order methods require a new \"format\" that still represents a linear (or more generally non-linear) operator, but not through a sparse matrix. The goal of libCEED is to propose such a format, as well as supporting implementations and data structures, that enable efficient operator evaluation on a variety of computational device types (CPUs, GPUs, etc.). This new operator description is based on algebraically factored form, which is easy to incorporate in a wide variety of applications, without significant refactoring of their own discretization infrastructure. This is an active area of research for our team and we are interested in feedback and collaboration. For more information, visit https://github.com/CEED/libCEED .","title":"libCEED "},{"location":"libparanumal/","text":"libParanumal libParanumal is an experimental set of finite element flow solvers for heterogeneous (GPU/CPU) systems developed by the Parallel Numerical Algorithms Group at Virginia Tech . See below for a brief summary of the major features. For more information, see the libParanumal website . Supported elements Triangles, quadrilaterals, tetrahedra, hexahedra. Lagrange basis functions up to degree 15. Partial support for Bezier-Bernstein basis functions. Mesh wrangling Gmsh format file loaders. Load balanced geometric partitioning using space filling curves (Hilbert or Morton ordering). Clustered partitioning for multirate time stepping. Elliptic solver Linear Poisson and screened Poisson potential solvers. GPU optimized matrix-vector products. Hybrid p-type multigrid and algebraic multigrid preconditioned conjugate gradient solver. Sparse matrix or nearly matrix-free algebraic multigrid for coarse levels of multigrid hierarchy. Heterogeneous accelerated flow solvers Linearized Euler equations. Isothermal compressible Navier-Stokes solver with: Upwind discontinuous Galerkin discretization in space. Dormand-Prince adaptive Runge-Kutta integration in time. Isothermal Galerkin-Boltzmann gas dynamics solver with: Penalty flux DG discretization in space. Adaptive semi-analytic (pointwise exponential) integration in time. Multiaxial quasi-perfectly matched absorbing layer far field boundary condition. Incompressible Navier-Stokes solver with: Choice of continuous FEM or interior penalty DG in space. Extrapolation-BDF integration in time. Sub-cycling (Operator Integration Factor Splitting) for advection. libParanumal depends on MPI and OCCA . It is freely available under an MIT license. In CEED, libParanumal is primarily involved in the efforts of the Finite Element , Applications and Software thrusts.","title":"libParanumal"},{"location":"libparanumal/#libparanumal","text":"libParanumal is an experimental set of finite element flow solvers for heterogeneous (GPU/CPU) systems developed by the Parallel Numerical Algorithms Group at Virginia Tech . See below for a brief summary of the major features. For more information, see the libParanumal website .","title":"libParanumal "},{"location":"libparanumal/#supported-elements","text":"Triangles, quadrilaterals, tetrahedra, hexahedra. Lagrange basis functions up to degree 15. Partial support for Bezier-Bernstein basis functions.","title":"Supported elements"},{"location":"libparanumal/#mesh-wrangling","text":"Gmsh format file loaders. Load balanced geometric partitioning using space filling curves (Hilbert or Morton ordering). Clustered partitioning for multirate time stepping.","title":"Mesh wrangling"},{"location":"libparanumal/#elliptic-solver","text":"Linear Poisson and screened Poisson potential solvers. GPU optimized matrix-vector products. Hybrid p-type multigrid and algebraic multigrid preconditioned conjugate gradient solver. Sparse matrix or nearly matrix-free algebraic multigrid for coarse levels of multigrid hierarchy.","title":"Elliptic solver"},{"location":"libparanumal/#heterogeneous-accelerated-flow-solvers","text":"Linearized Euler equations. Isothermal compressible Navier-Stokes solver with: Upwind discontinuous Galerkin discretization in space. Dormand-Prince adaptive Runge-Kutta integration in time. Isothermal Galerkin-Boltzmann gas dynamics solver with: Penalty flux DG discretization in space. Adaptive semi-analytic (pointwise exponential) integration in time. Multiaxial quasi-perfectly matched absorbing layer far field boundary condition. Incompressible Navier-Stokes solver with: Choice of continuous FEM or interior penalty DG in space. Extrapolation-BDF integration in time. Sub-cycling (Operator Integration Factor Splitting) for advection. libParanumal depends on MPI and OCCA . It is freely available under an MIT license. In CEED, libParanumal is primarily involved in the efforts of the Finite Element , Applications and Software thrusts.","title":"Heterogeneous accelerated flow solvers"},{"location":"linalg/","text":"Batched Dense Tensor Contractions The numerical kernels of efficient high-order operator evaluation reduce to many small dense tensor contractions, one for each element of the computational mesh. These contractions can be performed in parallel over the elements and can be implemented as a sequence of small matrix-matrix multiplications (dgemms). CEED is developing efficient algorithms for these numerical kernels on a variety of advanced hardware, both by directly working on the specifying tensor contraction found in high-order finite element kernels, as well as by recasting to batched interface of LAPACK-like linear algebra libraries providing efficient dgemms for matrices of small-to-medium size. CEED scientists are also involved in the standardization efforts for batched versions of Basic Linear Algebra Software Library (BLAS) routines, which are the foundations for LAPACK-type libraries. This is an active area of research for our team and we are interested in collaboration. Scalable Matrix-Free Solvers CEED is developing matrix-free linear solvers and preconditioners for partially assembled high-order operators utilizing expertise from solver libraries like hypre and PETSc . This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Linear Algebra"},{"location":"linalg/#batched-dense-tensor-contractions","text":"The numerical kernels of efficient high-order operator evaluation reduce to many small dense tensor contractions, one for each element of the computational mesh. These contractions can be performed in parallel over the elements and can be implemented as a sequence of small matrix-matrix multiplications (dgemms). CEED is developing efficient algorithms for these numerical kernels on a variety of advanced hardware, both by directly working on the specifying tensor contraction found in high-order finite element kernels, as well as by recasting to batched interface of LAPACK-like linear algebra libraries providing efficient dgemms for matrices of small-to-medium size. CEED scientists are also involved in the standardization efforts for batched versions of Basic Linear Algebra Software Library (BLAS) routines, which are the foundations for LAPACK-type libraries. This is an active area of research for our team and we are interested in collaboration.","title":"Batched Dense Tensor Contractions "},{"location":"linalg/#scalable-matrix-free-solvers","text":"CEED is developing matrix-free linear solvers and preconditioners for partially assembled high-order operators utilizing expertise from solver libraries like hypre and PETSc . This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Scalable Matrix-Free Solvers"},{"location":"magma/","text":"MAGMA MAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of next generation linear algebra libraries for heterogeneous architectures. MAGMA is designed and implemented by the team that developed LAPACK and ScaLAPACK, incorporating the latest developments in hybrid synchronization- and communication-avoiding algorithms, as well as dynamic runtime systems. Interfaces for the current LAPACK and BLAS standards are supported to allow computational scientists to seamlessly port any linear algebra reliant software components to heterogeneous architectures. MAGMA allows applications to fully exploit the power of current heterogeneous systems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible time to accurate solution within given energy constraints. Hybrid Algorithms MAGMA uses a hybridization methodology where algorithms of interest are split into tasks of varying granularity and their execution scheduled over the available hardware components. Scheduling can be static or dynamic. In either case, small non-parallelizable tasks, often on the critical path, are scheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS are scheduled on accelerators. Performance and Energy Efficiency MAGMA solvers run close to the machine's peak performance. The LU factorization for example, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM), as the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs on the GPU. MAGMA Batched MAGMA Batched targets small linear algebra operations. Small computational tasks are difficult to parallelize, but applications usually require the computation of many small problems, which can be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended now under the CEED project to support tensor data structures and tensor contractions for high-order methods. MAGMA Sparse MAGMA Sparse targets the development of high-performance sparse linear algebra operations on accelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace iterative solvers, eigensolvers, and preconditioners. MAGMA Development MAGMA is being developed at the Innovative Computing Laboratory of the University of Tennessee . In CEED, MAGMA is primarily involved in the efforts of the Software , Hardware and Finite Element thrusts. For more information, see the MAGMA website: http://icl.cs.utk.edu/magma .","title":"MAGMA"},{"location":"magma/#magma","text":"MAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of next generation linear algebra libraries for heterogeneous architectures. MAGMA is designed and implemented by the team that developed LAPACK and ScaLAPACK, incorporating the latest developments in hybrid synchronization- and communication-avoiding algorithms, as well as dynamic runtime systems. Interfaces for the current LAPACK and BLAS standards are supported to allow computational scientists to seamlessly port any linear algebra reliant software components to heterogeneous architectures. MAGMA allows applications to fully exploit the power of current heterogeneous systems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible time to accurate solution within given energy constraints.","title":"MAGMA"},{"location":"magma/#hybrid-algorithms","text":"MAGMA uses a hybridization methodology where algorithms of interest are split into tasks of varying granularity and their execution scheduled over the available hardware components. Scheduling can be static or dynamic. In either case, small non-parallelizable tasks, often on the critical path, are scheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS are scheduled on accelerators.","title":"Hybrid Algorithms"},{"location":"magma/#performance-and-energy-efficiency","text":"MAGMA solvers run close to the machine's peak performance. The LU factorization for example, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM), as the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs on the GPU.","title":"Performance and Energy Efficiency"},{"location":"magma/#magma-batched","text":"MAGMA Batched targets small linear algebra operations. Small computational tasks are difficult to parallelize, but applications usually require the computation of many small problems, which can be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended now under the CEED project to support tensor data structures and tensor contractions for high-order methods.","title":"MAGMA Batched"},{"location":"magma/#magma-sparse","text":"MAGMA Sparse targets the development of high-performance sparse linear algebra operations on accelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace iterative solvers, eigensolvers, and preconditioners.","title":"MAGMA Sparse"},{"location":"magma/#magma-development","text":"MAGMA is being developed at the Innovative Computing Laboratory of the University of Tennessee . In CEED, MAGMA is primarily involved in the efforts of the Software , Hardware and Finite Element thrusts. For more information, see the MAGMA website: http://icl.cs.utk.edu/magma .","title":"MAGMA Development"},{"location":"meshing/","text":"High-Order Meshes High-order (curved) meshes appear in many applications, e.g. due to curved material interfaces or due to mesh motion in simulations with moving meshes (for example, ALE methods). The use of high-order geometry representation can lead to better feature resolution, improved symmetry preservation, better solution adaptivity and increased robustness for problems where the mesh is being deformed in time. The CEED software supports arbitrary order quadrilateral, hexahedral, triangular and tetrahedral meshes, and our team members are developing algorithms for high-order mesh optimization and solution transfer between deformed high-order meshes (remap). This is an active area of research for our team and we are interested in collaboration. Unstructured AMR Adaptive Mesh Refinement (AMR) on unstructured grids is an important enabling technology for many applications. The CEED software supports both conforming AMR on triangular and tetrahedral meshes as well as general non-conforming AMR on quadrilateral and hexahedral meshes. Both approaches work for low-order meshes with no performance penalty and support parallel dynamic load balancing. One of the distinguishing characteristics of our AMR approach is that it is implemented at the level of the discretization library, decoupled from the physics, so it can easily be incorporated in a variety of applications. We support spaces in the whole de Rham complex, as well as AMR on high-order meshes. CEED scientists are working on practical error indicators and physics-conservative AMR interpolations to meet application needs. This is an active area of research for our team and we are interested in collaboration.","title":"Meshing"},{"location":"meshing/#high-order-meshes","text":"High-order (curved) meshes appear in many applications, e.g. due to curved material interfaces or due to mesh motion in simulations with moving meshes (for example, ALE methods). The use of high-order geometry representation can lead to better feature resolution, improved symmetry preservation, better solution adaptivity and increased robustness for problems where the mesh is being deformed in time. The CEED software supports arbitrary order quadrilateral, hexahedral, triangular and tetrahedral meshes, and our team members are developing algorithms for high-order mesh optimization and solution transfer between deformed high-order meshes (remap). This is an active area of research for our team and we are interested in collaboration.","title":"High-Order Meshes "},{"location":"meshing/#unstructured-amr","text":"Adaptive Mesh Refinement (AMR) on unstructured grids is an important enabling technology for many applications. The CEED software supports both conforming AMR on triangular and tetrahedral meshes as well as general non-conforming AMR on quadrilateral and hexahedral meshes. Both approaches work for low-order meshes with no performance penalty and support parallel dynamic load balancing. One of the distinguishing characteristics of our AMR approach is that it is implemented at the level of the discretization library, decoupled from the physics, so it can easily be incorporated in a variety of applications. We support spaces in the whole de Rham complex, as well as AMR on high-order meshes. CEED scientists are working on practical error indicators and physics-conservative AMR interpolations to meet application needs. This is an active area of research for our team and we are interested in collaboration.","title":"Unstructured AMR "},{"location":"mfem/","text":"MFEM MFEM is a free, lightweight, scalable C++ library for finite element methods. Its goal is to enable high-performance scalable finite element discretization research and application development on a wide variety of platforms, ranging from laptops to exascale supercomputers. Its features include: Arbitrary high-order finite element meshes and spaces . Wide variety of finite element discretization approaches. Conforming and nonconforming adaptive mesh refinement . Scalable to hundreds of thousands of cores. ... and many more . MFEM is being developed in CASC , LLNL and is freely available under LGPL 2.1. In CEED, MFEM is a main component of the efforts in the Applications and Finite Element thrusts. For more information, see the MFEM website: http://mfem.org .","title":"MFEM"},{"location":"mfem/#mfem","text":"MFEM is a free, lightweight, scalable C++ library for finite element methods. Its goal is to enable high-performance scalable finite element discretization research and application development on a wide variety of platforms, ranging from laptops to exascale supercomputers. Its features include: Arbitrary high-order finite element meshes and spaces . Wide variety of finite element discretization approaches. Conforming and nonconforming adaptive mesh refinement . Scalable to hundreds of thousands of cores. ... and many more . MFEM is being developed in CASC , LLNL and is freely available under LGPL 2.1. In CEED, MFEM is a main component of the efforts in the Applications and Finite Element thrusts. For more information, see the MFEM website: http://mfem.org .","title":"MFEM "},{"location":"miniapps/","text":"CEED Miniapps CEED is developing a variety of miniapps encapsulating key physics and numerical kernels of high-order applications. The miniapps are designed to be used in a variety of co-design activities with ECP vendors, software technologies projects as well as external partners. CEED's current miniapps are documented below and can be found on GitHub . Please contact the CEED team if you have any questions. Nekbone Nekbone solves a standard Poisson equation using a conjugate gradient iteration with a simple or spectral element multigrid preconditioner on a block or linear geometry. It exposes the principal computational kernel to reveal the essential elements of the algorithmic-architectural coupling that is pertinent to Nek5000 , relevant to large eddy simulation (LES) and direct numerical simulation (DNS) of turbulence in complex domains. Nekbone-3.1 was released in Oct 2013 as part of the CESAR co-design center . Documentation Download Nekbone Laghos Laghos solves the time-dependent Euler equation of compressible gas dynamics in a moving Lagrangian frame using high-order finite element spatial discretization and explicit high-order time-stepping. It captures the basic structure of many other compressible shock hydrocodes, including the BLAST code at Lawrence Livermore National Laboratory . The miniapp is build on top of a general discretization library, MFEM , thus separating the pointwise physics from finite element and meshing concerns. Computational motives in Laghos include the following: Support for moving (high-order) unstructured 2D and 3D meshes. Continuous and discontinuous high-order finite element discretization spaces of runtime-specified order. Explicit time-stepping loop with a variety of time integrator options. Laghos supports Runge-Kutta ODE solvers of orders 1, 2, 3, 4 and 6. Full and partial assembly options for deriving and solving the resulting ODE systems. Point-wise definition of mesh size, time-step estimate and artificial viscosity coefficient. Domain-decomposed MPI parallelism. Optional in-situ visualization with GLVis and data output for visualization / data analysis with VisIt . Documentation Download Laghos NekCEM CEEDling NekCEM CEEDling is a NekCEM miniapp, solving the time-domain Maxwell equations for electromagnetic systems. It allows software and hardware developers to understand the basic structure and computational costs of NekCEM over a broad spectrum of architectures ranging from software-based simulators running at one ten-thousandth the speed of current processors to exascale platforms running millions of times faster than single-core platforms. NekCEM provides flexibility to adapt new programming approaches for scalability and performance studies on a variety of platforms without having to understand all the features of NekCEM, currently supporting CPU/GPU runs. Download NekCEM CEEDling HPGMG-FE The High-Performance Geometric Multigrid Finite Element benchmark is a Full Multigrid solver for a third order accurate finite element discretization of an elliptic operator on mapped grids. The benchmark is designed to give a picture of the \"performance spectrum\" across a range of problem sizes and thus solution time, thereby giving a clear picture of both weak and strong scaling concerns. For details, see the HPGMG website . MFEM Example Codes The MFEM distribution includes a large number of simple example codes that can be viewed as simple miniapps for model high-order physics: Example 1 : nodal H1 FEM for the Laplace problem, Example 2 : vector FEM for linear elasticity, Example 3 : Nedelec H(curl) FEM for the definite Maxwell problem, Example 4 : Raviart-Thomas H(div) FEM for the grad-div problem, Example 5 : mixed pressure-velocity FEM for the Darcy problem, Example 6 : non-conforming adaptive mesh refinement (AMR) for the Laplace problem, Example 7 : Laplace problem on a surface (the unit sphere), Example 8 : Discontinuous Petrov-Galerkin (DPG) for the Laplace problem, Example 9 : Discontinuous Galerkin (DG) time-dependent advection, Example 10 : time-dependent implicit nonlinear elasticity, Example 11 : parallel Laplace eigensolver, Example 12 : parallel linear elasticity eigensolver, Example 13 : parallel Maxwell eigensolver, Example 14 : Discontinuous Galerkin (DG) for the Laplace problem, Example 15 : dynamic AMR for Laplace with prescribed time-dependent source, Example 16 : time-dependent nonlinear heat equation, Example 17 : Discontinuous Galerkin (DG) for linear elasticity, Example 18 : Discontinuous Galerkin (DG) for the Euler equations, Example 19 : incompressible nonlinear elasticity. Most of the examples have a serial and a parallel version, illustrating the ease of transition and the minimal code changes between the two. Of particular relevance to CEED is the HPC versions of the example codes that use a set of templated classes to efficiently implement high-order operator evaluation. Documentation Download MFEM","title":"Miniapps"},{"location":"miniapps/#ceed-miniapps","text":"CEED is developing a variety of miniapps encapsulating key physics and numerical kernels of high-order applications. The miniapps are designed to be used in a variety of co-design activities with ECP vendors, software technologies projects as well as external partners. CEED's current miniapps are documented below and can be found on GitHub . Please contact the CEED team if you have any questions.","title":"CEED Miniapps"},{"location":"miniapps/#nekbone","text":"Nekbone solves a standard Poisson equation using a conjugate gradient iteration with a simple or spectral element multigrid preconditioner on a block or linear geometry. It exposes the principal computational kernel to reveal the essential elements of the algorithmic-architectural coupling that is pertinent to Nek5000 , relevant to large eddy simulation (LES) and direct numerical simulation (DNS) of turbulence in complex domains. Nekbone-3.1 was released in Oct 2013 as part of the CESAR co-design center . Documentation Download Nekbone","title":"Nekbone"},{"location":"miniapps/#laghos","text":"Laghos solves the time-dependent Euler equation of compressible gas dynamics in a moving Lagrangian frame using high-order finite element spatial discretization and explicit high-order time-stepping. It captures the basic structure of many other compressible shock hydrocodes, including the BLAST code at Lawrence Livermore National Laboratory . The miniapp is build on top of a general discretization library, MFEM , thus separating the pointwise physics from finite element and meshing concerns. Computational motives in Laghos include the following: Support for moving (high-order) unstructured 2D and 3D meshes. Continuous and discontinuous high-order finite element discretization spaces of runtime-specified order. Explicit time-stepping loop with a variety of time integrator options. Laghos supports Runge-Kutta ODE solvers of orders 1, 2, 3, 4 and 6. Full and partial assembly options for deriving and solving the resulting ODE systems. Point-wise definition of mesh size, time-step estimate and artificial viscosity coefficient. Domain-decomposed MPI parallelism. Optional in-situ visualization with GLVis and data output for visualization / data analysis with VisIt . Documentation Download Laghos","title":"Laghos"},{"location":"miniapps/#nekcem-ceedling","text":"NekCEM CEEDling is a NekCEM miniapp, solving the time-domain Maxwell equations for electromagnetic systems. It allows software and hardware developers to understand the basic structure and computational costs of NekCEM over a broad spectrum of architectures ranging from software-based simulators running at one ten-thousandth the speed of current processors to exascale platforms running millions of times faster than single-core platforms. NekCEM provides flexibility to adapt new programming approaches for scalability and performance studies on a variety of platforms without having to understand all the features of NekCEM, currently supporting CPU/GPU runs. Download NekCEM CEEDling","title":"NekCEM CEEDling"},{"location":"miniapps/#hpgmg-fe","text":"The High-Performance Geometric Multigrid Finite Element benchmark is a Full Multigrid solver for a third order accurate finite element discretization of an elliptic operator on mapped grids. The benchmark is designed to give a picture of the \"performance spectrum\" across a range of problem sizes and thus solution time, thereby giving a clear picture of both weak and strong scaling concerns. For details, see the HPGMG website .","title":"HPGMG-FE"},{"location":"miniapps/#mfem-example-codes","text":"The MFEM distribution includes a large number of simple example codes that can be viewed as simple miniapps for model high-order physics: Example 1 : nodal H1 FEM for the Laplace problem, Example 2 : vector FEM for linear elasticity, Example 3 : Nedelec H(curl) FEM for the definite Maxwell problem, Example 4 : Raviart-Thomas H(div) FEM for the grad-div problem, Example 5 : mixed pressure-velocity FEM for the Darcy problem, Example 6 : non-conforming adaptive mesh refinement (AMR) for the Laplace problem, Example 7 : Laplace problem on a surface (the unit sphere), Example 8 : Discontinuous Petrov-Galerkin (DPG) for the Laplace problem, Example 9 : Discontinuous Galerkin (DG) time-dependent advection, Example 10 : time-dependent implicit nonlinear elasticity, Example 11 : parallel Laplace eigensolver, Example 12 : parallel linear elasticity eigensolver, Example 13 : parallel Maxwell eigensolver, Example 14 : Discontinuous Galerkin (DG) for the Laplace problem, Example 15 : dynamic AMR for Laplace with prescribed time-dependent source, Example 16 : time-dependent nonlinear heat equation, Example 17 : Discontinuous Galerkin (DG) for linear elasticity, Example 18 : Discontinuous Galerkin (DG) for the Euler equations, Example 19 : incompressible nonlinear elasticity. Most of the examples have a serial and a parallel version, illustrating the ease of transition and the minimal code changes between the two. Of particular relevance to CEED is the HPC versions of the example codes that use a set of templated classes to efficiently implement high-order operator evaluation. Documentation Download MFEM","title":"MFEM Example Codes"},{"location":"nek/","text":"Nek Nek5000/NekCEM is an open-source simulation-software package that delivers highly accurate solutions for a wide range of scientific applications, including fluid flow, thermal convection, combustion, magnetohydrodynamics, and electromagnetics. It features state-of-the-art, scalable, high-order spectral element-based algorithms that are fast and efficient on platforms ranging from laptops to the world\u2019s fastest computers. Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and industry. We highlight a few of the many applications using Nek5000/NekCEM today. Reactor analysis Aerospace application Engine application Ocean modeling Turbulence modeling Electromagnetics modeling Drift-diffusion modeling Nek5000 and NekCEM are freely available under a BSD license. In CEED, Nek is a main component of the efforts in the Applications and Finite Element thrusts. For more information, see the Nek5000 website: https://nek5000.mcs.anl.gov .","title":"Nek"},{"location":"nek/#nek","text":"Nek5000/NekCEM is an open-source simulation-software package that delivers highly accurate solutions for a wide range of scientific applications, including fluid flow, thermal convection, combustion, magnetohydrodynamics, and electromagnetics. It features state-of-the-art, scalable, high-order spectral element-based algorithms that are fast and efficient on platforms ranging from laptops to the world\u2019s fastest computers. Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and industry. We highlight a few of the many applications using Nek5000/NekCEM today. Reactor analysis Aerospace application Engine application Ocean modeling Turbulence modeling Electromagnetics modeling Drift-diffusion modeling Nek5000 and NekCEM are freely available under a BSD license. In CEED, Nek is a main component of the efforts in the Applications and Finite Element thrusts. For more information, see the Nek5000 website: https://nek5000.mcs.anl.gov .","title":"Nek "},{"location":"news/","text":"News Software release: MFEM v4.0 Version 4.0 of MFEM was released on May 24, 2019. Some of the new additions in this release are: Added initial support for hardware devices, such as GPUs, and programming models, such as CUDA, OCCA, RAJA and OpenMP. Added support for fast operator evaluation based on different assembly level representations , e.g. partial assembly . Added support for wedge elements and meshes with mixed element types. Five new examples and miniapps. For more details, see the interactive documentation at http://mfem.org and the full CHANGELOG . Third CEED annual meeting in Virginia Tech CEED will hold its third annual meeting August 6-8, 2019 in Virginia Tech. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of ECP applications, hardware vendors, software technology and other interested projects. See the meeting page for additional information. Software release: CEED v2.0 The CEED team released CEED 2.0 consisting of 12 integrated Spack packages for libCEED , mfem , nek5000 , nekcem , laghos , nekbone , hpgmg , occa , magma , gslib , petsc and pumi plus a new CEED meta-package . With Spack , a user can install the whole CEED software stack simply with: spack install ceed . Visit the CEED 2.0 release page for further information about the release and installation help for various systems. Software release: libCEED v0.4 Version 0.4 of libCEED was released in March, 2019. The release includes new CPU and GPU backends CPU backend optimizations initial support for operator composition performance benchmarking a Navier-Stokes demo Laghos CTS-2 benchmark The CEED team at LLNL developed a version of the Laghos miniapp to be used in the second edition of the Commodity Technology Systems procurement process. These systems leverage industry advances and open source software standards to build, field, and integrate Linux clusters of various sizes into production service. The CTS-2 version of Laghos features RAJA backend, improved robustness and figure of merit computation, and is available at https://github.com/CEED/Laghos/releases/tag/cts2. CEED researchers at NAHOMCon19 With the next International Conference on Spectral and High-Order Methods (ICOSAHOM) meetings scheduled for 2020 in Vienna and 2022 in Seoul, it will be at least 10 years between US-based settings of the principal high-order methods conference, ICOSAHOM. Because of the growing importance of high-order methods, several institutions have joined together to organize the inaugural North American High Order Methods Conference (NAHOMCon19) to be held in San Diego in summer of 2019, that will focus on the many developments in high-order discretizations and applications that are taking place in North America. Several CEED researchers will participate in the conference, including Misun Min and Tzanio Kolev, who will present two of the plenary talks, and Paul Fischer and Tim Warburton, who are serving on the scientific committee. Software release: Laghos 2.0 Version 2.0 of the Laghos miniapp was released on November 19, 2018. The release includes CUDA, RAJA, OCCA and AMR versions of Laghos in the cuda/ , raja/ , occa/ and amr/ directories. New example: Gresho vortex. Added a conservative time integrator (RK2Avg) and computation of total energy. Improved the computations of the matrix diagonal by contracting the squares of the B matrices. Added diagonal preconditioners for both partial and full assembly. Support the Bernstein positive basis When using partial assembly for the velocity. Travis CI regression testing on GitHub. CEED minisymposiums at SIAM CSE19 CEED researchers are organizing two minisymposiums at the SIAM Conference on Computational Science and Engineering (CSE19) in Spokane, Washington, Feb 25 - Mar 1, 2019: Exascale Applications with High-Order Methods : Part 1 and Part 2 Exascale Software for High-order Methods : Part 1 and Part 2 The minisymposiums will focus on high-order applications, discretization algorithms, and lightweight portable libraries targeting performance on exascale hardware. MFEM part of E4S-0.1 MFEM was included in the first release of the Extreme-Scale Scientific Software Stack (E4S) software collection developed by the Software Technologies focus area of the ECP. According to the E4S website , the primary purpose of this initial 0.1 release was to demonstrate the release approach based on Spack package maturity. Future release will include additional software products, with the ultimate goal of including all ECP ST software products. CEED researchers at ATPESC18 Several CEED researchers presented at the 2018 edition of the Argonne Training Program on Extreme-Scale Computing , which is also part of the Exascale Computing Project. The CEED presentations covered a wide variety of topics, from meshing, to GPU programming, dense and sparse linear algebra, and high-order discretizations on unstructured meshes. Videos of all 2018 talks are available on YouTube . Software release: libCEED 0.3 Version 0.3 of libCEED was released on September 30, 2018. The release includes New interface enabling active and passive fields Performance improvements and element vectorization Streamlined Fortran interface Improved testing and coverage Second CEED annual meeting in CU Boulder CEED held its second annual meeting August 8-10, 2018 at the University of Colorado Boulder. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of ECP applications, hardware vendors, software technology and other interested projects. See the meeting page for additional information. Software release: libParanumal v0.1.0 The inaugural version 0.1.0 of libParanumal was released on July 31, 2018. The release includes Incompressible flow solver. Compressible flow solver. Galerkin-Boltzmann kinetic flow solver. Discontinuous Galerkin and continuous Galerkin spatial discretizations. A range of time steppers custom chosen for each flow solver. Support for meshes consisting of triangles, quadrilaterals, tetrahedra, or hexahedra up to at least degree 10. OCCA 1.0 based computational kernels. MPI based distributed computing Software release: MAGMA v2.4.0 Version 2.4.0 of MAGMA was released on June 25, 2018. The release includes: Performance improvements across many batch routines, including batched TRSM, batched LU, batched LU-nopiv, and batched Cholesky Constrained least squares routines (magma_[sdcz]gglse) and dependencies Fixed some compilation issues with inf, nan, and nullptr. For further details and download, see MAGMA Download . Software release: OCCA v1.0 Version 1.0 of OCCA was released on June 13, 2018. The main focus for the release included: Updating the API to expose backend-specific features in a generic way New OKL Parser better suited for language transforms and error handling Useful links: v0.2 -> v1.0 porting guide libocca.org for more info about OCCA Software release: MFEM v3.4 Version 3.4 of MFEM was released on May 29, 2018. Some of the new additions in this release are: Significantly improved non-conforming unstructured AMR scalability. Integration with PUMI , the Parallel Unstructured Mesh Infrastructure from RPI. Block nonlinear operators and variable order NURBS. Conduit Mesh Blueprint support. General \"high-order\"-to-\"low-order refined\" field transfer. New specialized time integrators (symplectic, generalized-alpha). Twelve new examples and miniapps. For more details, see the interactive documentation and the full CHANGELOG at http://mfem.org . 6th Nek5000 User Meeting held at U Florida The 6th Nek5000 User/Developer Meeting was held in Tampa, FL, April 17-18, 2018. The event was hosted by the DOE Center for Multiphase Turbulence, which is headed by Prof. S. Balachandar at the University of Florida. Thirty five researchers from industry, national labs, and American, Canadian, and European universities attended the event, which featured twenty three presentations and extensive discussions about current and new trends in Nek5000 development. Next month will mark the 10th anniversary of Nek5000 going open source. Software release: CEED v1.0 The CEED team released its first software distribution, CEED 1.0 consisting of 12 integrated Spack packages for libCEED , mfem , nek5000 , nekcem , laghos , nekbone , hpgmg , occa , magma , gslib , petsc and pumi plus a new CEED meta-package . With Spack , a user can install the whole CEED software stack simply with: spack install ceed . As part of CEED 1.0, the team developed comprehensive documentation including software and compiler configurations for ALCF , OLCF , NERSC and LLNL : Platform Architecture Spack Configuration Mac darwin-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Cori (NERSC) cray-CNL-haswell packages Edison (NERSC) cray-CNL-ivybridge packages Theta (ALCF) cray-CNL-mic_knl packages Titan (OLCF) cray-CNL-interlagos packages CORAL-EA (LLNL) blueos_3_ppc64le_ib packages compilers TOSS3 (LLNL) toss_3_x86_64_ib packages compilers For more information visit http://ceed.exascaleproject.org/ceed-1.0 . Software release: libCEED v0.2 Version 0.2 of libCEED , the CEED API library, was released in March 2018 with major improvements in the OCCA backend. libCEED is a lightweight portable library that allows, for the first time, a wide variety of applications (written in C, C++, Fortran) to share a wide variety of discretization kernels (CPU, GPU, OpenMP, OpenCL), including high-performance GPU kernels. libCEED comes with several examples of its usage, ranging from standalone C codes in the /examples/ceed directory to examples based on external packages, such as MFEM , PETSc and Nek5000 . Below is an illustration how libCEED enables these very different codes (C++, C, F77) to take advantage of a common set of GPU kernels (see also the CEED 1.0 GPU demo ): # libCEED examples on CPU and GPU cd examples/ceed make ./ex1 -ceed /cpu/self ./ex1 -ceed /gpu/occa cd ../.. # MFEM+libCEED examples on CPU and GPU cd examples/mfem make ./bp1 -ceed /cpu/self -no-vis ./bp1 -ceed /gpu/occa -no-vis cd ../.. # PETSc+libCEED examples on CPU and GPU cd examples/petsc make ./bp1 -ceed /cpu/self ./bp1 -ceed /gpu/occa cd ../.. # Nek+libCEED examples on CPU and GPU cd examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self -b 3 ./run-nek-example.sh -ceed /gpu/occa -b 3 cd ../.. For more information visit https://github.com/CEED/libCEED . Panayot Vassilevski named SIAM fellow Panayot Vassilevski , who is part of the CEED team at LLNL, was named a 2018 SIAM fellow for his work on designing algebraic approaches for creating and analyzing multilevel algorithms . Panayot is the editor-in-chief for Numerical Linear Algebra with Applications and has published many papers and a monograph on Multilevel Block Factorization Preconditioners . In CEED, Panayot is working on matrix-free preconditioners for high-order finite element discretizations. Congratulations Panayot! Benchmark release by Paranumal team The CEED group at Virginia Tech released standalone implementations of CEED's BP1.0, BP3.0, and BP3.5 benchmark problems . For results and discussion, see the (\"CEED Code Competition: VT software release\")[https://www.paranumal.com/single-post/2018/02/01/CEED-Code-Competition-bake-off-problems] entry in the Paranumal blog . Workshop on Batched, Reproducible, and Reduced Precision BLAS CEED's [UTK team]](magma.md) organized a two-session minisymposium at the SIAM Conference on Parallel Processing and Scientific Computing in Tokyo, Japan from March 7-10, 2018, devoted on Batched BLAS Standardization . The minisymposium is part of our efforts on standardization and co-design of exascale discretization APIs with application developers, hardware vendors and ECP software technologies projects. The goal is to extend the BLAS standard to include batched BLAS computational patterns/\"application motifs\" that are essential for representing and implementing tensor contractions. Besides participation from the CEED project, stakeholders from ORNL, Sandia, NVidia, Intel, IBM, and Universities were invited. New website launched by the Virginia Tech CEED team A new website was recently launched by the Parallel Numerical Algorithms (Paranumal) research group at Virginia Tech here . The site includes a blog that gives some practical computing tips related to high performance implementations of finite element methods developed as part of the CEED project here . Initial release of libCEED: The CEED API library The initial version of libCEED , the CEED API library, was released in December 2017. libCEED is a high-order API library, that for the first time provides a common operator description on algebraic level , that allows a wide variety of applications to take advantage of the efficient operator evaluation algorithms in the different CEED packages (from a single source). Our long-term vision for libCEED is to include a variety of back-end implementations, ranging from simple reference kernels, to highly optimized kernels targeting specific devices (e.g. GPUs) or specific polynomial orders. For more information visit https://github.com/CEED/libCEED . Software release: MFEM v3.3.2 Version 3.3.2 of MFEM was released on November 10, 2017. Some of the new additions in this release are: Support for high-order mesh optimization based on the target-matrix optimization paradigm from the ETHOS project . Implementation of the community policies in xSDK, the Extreme-scale Scientific Software Development Kit . Integration with the STRUMPACK parallel sparse direct solver and preconditioner. Several new linear interpolators , five new examples and miniapps. Various memory, performance, discretization and solver improvements, including physical-to-reference space mapping capabilities. Continuous integration testing on Linux, Mac and Windows. For more details, see the interactive documentation and the full CHANGELOG at http://mfem.org . CEED participates in xSDK and FASTMath MFEM joined xSDK, the Extreme-scale Scientific Software Development Kit in ECP's software technologies focus area as of release xSDK-0.3.0, see https://xsdk.info/packages . MFEM and PUMI are also part of the FASTMath institute in the SciDAC program, see https://fastmath-scidac.llnl.gov/software-catalog.html . Software release: Nek5000 v17.0 Nek5000 version 17.0 was released as a major upgrade to Nek5000. Major features improvements include: Refactored build system. New user-input parameter file format ( .par replacing .rea ). Characteristics (large time-step) support for moving mesh problems. Moving mesh support for the $PN-PN$ formulation. Improved stability for $PN-PN$ with variable viscosity. Support for mixed Helmholtz/CVODE solves. New fast AMG setup tool based on HYPRE. New EXODUSII mesh converter. New interface to libxsmm (fast MATMUL library). Extended lowMach solver for time varying thermodynamic pressure. Added DG for scalars. Reduced solver initialization time (parallel binary reader for all input files). Automatic general mesh-to-mesh transfer for restarts. Refactored support for overlapping domains (NekNek). Added high-pass filter relaxation (alternative to explicit filter). Refactored residual projection including support for coupled Helmholtz solves. Nekbone and Laghos join proxy app suites The Nekbone and Laghos miniapps developed in CEED were selected to be part of ECP's initial Proxy Applications Suite . Both miniapps were also picked to be CORAL-2 benchmarks . Laghos was also selected as one of LLNL's ASC co-design miniapps . 6th Nek5000 User Meeting to be held at U Florida The 6th Nek5000 User/Developer Meeting will be hosted by the DOE PSAAP-II Compressible Multiphase Turbulence (CMT) center in Tampa, FL, March 17-18, 2018. Nek5000 hackathon at UIUC The inaugural Nek5000 Hackathon was held at NCSA Building, University of Illinois, Urbana-Champaign (UIUC), IL on Nov 12-14, 2017. The event was attended by researchers and Nek5000 developers to promote the application of Nek5000 to new problems from industry, national laboratories, and academia. Twenty-five participants spent three days working on setting up new examples, developing new features, and helping one another to get maximum performance on their applications. Some of the more prominent exchanges of ideas included standardization of synthetic turbulent inflow techniques, use of CVODE for pure advection-diffusion problems, and the use of the characteristics methods for moving geometry applications. For more details, see the Nek5000 hackathon website . CEED organizing minisymposium at ICOSAHOM 2018 CEED is organizing a minisymposium, Efficient High-Order Finite Element Discretizations at Large Scale , at the International Conference on Spectral and High-Order Methods (ICOSAHOM 2018) in London UK, Jul 9-13, 2018. The goal of the minisymposium is to discuss the next-generation high-order discretization algorithm and software, based on finite/spectral element approaches that will enable a wide range of important scientific applications to run efficiently on future architecture. Best Paper Award at NURETH-17 CEED researchers (P. Fischer, E. Merzari, A. Obabko) won a Best Paper Award at the 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17), held in China in September 2017, with a paper entitled High-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors . CEED attending Cray, AMD and Intel Deep-Dives CEED researchers and representatives of the Nek and MFEM teams will attend the October 2017, ECP vendor deep-dive meetings: Cray deep-dive in Bloomington, MN on Oct 18-19 AMD deep-dive in Austin, TX on Oct 24-25 Intel deep-dive in Hudson, MA on Oct 21-Nov 2 Topics of discussion include advanced technology and memory design, strong scaling considerations and the porting and evaluation of CEED's bake-off problems and miniapps (Nekbone, Laghos, NekCEM CEEDling). New Nekbench repository New Nekbench repository has been released to provide scripts that simplify the benchmarking of Nek5000 . The user provides ranges for important parameters ranges (e.g., processor counts and local problem size ranges) and a test type (e.g., scaling or ping-pong test). Nekbench will run the given test in the given parameter space using a Nek5000 case file which is also given by the user (in the ping-pong tests, the case file is optional). Nekbench is written using bash scripting language and runs any Unix-like operating system that supports bash. It has been successfully tested on Linux laptops/desktops, ALCF Theta, NERSC Cori (KNL and Haswell), and NERSC Edison machines for scaling tests. Planned extensions for Nekbench include adding more machine types like ANL's Cetus, additional support for the ping-pong test type, and automated plot generation (e.g., scaling study graphs) for each test run. GPU ports of Nek and Laghos GPU acceleration is a main focus of the performance optimization efforts in CEED. Recent progress in this direction include GPU ports of CEED's Nek5000 application and the Nekbone and Laghos miniapps . For Nek5000, an initial GPU-enabled version has been developed based on OpenACC. For Nekbone a pure OpenACC implementation as well as a hybrid OpenACC/CUDA implementation with a CUDA kernel for matrix-vector multiplication has been developed . For Laghos, a GPU-enabled version of has been released using the OCCA interface. With this approach, the user is able to run Laghos distributively using varying device types per MPI process, whether serial C++, OpenMP, or CUDA. First CEED annual meeting held at LLNL CEED held its first annual meeting in August, 2017 at the HPC Innovation Center of Lawrence Livermore National Laboratory. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of the ECP management, hardware vendors, software technology and other interested projects. CEED researchers at ATPESC17 Six CEED researchers presented at the 2017 edition of the Argonne Training Program on Extreme-Scale Computing , now part of the Exascale Computing Project. The CEED presentations covered a wide variety of topics, from overview of Theta, to GPU programming, dense and sparse linear algebra, and high-order discretizations on unstructured meshes. Videos of all 2017 talks are available on YouTube . CEED researchers have also participated in past editions of the meeting. CEED BPs and benchmarks repository released CEED released an initial set of bake-off (BP) problems , which are simple kernels designed to test and compare the performance of high-order codes, both internally in CEED, as well as in the broader high-order community . In addition to the benchmark descriptions on the CEED BPs page , a benchmarks repository is publicly available with several implementations of the CEED bake-off problems. Currently, MFEM, Nek5000 and deal.ii are included, see directories tests/mfem_bps , tests/nek5000_bps and tests/dealii_bps respectively. New Laghos and NekCEM CEEDling miniapps released Two new miniapps developed in CEED were released in June 2017: Laghos and NekCEM CEEDling. Laghos (LAGrangian High-Order Solver) is a new miniapp developed in CEED that solves the time-dependent Euler equations of compressible gas dynamics in a moving Lagrangian frame using unstructured high-order finite element spatial discretization and explicit high-order time-stepping. In CEED, Laghos serves as a proxy for a sub-component of the MARBL/LLNLApp application. NekCEM CEEDling is a new NekCEM miniapp, solving the time-domain Maxwell equation for electromagnetic systems. For more details, see the CEED miniapps page and the Laghos and NekCEM CEEDling repositories on GitHub. Paper with MPICH at SC17 Joint paper with the MPICH group, Why is MPI so Slow? Analyzing the fundamental limits in implementing MPI-3.1 accepted in Supercomputing 2017 . The paper provides an in-depth analysis of the software overheads in the MPI performance-critical path and exposes mandatory performance overheads that are unavoidable based on the MPI-3.1 specification. STRUMPACK support in MFEM Support for the sparse direct solver and preconditioner STRUMPACK has been integrated in MFEM. STRUMPACK is being developed at LBNL and is part of the ECP project Factorization Based Sparse Solvers and Preconditioners (Xiaoye Sherry Li and Pieter Ghysels). The STRUMPACK solver is based on multifrontal sparse Gaussian elimination and uses hierarchically semi-separable matrices to compress fill-in. It can be used as an exact direct solver or as an algebraic, robust and parallel preconditioner for a range of discretized PDE problems. 2017 PETSc User Meeting Over 75 participants from all over the world attended the PETSc User Meeting, held June 14-16 in Boulder, CO. Hosted by the University of Colorado Boulder, the event consisted of a one-day tutorial on the solver library PETSc and showcased the latest research enabled by the functionality available in PETSc. The meeting agenda covered a total of 15 talks, four posters, and two panels. Thanks to generous support from Intel and Tech-X, 22 students received travel grants and got to learn about the latest techniques on the large-scale numerical solution of partial differential equations. PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. It has become one of the most widely used numerical software packages of its kind and has users in application areas ranging from acoustics and arterial flow to seismology and semiconductors. GPU Hackathon at BNL Nek/CEED team participated the GPU Hackathon 2017 that was held in Brookhaven National Laboratory on June 5-9, 2017. Our team focused on performing and tuning GPU-enabled Nek5000/Nekbone/NekCEM version on large-scale GPU systems for small modular reactor, thermal fluids, and meta-materials modeling. Workshop on Batched, Reproducible, and Reduced Precision BLAS The second Workshop on Batched, Reproducible, and Reduced Precision BLAS was held in Atlanta, GA on February 23-25, 2017 including many members of the CEED MAGMA team. The goal of this workshop was to touch on extending the Basic Linear Algebra Software Library (BLAS). The existing BLAS have proven to be very effective in assisting portable, efficient software for sequential and some of the current class of high-performance computers. New computational needs in many applications have motivated the need to investigate the possibility of extending the currently accepted standards to provide greater parallelism for small size operations, reproducibility, and reduced precision support. Of particular interest to CEED is the use of batched BLAS for finite element tensor contractions, and thus our team is interested in the establishment of a batched BLAS standard, highly-optimized implementations, and support from vendors on various architectures. This is the second workshop of an open forum to discuss and formalize details related to batched, reproducible, and reduced precision BLAS. The agenda and the talks from the first workshop can be found here . Software release: MFEM v3.3 Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element methods and a main partner in CEED, was released on January 28, 2017 at http://mfem.org The goal of MFEM is to enable high-performance scalable finite element discretization research and application development on a wide variety of platforms, ranging from laptops to exascale supercomputers. It has many features, including: 2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements. Parallel version scalable to hundreds of thousands of MPI cores. Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing. Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations. Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes. Scalable algebraic multigrid, time integrators, and eigensolvers. Lightweight interactive OpenGL visualization with the MFEM-based GLVis tool. Some of the new additions in version 3.3 are: Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features from the PETSc and SUNDIALS suites. Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces. General quadrature and nodal finite element basis types. Scalable parallel mesh format. Thirty six new integrators for common families of operators. Sixteen new serial and parallel example codes. Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the Conduit mesh blueprint specification. MFEM is being developed in CASC , LLNL and is freely available under LGPL 2.1. For more details, see the interactive documentation and the full CHANGELOG . CEED co-design center announced The Exascale Computing Project (ECP) announced on November 11, 2016 its selection of four co-design centers , including CEED: the Center for Efficient Exascale Discretizations, which is a research partnership between Lawrence Livermore National Laboratory; Argonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee, Knoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI). Additional news coverage can be found in LLNL Newsline and the ANL press release . R&D 100 Award for NekCEM / Nek5000 NekCEM/Nek5000: Scalable High-Order Simulation Codes received a 2016 R&D 100 Award, given by R&D Magazine to 100 top new technologies for the year. The R&D 100 citation reads: \"NekCEM/Nek5000: Release 4.0: Scalable High-Order Simulation Codes is an open-source simulation-software package that delivers highly accurate solutions for a wide range of scientific applications including electromagnetics, quantum optics, fluid flow, thermal convection, combustion and magnetohydrodynamics. It features state-of-the-art, scalable, high-order algorithms that are fast and efficient on platforms ranging from laptops to the world\u2019s fastest computers. The size of the physical phenomena that can be simulated with this package ranges from quantum dots for nanoscale devices to accretion disks surrounding black holes. NekCEM provides simulation capabilities for the analysis of electromagnetic and quantum optical devices, such as particle accelerators and solar cells. Nek5000 provides turbulent flow simulation capabilities for a variety of thermal-fluid problems including nuclear reactors, internal combustion engines, vascular flows, and ocean currents.\" See the ANL press release for more information.","title":"News"},{"location":"news/#news","text":"","title":"News"},{"location":"news/#software-release-mfem-v40","text":"Version 4.0 of MFEM was released on May 24, 2019. Some of the new additions in this release are: Added initial support for hardware devices, such as GPUs, and programming models, such as CUDA, OCCA, RAJA and OpenMP. Added support for fast operator evaluation based on different assembly level representations , e.g. partial assembly . Added support for wedge elements and meshes with mixed element types. Five new examples and miniapps. For more details, see the interactive documentation at http://mfem.org and the full CHANGELOG .","title":"Software release: MFEM v4.0"},{"location":"news/#third-ceed-annual-meeting-in-virginia-tech","text":"CEED will hold its third annual meeting August 6-8, 2019 in Virginia Tech. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of ECP applications, hardware vendors, software technology and other interested projects. See the meeting page for additional information.","title":"Third CEED annual meeting in Virginia Tech"},{"location":"news/#software-release-ceed-v20","text":"The CEED team released CEED 2.0 consisting of 12 integrated Spack packages for libCEED , mfem , nek5000 , nekcem , laghos , nekbone , hpgmg , occa , magma , gslib , petsc and pumi plus a new CEED meta-package . With Spack , a user can install the whole CEED software stack simply with: spack install ceed . Visit the CEED 2.0 release page for further information about the release and installation help for various systems.","title":"Software release: CEED v2.0"},{"location":"news/#software-release-libceed-v04","text":"Version 0.4 of libCEED was released in March, 2019. The release includes new CPU and GPU backends CPU backend optimizations initial support for operator composition performance benchmarking a Navier-Stokes demo","title":"Software release: libCEED v0.4"},{"location":"news/#laghos-cts-2-benchmark","text":"The CEED team at LLNL developed a version of the Laghos miniapp to be used in the second edition of the Commodity Technology Systems procurement process. These systems leverage industry advances and open source software standards to build, field, and integrate Linux clusters of various sizes into production service. The CTS-2 version of Laghos features RAJA backend, improved robustness and figure of merit computation, and is available at https://github.com/CEED/Laghos/releases/tag/cts2.","title":"Laghos CTS-2 benchmark"},{"location":"news/#ceed-researchers-at-nahomcon19","text":"With the next International Conference on Spectral and High-Order Methods (ICOSAHOM) meetings scheduled for 2020 in Vienna and 2022 in Seoul, it will be at least 10 years between US-based settings of the principal high-order methods conference, ICOSAHOM. Because of the growing importance of high-order methods, several institutions have joined together to organize the inaugural North American High Order Methods Conference (NAHOMCon19) to be held in San Diego in summer of 2019, that will focus on the many developments in high-order discretizations and applications that are taking place in North America. Several CEED researchers will participate in the conference, including Misun Min and Tzanio Kolev, who will present two of the plenary talks, and Paul Fischer and Tim Warburton, who are serving on the scientific committee.","title":"CEED researchers at NAHOMCon19"},{"location":"news/#software-release-laghos-20","text":"Version 2.0 of the Laghos miniapp was released on November 19, 2018. The release includes CUDA, RAJA, OCCA and AMR versions of Laghos in the cuda/ , raja/ , occa/ and amr/ directories. New example: Gresho vortex. Added a conservative time integrator (RK2Avg) and computation of total energy. Improved the computations of the matrix diagonal by contracting the squares of the B matrices. Added diagonal preconditioners for both partial and full assembly. Support the Bernstein positive basis When using partial assembly for the velocity. Travis CI regression testing on GitHub.","title":"Software release: Laghos 2.0"},{"location":"news/#ceed-minisymposiums-at-siam-cse19","text":"CEED researchers are organizing two minisymposiums at the SIAM Conference on Computational Science and Engineering (CSE19) in Spokane, Washington, Feb 25 - Mar 1, 2019: Exascale Applications with High-Order Methods : Part 1 and Part 2 Exascale Software for High-order Methods : Part 1 and Part 2 The minisymposiums will focus on high-order applications, discretization algorithms, and lightweight portable libraries targeting performance on exascale hardware.","title":"CEED minisymposiums at SIAM CSE19"},{"location":"news/#mfem-part-of-e4s-01","text":"MFEM was included in the first release of the Extreme-Scale Scientific Software Stack (E4S) software collection developed by the Software Technologies focus area of the ECP. According to the E4S website , the primary purpose of this initial 0.1 release was to demonstrate the release approach based on Spack package maturity. Future release will include additional software products, with the ultimate goal of including all ECP ST software products.","title":"MFEM part of E4S-0.1"},{"location":"news/#ceed-researchers-at-atpesc18","text":"Several CEED researchers presented at the 2018 edition of the Argonne Training Program on Extreme-Scale Computing , which is also part of the Exascale Computing Project. The CEED presentations covered a wide variety of topics, from meshing, to GPU programming, dense and sparse linear algebra, and high-order discretizations on unstructured meshes. Videos of all 2018 talks are available on YouTube .","title":"CEED researchers at ATPESC18"},{"location":"news/#software-release-libceed-03","text":"Version 0.3 of libCEED was released on September 30, 2018. The release includes New interface enabling active and passive fields Performance improvements and element vectorization Streamlined Fortran interface Improved testing and coverage","title":"Software release: libCEED 0.3"},{"location":"news/#second-ceed-annual-meeting-in-cu-boulder","text":"CEED held its second annual meeting August 8-10, 2018 at the University of Colorado Boulder. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of ECP applications, hardware vendors, software technology and other interested projects. See the meeting page for additional information.","title":"Second CEED annual meeting in CU Boulder"},{"location":"news/#software-release-libparanumal-v010","text":"The inaugural version 0.1.0 of libParanumal was released on July 31, 2018. The release includes Incompressible flow solver. Compressible flow solver. Galerkin-Boltzmann kinetic flow solver. Discontinuous Galerkin and continuous Galerkin spatial discretizations. A range of time steppers custom chosen for each flow solver. Support for meshes consisting of triangles, quadrilaterals, tetrahedra, or hexahedra up to at least degree 10. OCCA 1.0 based computational kernels. MPI based distributed computing","title":"Software release: libParanumal v0.1.0"},{"location":"news/#software-release-magma-v240","text":"Version 2.4.0 of MAGMA was released on June 25, 2018. The release includes: Performance improvements across many batch routines, including batched TRSM, batched LU, batched LU-nopiv, and batched Cholesky Constrained least squares routines (magma_[sdcz]gglse) and dependencies Fixed some compilation issues with inf, nan, and nullptr. For further details and download, see MAGMA Download .","title":"Software release: MAGMA v2.4.0"},{"location":"news/#software-release-occa-v10","text":"Version 1.0 of OCCA was released on June 13, 2018. The main focus for the release included: Updating the API to expose backend-specific features in a generic way New OKL Parser better suited for language transforms and error handling Useful links: v0.2 -> v1.0 porting guide libocca.org for more info about OCCA","title":"Software release: OCCA v1.0"},{"location":"news/#software-release-mfem-v34","text":"Version 3.4 of MFEM was released on May 29, 2018. Some of the new additions in this release are: Significantly improved non-conforming unstructured AMR scalability. Integration with PUMI , the Parallel Unstructured Mesh Infrastructure from RPI. Block nonlinear operators and variable order NURBS. Conduit Mesh Blueprint support. General \"high-order\"-to-\"low-order refined\" field transfer. New specialized time integrators (symplectic, generalized-alpha). Twelve new examples and miniapps. For more details, see the interactive documentation and the full CHANGELOG at http://mfem.org .","title":"Software release: MFEM v3.4"},{"location":"news/#6th-nek5000-user-meeting-held-at-u-florida","text":"The 6th Nek5000 User/Developer Meeting was held in Tampa, FL, April 17-18, 2018. The event was hosted by the DOE Center for Multiphase Turbulence, which is headed by Prof. S. Balachandar at the University of Florida. Thirty five researchers from industry, national labs, and American, Canadian, and European universities attended the event, which featured twenty three presentations and extensive discussions about current and new trends in Nek5000 development. Next month will mark the 10th anniversary of Nek5000 going open source.","title":"6th Nek5000 User Meeting held at U Florida"},{"location":"news/#software-release-ceed-v10","text":"The CEED team released its first software distribution, CEED 1.0 consisting of 12 integrated Spack packages for libCEED , mfem , nek5000 , nekcem , laghos , nekbone , hpgmg , occa , magma , gslib , petsc and pumi plus a new CEED meta-package . With Spack , a user can install the whole CEED software stack simply with: spack install ceed . As part of CEED 1.0, the team developed comprehensive documentation including software and compiler configurations for ALCF , OLCF , NERSC and LLNL : Platform Architecture Spack Configuration Mac darwin-x86_64 packages Linux (RHEL7) linux-rhel7-x86_64 packages Cori (NERSC) cray-CNL-haswell packages Edison (NERSC) cray-CNL-ivybridge packages Theta (ALCF) cray-CNL-mic_knl packages Titan (OLCF) cray-CNL-interlagos packages CORAL-EA (LLNL) blueos_3_ppc64le_ib packages compilers TOSS3 (LLNL) toss_3_x86_64_ib packages compilers For more information visit http://ceed.exascaleproject.org/ceed-1.0 .","title":"Software release: CEED v1.0"},{"location":"news/#software-release-libceed-v02","text":"Version 0.2 of libCEED , the CEED API library, was released in March 2018 with major improvements in the OCCA backend. libCEED is a lightweight portable library that allows, for the first time, a wide variety of applications (written in C, C++, Fortran) to share a wide variety of discretization kernels (CPU, GPU, OpenMP, OpenCL), including high-performance GPU kernels. libCEED comes with several examples of its usage, ranging from standalone C codes in the /examples/ceed directory to examples based on external packages, such as MFEM , PETSc and Nek5000 . Below is an illustration how libCEED enables these very different codes (C++, C, F77) to take advantage of a common set of GPU kernels (see also the CEED 1.0 GPU demo ): # libCEED examples on CPU and GPU cd examples/ceed make ./ex1 -ceed /cpu/self ./ex1 -ceed /gpu/occa cd ../.. # MFEM+libCEED examples on CPU and GPU cd examples/mfem make ./bp1 -ceed /cpu/self -no-vis ./bp1 -ceed /gpu/occa -no-vis cd ../.. # PETSc+libCEED examples on CPU and GPU cd examples/petsc make ./bp1 -ceed /cpu/self ./bp1 -ceed /gpu/occa cd ../.. # Nek+libCEED examples on CPU and GPU cd examples/nek5000 ./make-nek-examples.sh ./run-nek-example.sh -ceed /cpu/self -b 3 ./run-nek-example.sh -ceed /gpu/occa -b 3 cd ../.. For more information visit https://github.com/CEED/libCEED .","title":"Software release: libCEED v0.2"},{"location":"news/#panayot-vassilevski-named-siam-fellow","text":"Panayot Vassilevski , who is part of the CEED team at LLNL, was named a 2018 SIAM fellow for his work on designing algebraic approaches for creating and analyzing multilevel algorithms . Panayot is the editor-in-chief for Numerical Linear Algebra with Applications and has published many papers and a monograph on Multilevel Block Factorization Preconditioners . In CEED, Panayot is working on matrix-free preconditioners for high-order finite element discretizations. Congratulations Panayot!","title":"Panayot Vassilevski named SIAM fellow"},{"location":"news/#benchmark-release-by-paranumal-team","text":"The CEED group at Virginia Tech released standalone implementations of CEED's BP1.0, BP3.0, and BP3.5 benchmark problems . For results and discussion, see the (\"CEED Code Competition: VT software release\")[https://www.paranumal.com/single-post/2018/02/01/CEED-Code-Competition-bake-off-problems] entry in the Paranumal blog .","title":"Benchmark release by Paranumal team"},{"location":"news/#workshop-on-batched-reproducible-and-reduced-precision-blas","text":"CEED's [UTK team]](magma.md) organized a two-session minisymposium at the SIAM Conference on Parallel Processing and Scientific Computing in Tokyo, Japan from March 7-10, 2018, devoted on Batched BLAS Standardization . The minisymposium is part of our efforts on standardization and co-design of exascale discretization APIs with application developers, hardware vendors and ECP software technologies projects. The goal is to extend the BLAS standard to include batched BLAS computational patterns/\"application motifs\" that are essential for representing and implementing tensor contractions. Besides participation from the CEED project, stakeholders from ORNL, Sandia, NVidia, Intel, IBM, and Universities were invited.","title":"Workshop on Batched, Reproducible, and Reduced Precision BLAS"},{"location":"news/#new-website-launched-by-the-virginia-tech-ceed-team","text":"A new website was recently launched by the Parallel Numerical Algorithms (Paranumal) research group at Virginia Tech here . The site includes a blog that gives some practical computing tips related to high performance implementations of finite element methods developed as part of the CEED project here .","title":"New website launched by the Virginia Tech CEED team"},{"location":"news/#initial-release-of-libceed-the-ceed-api-library","text":"The initial version of libCEED , the CEED API library, was released in December 2017. libCEED is a high-order API library, that for the first time provides a common operator description on algebraic level , that allows a wide variety of applications to take advantage of the efficient operator evaluation algorithms in the different CEED packages (from a single source). Our long-term vision for libCEED is to include a variety of back-end implementations, ranging from simple reference kernels, to highly optimized kernels targeting specific devices (e.g. GPUs) or specific polynomial orders. For more information visit https://github.com/CEED/libCEED .","title":"Initial release of libCEED: The CEED API library"},{"location":"news/#software-release-mfem-v332","text":"Version 3.3.2 of MFEM was released on November 10, 2017. Some of the new additions in this release are: Support for high-order mesh optimization based on the target-matrix optimization paradigm from the ETHOS project . Implementation of the community policies in xSDK, the Extreme-scale Scientific Software Development Kit . Integration with the STRUMPACK parallel sparse direct solver and preconditioner. Several new linear interpolators , five new examples and miniapps. Various memory, performance, discretization and solver improvements, including physical-to-reference space mapping capabilities. Continuous integration testing on Linux, Mac and Windows. For more details, see the interactive documentation and the full CHANGELOG at http://mfem.org .","title":"Software release: MFEM v3.3.2"},{"location":"news/#ceed-participates-in-xsdk-and-fastmath","text":"MFEM joined xSDK, the Extreme-scale Scientific Software Development Kit in ECP's software technologies focus area as of release xSDK-0.3.0, see https://xsdk.info/packages . MFEM and PUMI are also part of the FASTMath institute in the SciDAC program, see https://fastmath-scidac.llnl.gov/software-catalog.html .","title":"CEED participates in xSDK and FASTMath"},{"location":"news/#software-release-nek5000-v170","text":"Nek5000 version 17.0 was released as a major upgrade to Nek5000. Major features improvements include: Refactored build system. New user-input parameter file format ( .par replacing .rea ). Characteristics (large time-step) support for moving mesh problems. Moving mesh support for the $PN-PN$ formulation. Improved stability for $PN-PN$ with variable viscosity. Support for mixed Helmholtz/CVODE solves. New fast AMG setup tool based on HYPRE. New EXODUSII mesh converter. New interface to libxsmm (fast MATMUL library). Extended lowMach solver for time varying thermodynamic pressure. Added DG for scalars. Reduced solver initialization time (parallel binary reader for all input files). Automatic general mesh-to-mesh transfer for restarts. Refactored support for overlapping domains (NekNek). Added high-pass filter relaxation (alternative to explicit filter). Refactored residual projection including support for coupled Helmholtz solves.","title":"Software release: Nek5000 v17.0"},{"location":"news/#nekbone-and-laghos-join-proxy-app-suites","text":"The Nekbone and Laghos miniapps developed in CEED were selected to be part of ECP's initial Proxy Applications Suite . Both miniapps were also picked to be CORAL-2 benchmarks . Laghos was also selected as one of LLNL's ASC co-design miniapps .","title":"Nekbone and Laghos join proxy app suites"},{"location":"news/#6th-nek5000-user-meeting-to-be-held-at-u-florida","text":"The 6th Nek5000 User/Developer Meeting will be hosted by the DOE PSAAP-II Compressible Multiphase Turbulence (CMT) center in Tampa, FL, March 17-18, 2018.","title":"6th Nek5000 User Meeting to be held at U Florida"},{"location":"news/#nek5000-hackathon-at-uiuc","text":"The inaugural Nek5000 Hackathon was held at NCSA Building, University of Illinois, Urbana-Champaign (UIUC), IL on Nov 12-14, 2017. The event was attended by researchers and Nek5000 developers to promote the application of Nek5000 to new problems from industry, national laboratories, and academia. Twenty-five participants spent three days working on setting up new examples, developing new features, and helping one another to get maximum performance on their applications. Some of the more prominent exchanges of ideas included standardization of synthetic turbulent inflow techniques, use of CVODE for pure advection-diffusion problems, and the use of the characteristics methods for moving geometry applications. For more details, see the Nek5000 hackathon website .","title":"Nek5000 hackathon at UIUC"},{"location":"news/#ceed-organizing-minisymposium-at-icosahom-2018","text":"CEED is organizing a minisymposium, Efficient High-Order Finite Element Discretizations at Large Scale , at the International Conference on Spectral and High-Order Methods (ICOSAHOM 2018) in London UK, Jul 9-13, 2018. The goal of the minisymposium is to discuss the next-generation high-order discretization algorithm and software, based on finite/spectral element approaches that will enable a wide range of important scientific applications to run efficiently on future architecture.","title":"CEED organizing minisymposium at ICOSAHOM 2018"},{"location":"news/#best-paper-award-at-nureth-17","text":"CEED researchers (P. Fischer, E. Merzari, A. Obabko) won a Best Paper Award at the 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17), held in China in September 2017, with a paper entitled High-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors .","title":"Best Paper Award at NURETH-17"},{"location":"news/#ceed-attending-cray-amd-and-intel-deep-dives","text":"CEED researchers and representatives of the Nek and MFEM teams will attend the October 2017, ECP vendor deep-dive meetings: Cray deep-dive in Bloomington, MN on Oct 18-19 AMD deep-dive in Austin, TX on Oct 24-25 Intel deep-dive in Hudson, MA on Oct 21-Nov 2 Topics of discussion include advanced technology and memory design, strong scaling considerations and the porting and evaluation of CEED's bake-off problems and miniapps (Nekbone, Laghos, NekCEM CEEDling).","title":"CEED attending Cray, AMD and Intel Deep-Dives"},{"location":"news/#new-nekbench-repository","text":"New Nekbench repository has been released to provide scripts that simplify the benchmarking of Nek5000 . The user provides ranges for important parameters ranges (e.g., processor counts and local problem size ranges) and a test type (e.g., scaling or ping-pong test). Nekbench will run the given test in the given parameter space using a Nek5000 case file which is also given by the user (in the ping-pong tests, the case file is optional). Nekbench is written using bash scripting language and runs any Unix-like operating system that supports bash. It has been successfully tested on Linux laptops/desktops, ALCF Theta, NERSC Cori (KNL and Haswell), and NERSC Edison machines for scaling tests. Planned extensions for Nekbench include adding more machine types like ANL's Cetus, additional support for the ping-pong test type, and automated plot generation (e.g., scaling study graphs) for each test run.","title":"New Nekbench repository"},{"location":"news/#gpu-ports-of-nek-and-laghos","text":"GPU acceleration is a main focus of the performance optimization efforts in CEED. Recent progress in this direction include GPU ports of CEED's Nek5000 application and the Nekbone and Laghos miniapps . For Nek5000, an initial GPU-enabled version has been developed based on OpenACC. For Nekbone a pure OpenACC implementation as well as a hybrid OpenACC/CUDA implementation with a CUDA kernel for matrix-vector multiplication has been developed . For Laghos, a GPU-enabled version of has been released using the OCCA interface. With this approach, the user is able to run Laghos distributively using varying device types per MPI process, whether serial C++, OpenMP, or CUDA.","title":"GPU ports of Nek and Laghos"},{"location":"news/#first-ceed-annual-meeting-held-at-llnl","text":"CEED held its first annual meeting in August, 2017 at the HPC Innovation Center of Lawrence Livermore National Laboratory. The goal of the meeting was to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting included representatives of the ECP management, hardware vendors, software technology and other interested projects.","title":"First CEED annual meeting held at LLNL"},{"location":"news/#ceed-researchers-at-atpesc17","text":"Six CEED researchers presented at the 2017 edition of the Argonne Training Program on Extreme-Scale Computing , now part of the Exascale Computing Project. The CEED presentations covered a wide variety of topics, from overview of Theta, to GPU programming, dense and sparse linear algebra, and high-order discretizations on unstructured meshes. Videos of all 2017 talks are available on YouTube . CEED researchers have also participated in past editions of the meeting.","title":"CEED researchers at ATPESC17"},{"location":"news/#ceed-bps-and-benchmarks-repository-released","text":"CEED released an initial set of bake-off (BP) problems , which are simple kernels designed to test and compare the performance of high-order codes, both internally in CEED, as well as in the broader high-order community . In addition to the benchmark descriptions on the CEED BPs page , a benchmarks repository is publicly available with several implementations of the CEED bake-off problems. Currently, MFEM, Nek5000 and deal.ii are included, see directories tests/mfem_bps , tests/nek5000_bps and tests/dealii_bps respectively.","title":"CEED BPs and benchmarks repository released"},{"location":"news/#new-laghos-and-nekcem-ceedling-miniapps-released","text":"Two new miniapps developed in CEED were released in June 2017: Laghos and NekCEM CEEDling. Laghos (LAGrangian High-Order Solver) is a new miniapp developed in CEED that solves the time-dependent Euler equations of compressible gas dynamics in a moving Lagrangian frame using unstructured high-order finite element spatial discretization and explicit high-order time-stepping. In CEED, Laghos serves as a proxy for a sub-component of the MARBL/LLNLApp application. NekCEM CEEDling is a new NekCEM miniapp, solving the time-domain Maxwell equation for electromagnetic systems. For more details, see the CEED miniapps page and the Laghos and NekCEM CEEDling repositories on GitHub.","title":"New Laghos and NekCEM CEEDling miniapps released"},{"location":"news/#paper-with-mpich-at-sc17","text":"Joint paper with the MPICH group, Why is MPI so Slow? Analyzing the fundamental limits in implementing MPI-3.1 accepted in Supercomputing 2017 . The paper provides an in-depth analysis of the software overheads in the MPI performance-critical path and exposes mandatory performance overheads that are unavoidable based on the MPI-3.1 specification.","title":"Paper with MPICH at SC17"},{"location":"news/#strumpack-support-in-mfem","text":"Support for the sparse direct solver and preconditioner STRUMPACK has been integrated in MFEM. STRUMPACK is being developed at LBNL and is part of the ECP project Factorization Based Sparse Solvers and Preconditioners (Xiaoye Sherry Li and Pieter Ghysels). The STRUMPACK solver is based on multifrontal sparse Gaussian elimination and uses hierarchically semi-separable matrices to compress fill-in. It can be used as an exact direct solver or as an algebraic, robust and parallel preconditioner for a range of discretized PDE problems.","title":"STRUMPACK support in MFEM"},{"location":"news/#2017-petsc-user-meeting","text":"Over 75 participants from all over the world attended the PETSc User Meeting, held June 14-16 in Boulder, CO. Hosted by the University of Colorado Boulder, the event consisted of a one-day tutorial on the solver library PETSc and showcased the latest research enabled by the functionality available in PETSc. The meeting agenda covered a total of 15 talks, four posters, and two panels. Thanks to generous support from Intel and Tech-X, 22 students received travel grants and got to learn about the latest techniques on the large-scale numerical solution of partial differential equations. PETSc is a suite of data structures and routines for the scalable (parallel) solution of scientific applications modeled by partial differential equations. It has become one of the most widely used numerical software packages of its kind and has users in application areas ranging from acoustics and arterial flow to seismology and semiconductors.","title":"2017 PETSc User Meeting"},{"location":"news/#gpu-hackathon-at-bnl","text":"Nek/CEED team participated the GPU Hackathon 2017 that was held in Brookhaven National Laboratory on June 5-9, 2017. Our team focused on performing and tuning GPU-enabled Nek5000/Nekbone/NekCEM version on large-scale GPU systems for small modular reactor, thermal fluids, and meta-materials modeling.","title":"GPU Hackathon at BNL"},{"location":"news/#workshop-on-batched-reproducible-and-reduced-precision-blas_1","text":"The second Workshop on Batched, Reproducible, and Reduced Precision BLAS was held in Atlanta, GA on February 23-25, 2017 including many members of the CEED MAGMA team. The goal of this workshop was to touch on extending the Basic Linear Algebra Software Library (BLAS). The existing BLAS have proven to be very effective in assisting portable, efficient software for sequential and some of the current class of high-performance computers. New computational needs in many applications have motivated the need to investigate the possibility of extending the currently accepted standards to provide greater parallelism for small size operations, reproducibility, and reduced precision support. Of particular interest to CEED is the use of batched BLAS for finite element tensor contractions, and thus our team is interested in the establishment of a batched BLAS standard, highly-optimized implementations, and support from vendors on various architectures. This is the second workshop of an open forum to discuss and formalize details related to batched, reproducible, and reduced precision BLAS. The agenda and the talks from the first workshop can be found here .","title":"Workshop on Batched, Reproducible, and Reduced Precision BLAS"},{"location":"news/#software-release-mfem-v33","text":"Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element methods and a main partner in CEED, was released on January 28, 2017 at http://mfem.org The goal of MFEM is to enable high-performance scalable finite element discretization research and application development on a wide variety of platforms, ranging from laptops to exascale supercomputers. It has many features, including: 2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements. Parallel version scalable to hundreds of thousands of MPI cores. Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing. Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations. Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes. Scalable algebraic multigrid, time integrators, and eigensolvers. Lightweight interactive OpenGL visualization with the MFEM-based GLVis tool. Some of the new additions in version 3.3 are: Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features from the PETSc and SUNDIALS suites. Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces. General quadrature and nodal finite element basis types. Scalable parallel mesh format. Thirty six new integrators for common families of operators. Sixteen new serial and parallel example codes. Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the Conduit mesh blueprint specification. MFEM is being developed in CASC , LLNL and is freely available under LGPL 2.1. For more details, see the interactive documentation and the full CHANGELOG .","title":"Software release: MFEM v3.3"},{"location":"news/#ceed-co-design-center-announced","text":"The Exascale Computing Project (ECP) announced on November 11, 2016 its selection of four co-design centers , including CEED: the Center for Efficient Exascale Discretizations, which is a research partnership between Lawrence Livermore National Laboratory; Argonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee, Knoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI). Additional news coverage can be found in LLNL Newsline and the ANL press release .","title":"CEED co-design center announced"},{"location":"news/#rd-100-award-for-nekcem-nek5000","text":"NekCEM/Nek5000: Scalable High-Order Simulation Codes received a 2016 R&D 100 Award, given by R&D Magazine to 100 top new technologies for the year. The R&D 100 citation reads: \"NekCEM/Nek5000: Release 4.0: Scalable High-Order Simulation Codes is an open-source simulation-software package that delivers highly accurate solutions for a wide range of scientific applications including electromagnetics, quantum optics, fluid flow, thermal convection, combustion and magnetohydrodynamics. It features state-of-the-art, scalable, high-order algorithms that are fast and efficient on platforms ranging from laptops to the world\u2019s fastest computers. The size of the physical phenomena that can be simulated with this package ranges from quantum dots for nanoscale devices to accretion disks surrounding black holes. NekCEM provides simulation capabilities for the analysis of electromagnetic and quantum optical devices, such as particle accelerators and solar cells. Nek5000 provides turbulent flow simulation capabilities for a variety of thermal-fluid problems including nuclear reactors, internal combustion engines, vascular flows, and ocean currents.\" See the ANL press release for more information.","title":"R&amp;D 100 Award for NekCEM / Nek5000"},{"location":"occa/","text":"OCCA OCCA is an open-source library that facilitates programming in an environment containing different types of devices. It abstracts devices and lets the user pick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs. OCCA abstracts the variety of device programming languages into one kernel language, the OCCA kernel language (OKL). OKL minimally extends C and restricts the user to write parallel code that is JIT compiled. OCCA is freely available under an MIT license. In CEED, OCCA is primarily involved in the efforts of the Software and Finite Element thrusts. For more information, see the OCCA website: http://libocca.org .","title":"OCCA"},{"location":"occa/#occa","text":"OCCA is an open-source library that facilitates programming in an environment containing different types of devices. It abstracts devices and lets the user pick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs. OCCA abstracts the variety of device programming languages into one kernel language, the OCCA kernel language (OKL). OKL minimally extends C and restricts the user to write parallel code that is JIT compiled. OCCA is freely available under an MIT license. In CEED, OCCA is primarily involved in the efforts of the Software and Finite Element thrusts. For more information, see the OCCA website: http://libocca.org .","title":"OCCA "},{"location":"petsc/","text":"PETSc PETSc is a scalable package for solving differential and algebraic equations. It supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU parallelism. As part of CEED, the PETSc project will coordinate the development of expressive interfaces for efficient and robust solution of the algebraic equations appearing in high-order/spectral element methods. This will include multilevel solvers that work with unassembled representations of linear operators. PETSc is freely available under a BSD license. In CEED, PETSc is primarily involved in the efforts of the Software thrust. For more information, see the PETSc website: https://www.mcs.anl.gov/petsc/ .","title":"PETSc"},{"location":"petsc/#petsc","text":"PETSc is a scalable package for solving differential and algebraic equations. It supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU parallelism. As part of CEED, the PETSc project will coordinate the development of expressive interfaces for efficient and robust solution of the algebraic equations appearing in high-order/spectral element methods. This will include multilevel solvers that work with unassembled representations of linear operators. PETSc is freely available under a BSD license. In CEED, PETSc is primarily involved in the efforts of the Software thrust. For more information, see the PETSc website: https://www.mcs.anl.gov/petsc/ .","title":"PETSc"},{"location":"pubs/","text":"Publications and Outreach CEED Documents CEED's high-order Benchmarks and Miniapps . Activities in the Applications , Hardware , Software and Finite Element thrusts. CEED-proposed high-order Operator and Visualization formats. CEED-MS1 report: Engage first wave ECP/CEED apps . CEED-MS6 report: Identify initial kernels, benchmarks and miniapps . CEED-MS8 report: Initial integration of CEED software in ECP apps . CEED-MS10 report: Initial CEED API . CEED-MS13 report: Public release of CEED 1.0 . CEED-MS18 report: Propose high-order mesh/data format . CEED-MS20 report: Performance tuning of CEED software and first wave apps . CEED-MS23 report: Engage second wave ECP/CEED applications . CEED-MS25 report: Public release of CEED 2.0 . CEED-MS29 report: Improved Support for Parallel Adaptive Simulation in CEED . Publications 2019 H. Hajduk, D. Kuzmin, Tz. Kolev and R. Abgrall, Matrix-free subcell residual distribution for Bernstein finite elements: Low-order schemes and FCT , submitted , 2019 . V. Dobrev, Tz. Kolev, C. Lee, V. Tomov and P. Vassilevski, Algebraic Hybridization and Static Condensation with Application to Scalable H(div) Preconditioning , SIAM J. Sci. Comp. , to appear, 2019 . J. Ceverny, V. Dobrev, and Tz. Kolev, Non-Conforming Mesh Refinement For High-Order Finite Elements , SIAM J. Sci. Comp. , to appear, 2019 . V. Dobrev, P. Knupp, Tz. Kolev, and V. Tomov, Towards Simulation-Driven Optimization of High-Order Meshes by the Target-Matrix Optimization Paradigm , Roca X., Loseille A. (eds) 27th International Meshing Roundtable. IMR 2018. Lecture Notes in Computational Science and Engineering, vol 127. , pp. 285-302, Springer, 2019 . I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J Dongarra, Algorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices , Parallel Computing, vol. 81, pp. 1-21, 2019 . V. Dobrev, P. Knupp, Tz. Kolev, K. Mittal, V. Tomov, The Target-Matrix Optimization Paradigm for High-Order Meshes , SIAM J. Sci. Comp. , 41(1), pp. B50\u2013B68, 2019 . 2018 P. Bello-Maldonado and P. Fischer, Scalable Low-Order Finite Element Preconditioners for High-Order Spectral Element Poisson Solvers , submitted, 2018 . A. Karakus, N. Chalmers, J.S. Hesthaven and T. Warburton, Discontinuous Galerkin Discretizations of the Boltzmann Equations in 2D: semi-analytic time stepping and absorbing boundary layers , submitted, 2018 . A. Haidar, S. Tomov, and J. Dongarra, Optimizing GPU Kernels for Irregular Batch Workloads: A Case Study for Cholesky Factorization , HPEC'18, submitted, 2018 . A. Karakus, N. Chalmers, K. Swirydowicz, T. Warburton, GPU Acceleration of a High-Order Discontinuous Galerkin Incompressible Flow Solver , submitted, 2018 . A. Barker, V. Dobrev, J. Gopalakrishnan and T. Kolev, A scalable preconditioner for a primal discontinuous Petrov-Galerkin method , SIAM J. Sci. Comp. , 40(1), pp. B32-B58, 2018 . R. Anderson, V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, High-Order Multi-Material ALE Hydrodynamics , SIAM J. Sci. Comp. , 40(1):B32-B58, 2018 . V. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov, Sequential limiting in continuous and discontinuous Galerkin methods for the Euler equations , Journal of Computational Physics, 356:372-390, 2018 . 2017 K. Swirydowicz, N. Chalmers, A. Karakus, T. Warburton, Acceleration of tensor-product operations for high-order finite element methods , submitted, 2017 J. Solberg, E. Merzari, H. Yuan, A. Obabko and P. Fischer, S. Lee, J. Lai, M. Delgado, S. J. Lee, and Y. Hassan, High-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors , Best Paper Award at The 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17) , September, 2017 . S. Patel, P. Fisher, M. Min, and A. Tomboulides, A Characteristic-based, spectral element method for moving-domain problems , J. Sci. Comp. , submitted, 2017 . I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J. Dongarra, Algorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices , Parallel Computing (PARCO) , submitted, 2017 . K. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji, Why is MPI so slow? Analyzing the fundamental limits in implementing MPI-3.1 , accepted, SC'17 , 2017 . P. Fischer, M. Schmitt, and A. Tomboulides, Recent developments in spectral element simulations of moving-domain problems , vol. 79, Fields Institute Communications , 213\u2013244, 2017 . S. Lomperski, A. Obabko, P. Fischer, E. Merzari, and W.D. Pointer, Jet stability and wall impingement flow field in a thermal striping experiment , Int. J. Heat Mass Transfer , 115A:1125\u2013 1136, 2017 . V. Makarashvilia, E. Merzari, A. Obabko, A. Siegel, and P. Fischer, A performance analysis of ensemble averaging for high fidelity turbulence simulations at the strong scaling limit , Comp. Phys. Comm. , in press, 2017 . A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures , Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14, Procedia Computer Science , 2017 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, University of Tennessee, 2017 . R. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov, High-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation , Journal of Computational Physics, 334:102\u2013124, 2017 . Bazilevs, Y., Kamran, K., Moutsanidis, G., Benson, D. J., & O\u00f1ate, E, A new formulation for air-blast fluid\u2013structure interaction using an immersed approach. Part I: basic methodology and FEM-based simulations , Computational Mechanics, 1-18, 2017 . Abdelfattah, A., Haidar, A., Tomov, S., and Dongarra, J. Novel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs , International Conference on Supercomputing (ICS'17), ACM, Chicago, Illinois, pp. 1-10, June 14-16, 2017 . Ibanez, D., Shephard, M.S., Modifiable Array Data Structures for Mesh Topology , SIAM Journal on Scientific Computing, 39(2):C144-C161, 2017 . Granzow, B.N., Shephard M.S., Oberai, A.A., Output-based error estimation and mesh adaptation for variational multiscale methods , Computer Methods in Applied Mechanics and Engineering, Vol. 322, pp. 441-459, 2017 . 2016 E. Merzari, A. Obabko, P. Fischer, N. Halford, J. Walker, A. Siegel, and Y. Q. Yu, Large-scale large eddy simulation of nuclear reactor flows: Issues and perspectives , Nuclear Engineering and Design , page 13, Oct., 2016 . E. Merzari, P. Fischer, H. Yuan, K. Van Tichelen, S. Keijers, J. De Ridder, J. Degroote, J. Vierendeels, H. Doolaard, V. R. Gopala, and F. Roelofs, Benchmark exercise for fluid flow simulations in a liquid metal fast reactor fuel assembly , Nuclear Engineering and Design , 298(3):218\u2013228, 2016 . V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, Multi-material closure model for high-order finite element Lagrangian hydrodynamics , Int. J. Numer. Meth. Fluids , 82(10), pp. 689\u2013706, 2016 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, High-performance Tensor Contractions for GPUs , Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509, 2016 . M. B.E., Y. Peet, P. Fischer, and J. Lottes, A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations , J. Comp. Phys. , 307:60\u201393, 2016 . M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication , The International Journal of High Performance Computing Application , 30(3):320\u2013334, 2016 . J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations , Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing , 2016 . Ibanez, Daniel A., et al. PUMI: Parallel unstructured mesh infrastructure. ACM Transactions on Mathematical Software (TOMS) 42.3 2016 . Smith, Cameron W., et al. In-memory Integration of Existing Software Components for Parallel Adaptive Unstructured Mesh Workflows. Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM, 2016 . Ibanez, D., Dunn, I., Shephard M.S., Hybrid MPI-thread parallelization of adaptive mesh operations , Parallel Computing, 52:133-143, 2016 . 2015 and earlier P. Fischer, K. Heisey, and M. Min, Scaling limits for PDE-based simulation , In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049, 2015 . A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461. American Nuclear Society , 2015 . E. Merzari, P. Fischer, and J. Walker, Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1. American Society of Mechanical Engineers , 2015 . M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, Entanglement of two, three and four plasmonically coupled quantum dots , Physical Review B , 92:125432, 2015 . D. A. May, J. Brown, and L. Le Pourhiet. pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM, 2014 . R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, Monotonicity in high-order curvilinear finite element ALE remap , Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273, 2014 . Kamran, Kazem, et al. A compressible Lagrangian framework for modeling the fluid\u2013structure interaction in the underwater implosion of an aluminum cylinder. Mathematical Models and Methods in Applied Sciences 23.02 2013 . Tz. Kolev and P. Vassilevski, Parallel auxiliary space AMG solver for H(div) problems , SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098, 2012 . V. Dobrev, Tz. Kolev and R. Rieben, High-order curvilinear finite element methods for Lagrangian hydrodynamics , SIAM J. Sci. Comp. , 34, pp. B606\u2013B641, 2012 . J. Brown, Efficient nonlinear solvers for nodal high-order finite elements in 3D , Journal of Scientific Computing , 45:48\u201363, 2010 . doi:10.1007/s10915-010-9396-8 Tz. Kolev and P. Vassilevski, Parallel auxiliary space AMG for H(curl) problems , J. Comput. Math. , 27, pp. 604-623, 2009 . Presentations 2017 Tz. Kolev and M. Shephard, Conforming & Nonconforming Adaptivity for Unstructured Meshes , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . Tz. Kolev and M. Shephard, Unstructured Mesh Technologies , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . B. Smith, Nonlinear and Krylov Solvers , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . J. Dongarra, Adaptive Linear Solvers and Eigensolvers , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . T. Warburton, An Intro to GPU Architecture and Programming Models , Argonne Training Program on Extreme-Scale Computing, Aug 3, 2017 . S. Parker, Architectures of the Argonne Cray XC40 KNL System \"Theta\" , Argonne Training Program on Extreme-Scale Computing, Jul 31, 2017 . S. Tomov and A. Haidar, MAGMA Tensors and Batched Computing for Accelerating Applications on GPUs , GPU Technology Conference (GTC'17), Session S7728, May 8-11, 2017 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched , SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . C. Smith, G. Diamond and M.S. Shephard, Fast Dynamic Load Balancing Tools for Extreme Scale System, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . S. Tendulkar, O. Klaas, M.W. Beall, and M.S. Shephard, Parallel Geometry and Meshing Adaptation with Application to Problems with Evolving Domains, SIAM Conf. on Computational Science and Engineering , Atlanta, GA, Mar 3, 2017 . P. Fischer, CFD, PDEs, and HPC: A 30-Year Perspective , Argonne Training Program on Extreme-Scale Computing, Aug 2, 2016 . Highlights DEIXIS article about CEED: Scaling the Unknown , Jul 2018. ECP article: Co-Design is Key to ECP's Holistic Approach to Capable Exascale Computing , Apr 2018. MFEM highlighted in LLNL's Science & Technology Review magazine, including on the cover , Jan/Feb 2018. GCN article: Exascale a \"main priority\" for DOE , Jan 2018. ECP article: Co-Design Center Develops Next-Generation Simulation Tools , also in HPCwire , Nov 2017. Work with LLNL's Center for Design and Optimization mentioned in LLNL Newsline, Oct 2017. Highlight in CASC Newsletter #3 , Oct 2017. Highlight in LLNL\u2019s 65th Anniversary Book (2017 page), Oct 2017. GPU work highlight in LLNL's COMP News page and Livermore_Comp's Twitter feed , May 2017. Work with Cardioid mentioned in LLNL's Science & Technology Review magazine, Mar 2017. News coverage of CEED announcement in LLNL Newsline and the ANL press release , Nov 2016. Work with BLAST mentioned in LLNL's Science & Technology Review magazine, Sep 2016. Other Resources CEED-tagged topics on ECP's website . ANL's exascale computing website . LLNL's exascale computing website . Blog of Virginia Tech's Parallel Numerical Algorithms research group . U.S. Department of Energy Exascale Initiative .","title":"Publications"},{"location":"pubs/#publications-and-outreach","text":"","title":"Publications and Outreach"},{"location":"pubs/#ceed-documents","text":"CEED's high-order Benchmarks and Miniapps . Activities in the Applications , Hardware , Software and Finite Element thrusts. CEED-proposed high-order Operator and Visualization formats. CEED-MS1 report: Engage first wave ECP/CEED apps . CEED-MS6 report: Identify initial kernels, benchmarks and miniapps . CEED-MS8 report: Initial integration of CEED software in ECP apps . CEED-MS10 report: Initial CEED API . CEED-MS13 report: Public release of CEED 1.0 . CEED-MS18 report: Propose high-order mesh/data format . CEED-MS20 report: Performance tuning of CEED software and first wave apps . CEED-MS23 report: Engage second wave ECP/CEED applications . CEED-MS25 report: Public release of CEED 2.0 . CEED-MS29 report: Improved Support for Parallel Adaptive Simulation in CEED .","title":"CEED Documents"},{"location":"pubs/#publications","text":"","title":"Publications"},{"location":"pubs/#2019","text":"H. Hajduk, D. Kuzmin, Tz. Kolev and R. Abgrall, Matrix-free subcell residual distribution for Bernstein finite elements: Low-order schemes and FCT , submitted , 2019 . V. Dobrev, Tz. Kolev, C. Lee, V. Tomov and P. Vassilevski, Algebraic Hybridization and Static Condensation with Application to Scalable H(div) Preconditioning , SIAM J. Sci. Comp. , to appear, 2019 . J. Ceverny, V. Dobrev, and Tz. Kolev, Non-Conforming Mesh Refinement For High-Order Finite Elements , SIAM J. Sci. Comp. , to appear, 2019 . V. Dobrev, P. Knupp, Tz. Kolev, and V. Tomov, Towards Simulation-Driven Optimization of High-Order Meshes by the Target-Matrix Optimization Paradigm , Roca X., Loseille A. (eds) 27th International Meshing Roundtable. IMR 2018. Lecture Notes in Computational Science and Engineering, vol 127. , pp. 285-302, Springer, 2019 . I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J Dongarra, Algorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices , Parallel Computing, vol. 81, pp. 1-21, 2019 . V. Dobrev, P. Knupp, Tz. Kolev, K. Mittal, V. Tomov, The Target-Matrix Optimization Paradigm for High-Order Meshes , SIAM J. Sci. Comp. , 41(1), pp. B50\u2013B68, 2019 .","title":"2019"},{"location":"pubs/#2018","text":"P. Bello-Maldonado and P. Fischer, Scalable Low-Order Finite Element Preconditioners for High-Order Spectral Element Poisson Solvers , submitted, 2018 . A. Karakus, N. Chalmers, J.S. Hesthaven and T. Warburton, Discontinuous Galerkin Discretizations of the Boltzmann Equations in 2D: semi-analytic time stepping and absorbing boundary layers , submitted, 2018 . A. Haidar, S. Tomov, and J. Dongarra, Optimizing GPU Kernels for Irregular Batch Workloads: A Case Study for Cholesky Factorization , HPEC'18, submitted, 2018 . A. Karakus, N. Chalmers, K. Swirydowicz, T. Warburton, GPU Acceleration of a High-Order Discontinuous Galerkin Incompressible Flow Solver , submitted, 2018 . A. Barker, V. Dobrev, J. Gopalakrishnan and T. Kolev, A scalable preconditioner for a primal discontinuous Petrov-Galerkin method , SIAM J. Sci. Comp. , 40(1), pp. B32-B58, 2018 . R. Anderson, V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, High-Order Multi-Material ALE Hydrodynamics , SIAM J. Sci. Comp. , 40(1):B32-B58, 2018 . V. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov, Sequential limiting in continuous and discontinuous Galerkin methods for the Euler equations , Journal of Computational Physics, 356:372-390, 2018 .","title":"2018"},{"location":"pubs/#2017","text":"K. Swirydowicz, N. Chalmers, A. Karakus, T. Warburton, Acceleration of tensor-product operations for high-order finite element methods , submitted, 2017 J. Solberg, E. Merzari, H. Yuan, A. Obabko and P. Fischer, S. Lee, J. Lai, M. Delgado, S. J. Lee, and Y. Hassan, High-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors , Best Paper Award at The 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17) , September, 2017 . S. Patel, P. Fisher, M. Min, and A. Tomboulides, A Characteristic-based, spectral element method for moving-domain problems , J. Sci. Comp. , submitted, 2017 . I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J. Dongarra, Algorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices , Parallel Computing (PARCO) , submitted, 2017 . K. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji, Why is MPI so slow? Analyzing the fundamental limits in implementing MPI-3.1 , accepted, SC'17 , 2017 . P. Fischer, M. Schmitt, and A. Tomboulides, Recent developments in spectral element simulations of moving-domain problems , vol. 79, Fields Institute Communications , 213\u2013244, 2017 . S. Lomperski, A. Obabko, P. Fischer, E. Merzari, and W.D. Pointer, Jet stability and wall impingement flow field in a thermal striping experiment , Int. J. Heat Mass Transfer , 115A:1125\u2013 1136, 2017 . V. Makarashvilia, E. Merzari, A. Obabko, A. Siegel, and P. Fischer, A performance analysis of ensemble averaging for high fidelity turbulence simulations at the strong scaling limit , Comp. Phys. Comm. , in press, 2017 . A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures , Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14, Procedia Computer Science , 2017 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, University of Tennessee, 2017 . R. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov, High-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation , Journal of Computational Physics, 334:102\u2013124, 2017 . Bazilevs, Y., Kamran, K., Moutsanidis, G., Benson, D. J., & O\u00f1ate, E, A new formulation for air-blast fluid\u2013structure interaction using an immersed approach. Part I: basic methodology and FEM-based simulations , Computational Mechanics, 1-18, 2017 . Abdelfattah, A., Haidar, A., Tomov, S., and Dongarra, J. Novel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs , International Conference on Supercomputing (ICS'17), ACM, Chicago, Illinois, pp. 1-10, June 14-16, 2017 . Ibanez, D., Shephard, M.S., Modifiable Array Data Structures for Mesh Topology , SIAM Journal on Scientific Computing, 39(2):C144-C161, 2017 . Granzow, B.N., Shephard M.S., Oberai, A.A., Output-based error estimation and mesh adaptation for variational multiscale methods , Computer Methods in Applied Mechanics and Engineering, Vol. 322, pp. 441-459, 2017 .","title":"2017"},{"location":"pubs/#2016","text":"E. Merzari, A. Obabko, P. Fischer, N. Halford, J. Walker, A. Siegel, and Y. Q. Yu, Large-scale large eddy simulation of nuclear reactor flows: Issues and perspectives , Nuclear Engineering and Design , page 13, Oct., 2016 . E. Merzari, P. Fischer, H. Yuan, K. Van Tichelen, S. Keijers, J. De Ridder, J. Degroote, J. Vierendeels, H. Doolaard, V. R. Gopala, and F. Roelofs, Benchmark exercise for fluid flow simulations in a liquid metal fast reactor fuel assembly , Nuclear Engineering and Design , 298(3):218\u2013228, 2016 . V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, Multi-material closure model for high-order finite element Lagrangian hydrodynamics , Int. J. Numer. Meth. Fluids , 82(10), pp. 689\u2013706, 2016 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, High-performance Tensor Contractions for GPUs , Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509, 2016 . M. B.E., Y. Peet, P. Fischer, and J. Lottes, A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations , J. Comp. Phys. , 307:60\u201393, 2016 . M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication , The International Journal of High Performance Computing Application , 30(3):320\u2013334, 2016 . J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations , Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing , 2016 . Ibanez, Daniel A., et al. PUMI: Parallel unstructured mesh infrastructure. ACM Transactions on Mathematical Software (TOMS) 42.3 2016 . Smith, Cameron W., et al. In-memory Integration of Existing Software Components for Parallel Adaptive Unstructured Mesh Workflows. Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM, 2016 . Ibanez, D., Dunn, I., Shephard M.S., Hybrid MPI-thread parallelization of adaptive mesh operations , Parallel Computing, 52:133-143, 2016 .","title":"2016"},{"location":"pubs/#2015-and-earlier","text":"P. Fischer, K. Heisey, and M. Min, Scaling limits for PDE-based simulation , In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049, 2015 . A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461. American Nuclear Society , 2015 . E. Merzari, P. Fischer, and J. Walker, Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1. American Society of Mechanical Engineers , 2015 . M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, Entanglement of two, three and four plasmonically coupled quantum dots , Physical Review B , 92:125432, 2015 . D. A. May, J. Brown, and L. Le Pourhiet. pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM, 2014 . R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, Monotonicity in high-order curvilinear finite element ALE remap , Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273, 2014 . Kamran, Kazem, et al. A compressible Lagrangian framework for modeling the fluid\u2013structure interaction in the underwater implosion of an aluminum cylinder. Mathematical Models and Methods in Applied Sciences 23.02 2013 . Tz. Kolev and P. Vassilevski, Parallel auxiliary space AMG solver for H(div) problems , SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098, 2012 . V. Dobrev, Tz. Kolev and R. Rieben, High-order curvilinear finite element methods for Lagrangian hydrodynamics , SIAM J. Sci. Comp. , 34, pp. B606\u2013B641, 2012 . J. Brown, Efficient nonlinear solvers for nodal high-order finite elements in 3D , Journal of Scientific Computing , 45:48\u201363, 2010 . doi:10.1007/s10915-010-9396-8 Tz. Kolev and P. Vassilevski, Parallel auxiliary space AMG for H(curl) problems , J. Comput. Math. , 27, pp. 604-623, 2009 .","title":"2015 and earlier"},{"location":"pubs/#presentations","text":"","title":"Presentations"},{"location":"pubs/#2017_1","text":"Tz. Kolev and M. Shephard, Conforming & Nonconforming Adaptivity for Unstructured Meshes , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . Tz. Kolev and M. Shephard, Unstructured Mesh Technologies , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . B. Smith, Nonlinear and Krylov Solvers , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . J. Dongarra, Adaptive Linear Solvers and Eigensolvers , Argonne Training Program on Extreme-Scale Computing, Aug 7, 2017 . T. Warburton, An Intro to GPU Architecture and Programming Models , Argonne Training Program on Extreme-Scale Computing, Aug 3, 2017 . S. Parker, Architectures of the Argonne Cray XC40 KNL System \"Theta\" , Argonne Training Program on Extreme-Scale Computing, Jul 31, 2017 . S. Tomov and A. Haidar, MAGMA Tensors and Batched Computing for Accelerating Applications on GPUs , GPU Technology Conference (GTC'17), Session S7728, May 8-11, 2017 . A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched , SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . C. Smith, G. Diamond and M.S. Shephard, Fast Dynamic Load Balancing Tools for Extreme Scale System, SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3, 2017 . S. Tendulkar, O. Klaas, M.W. Beall, and M.S. Shephard, Parallel Geometry and Meshing Adaptation with Application to Problems with Evolving Domains, SIAM Conf. on Computational Science and Engineering , Atlanta, GA, Mar 3, 2017 . P. Fischer, CFD, PDEs, and HPC: A 30-Year Perspective , Argonne Training Program on Extreme-Scale Computing, Aug 2, 2016 .","title":"2017"},{"location":"pubs/#highlights","text":"DEIXIS article about CEED: Scaling the Unknown , Jul 2018. ECP article: Co-Design is Key to ECP's Holistic Approach to Capable Exascale Computing , Apr 2018. MFEM highlighted in LLNL's Science & Technology Review magazine, including on the cover , Jan/Feb 2018. GCN article: Exascale a \"main priority\" for DOE , Jan 2018. ECP article: Co-Design Center Develops Next-Generation Simulation Tools , also in HPCwire , Nov 2017. Work with LLNL's Center for Design and Optimization mentioned in LLNL Newsline, Oct 2017. Highlight in CASC Newsletter #3 , Oct 2017. Highlight in LLNL\u2019s 65th Anniversary Book (2017 page), Oct 2017. GPU work highlight in LLNL's COMP News page and Livermore_Comp's Twitter feed , May 2017. Work with Cardioid mentioned in LLNL's Science & Technology Review magazine, Mar 2017. News coverage of CEED announcement in LLNL Newsline and the ANL press release , Nov 2016. Work with BLAST mentioned in LLNL's Science & Technology Review magazine, Sep 2016.","title":"Highlights"},{"location":"pubs/#other-resources","text":"CEED-tagged topics on ECP's website . ANL's exascale computing website . LLNL's exascale computing website . Blog of Virginia Tech's Parallel Numerical Algorithms research group . U.S. Department of Energy Exascale Initiative .","title":"Other Resources"},{"location":"pumi/","text":"PUMI PUMI is an unstructured, distributed mesh data management system designed for massively parallel computing. PUMI supports a full range of operations on unstructured meshes on massively parallel computers consisting of five libraries: PCU for phased message passing and thread management. GMI for geometric model interface. MDS for unstructured mesh representation. APF Mesh for partition model and distributed mesh management. APF_Field for field management. PUMI is being developed at RPI's Scientific Computation Research Center and is currently being used on projects sponsored by the DOE, NSF, Army, NASA, IBM and several companies. In CEED, PUMI is primarily involved in the efforts of the Finite Element and Software thrusts. For more information, see the PUMI documents .","title":"PUMI"},{"location":"pumi/#pumi","text":"PUMI is an unstructured, distributed mesh data management system designed for massively parallel computing. PUMI supports a full range of operations on unstructured meshes on massively parallel computers consisting of five libraries: PCU for phased message passing and thread management. GMI for geometric model interface. MDS for unstructured mesh representation. APF Mesh for partition model and distributed mesh management. APF_Field for field management. PUMI is being developed at RPI's Scientific Computation Research Center and is currently being used on projects sponsored by the DOE, NSF, Army, NASA, IBM and several companies. In CEED, PUMI is primarily involved in the efforts of the Finite Element and Software thrusts. For more information, see the PUMI documents .","title":"PUMI "},{"location":"software/","text":"Latest Releases The CEED software is open-source and publicly available on GitHub . The latest releases of our main software components are available from: CEED-2.0 libCEED-0.2 FMS-0.1 Nek5000-17.0 mfem-4.0 Laghos-2.0 Nekbone-17.0 OCCA-1.0 libParanumal-0.1.2 Please use the CEED user forum to report bugs or post questions or comments . Software Catalog The CEED team includes members of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects. Building on these efforts, the co-design center is producing a range of software products, including: Next-generation finite element discretization libraries that enable unstructured PDE-based applications to take full advantage of exascale resources. These libraries cover the full spectrum of discretizations, from assembled low-order to matrix-free high-order methods. Miniapps combining applications-relevant physics with key high-order kernels that use matrix-free forms for efficient performance. CEED also develops element-level kernels and benchmark problems . Broadly applicable technologies, including extensions of dense linear algebra libraries to support fast tensor contractions , scalable matrix-free linear solvers and programming models for performance portability .","title":"Catalog"},{"location":"software/#latest-releases","text":"The CEED software is open-source and publicly available on GitHub . The latest releases of our main software components are available from: CEED-2.0 libCEED-0.2 FMS-0.1 Nek5000-17.0 mfem-4.0 Laghos-2.0 Nekbone-17.0 OCCA-1.0 libParanumal-0.1.2 Please use the CEED user forum to report bugs or post questions or comments .","title":"Latest Releases"},{"location":"software/#software-catalog","text":"The CEED team includes members of the Nek5000 , MFEM , MAGMA , OCCA and PETSc projects. Building on these efforts, the co-design center is producing a range of software products, including: Next-generation finite element discretization libraries that enable unstructured PDE-based applications to take full advantage of exascale resources. These libraries cover the full spectrum of discretizations, from assembled low-order to matrix-free high-order methods. Miniapps combining applications-relevant physics with key high-order kernels that use matrix-free forms for efficient performance. CEED also develops element-level kernels and benchmark problems . Broadly applicable technologies, including extensions of dense linear algebra libraries to support fast tensor contractions , scalable matrix-free linear solvers and programming models for performance portability .","title":"Software Catalog"},{"location":"sw/","text":"Software Thrust The goal of CEED's Software (SW) thrust, led by Jed Brown from University of Colorado Boulder , is to participate in the development of software libraries and frameworks of general interest to the scientific computing community, facilitate collaboration between CEED software packages, enable integration into and/or interoperability with overall ECP software technologies stack, streamline developer and user workflows, maintain testing and benchmarking infrastructure, and coordinate CEED software releases. In addition to maintaining a close connection with ECP software technologies projects, the SW thrust develops continuous integration and performance regression testing for CEED, helps with the benchmarking suite and implements support for package managers, such as Spack . Members of the SW team are also actively involved with the matrix-free solvers work and the efforts in general interpolation and visualization of high-order meshes and functions . The SW thrust participates in a variety of co-design activities such as the coordination of the design of CEED's APIs and the identification of common kernels and their regimes of relevance relative to the parent application.","title":"Software"},{"location":"sw/#software-thrust","text":"The goal of CEED's Software (SW) thrust, led by Jed Brown from University of Colorado Boulder , is to participate in the development of software libraries and frameworks of general interest to the scientific computing community, facilitate collaboration between CEED software packages, enable integration into and/or interoperability with overall ECP software technologies stack, streamline developer and user workflows, maintain testing and benchmarking infrastructure, and coordinate CEED software releases. In addition to maintaining a close connection with ECP software technologies projects, the SW thrust develops continuous integration and performance regression testing for CEED, helps with the benchmarking suite and implements support for package managers, such as Spack . Members of the SW team are also actively involved with the matrix-free solvers work and the efforts in general interpolation and visualization of high-order meshes and functions . The SW thrust participates in a variety of co-design activities such as the coordination of the design of CEED's APIs and the identification of common kernels and their regimes of relevance relative to the parent application.","title":"Software Thrust "},{"location":"thrusts/","text":"R&D Thrusts CEED scientists work closely with hardware vendors, algorithm and software developers, and collaborate with application scientists to meet their needs. Our co-design efforts are organized in four interconnected R&D thrusts focused on these customers and tied together by the foundational finite element thrust . The specific goals and responsibilities of each thrust are described below. You can find our publications and related documents on the Outreach page . Applications Thrust (AP) The goal of CEED's Applications thrust is to impact a wide range of ECP application teams through focused one-on-one interactions, facilitated by CEED application liaisons, as well as through one-to-many interactions, based on the development of easy-to-use discretization libraries for high-order finite element methods. Hardware Thrust (HW) The goal of CEED's Hardware thrust is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope. Software Thrust (SW) The goal of CEED's Software thrust is to participate in the development of software libraries and frameworks of general interest to the scientific computing community, facilitate collaboration between CEED software packages, enable integration into and/or interoperability with overall ECP software technologies stack, streamline developer and user workflows, maintain testing and benchmarking infrastructure, and coordinate CEED software releases. Finite Elements Thrust (FE) The goal of CEED's Finite Element thrust is to continue to improve the state-of-the-art spectral element/high-order finite element algorithms and kernels in the CEED software targeting exascale architectures, connect and contribute to the efforts of the other thrusts, and lead the development of discretization libraries, benchmarks and miniapps.","title":"R&D Thrusts"},{"location":"thrusts/#rd-thrusts","text":"CEED scientists work closely with hardware vendors, algorithm and software developers, and collaborate with application scientists to meet their needs. Our co-design efforts are organized in four interconnected R&D thrusts focused on these customers and tied together by the foundational finite element thrust . The specific goals and responsibilities of each thrust are described below. You can find our publications and related documents on the Outreach page .","title":"R&amp;D Thrusts"},{"location":"thrusts/#applications-thrust-ap","text":"The goal of CEED's Applications thrust is to impact a wide range of ECP application teams through focused one-on-one interactions, facilitated by CEED application liaisons, as well as through one-to-many interactions, based on the development of easy-to-use discretization libraries for high-order finite element methods.","title":"Applications Thrust (AP)"},{"location":"thrusts/#hardware-thrust-hw","text":"The goal of CEED's Hardware thrust is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope.","title":"Hardware Thrust (HW)"},{"location":"thrusts/#software-thrust-sw","text":"The goal of CEED's Software thrust is to participate in the development of software libraries and frameworks of general interest to the scientific computing community, facilitate collaboration between CEED software packages, enable integration into and/or interoperability with overall ECP software technologies stack, streamline developer and user workflows, maintain testing and benchmarking infrastructure, and coordinate CEED software releases.","title":"Software Thrust (SW)"},{"location":"thrusts/#finite-elements-thrust-fe","text":"The goal of CEED's Finite Element thrust is to continue to improve the state-of-the-art spectral element/high-order finite element algorithms and kernels in the CEED software targeting exascale architectures, connect and contribute to the efforts of the other thrusts, and lead the development of discretization libraries, benchmarks and miniapps.","title":"Finite Elements Thrust (FE)"},{"location":"vis/","text":"Visualization of High-Order Meshes and Functions Accurate visualization of general finite element meshes and functions in the de Rham complex requires finite element knowledge that may not be present in visualization tools employed by applications. The visualization needs to account for the orders of the mesh and solution fields, as well as the type of finite element basis used for each of them. An additional challenge for high-order meshes and functions is that there is no common community standard for the description of high-order data at arbitrary other. CEED is working with visualization and application teams to develop a standard that not only improves visualization capabilities but also enables consistent data transfer between high-order applications. Our work is based on the current capabilities in MFEM , illustrated in its native GLVis visualization tool, as well as in the VisIt visualization and data analysis application. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Visualization"},{"location":"vis/#visualization-of-high-order-meshes-and-functions","text":"Accurate visualization of general finite element meshes and functions in the de Rham complex requires finite element knowledge that may not be present in visualization tools employed by applications. The visualization needs to account for the orders of the mesh and solution fields, as well as the type of finite element basis used for each of them. An additional challenge for high-order meshes and functions is that there is no common community standard for the description of high-order data at arbitrary other. CEED is working with visualization and application teams to develop a standard that not only improves visualization capabilities but also enables consistent data transfer between high-order applications. Our work is based on the current capabilities in MFEM , illustrated in its native GLVis visualization tool, as well as in the VisIt visualization and data analysis application. This is an active area of research for our team and we are interested in collaboration. Stay tuned for more details...","title":"Visualization of High-Order Meshes and Functions "}]}