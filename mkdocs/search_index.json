{
    "docs": [
        {
            "location": "/", 
            "text": "ECP Co-Design\n\n\nThe Center for Efficient Exascale Discretizations (CEED) is an \nExascale\nComputing Project (ECP)\n co-design center focused on\nthe following goals:\n\n\n\n\n\n\nHelp applications leverage future architectures by providing them with\n  \nstate-of-the-art discretization algorithms\n that better exploit\n  the hardware and deliver a significant performance gain over conventional\n  methods.\n\n\n\n\n\n\nLearn from and educate hardware vendors and software technologies projects\n  about efficient finite element algorithms through CEED-developed \nproxies and\n  miniapps\n.\n\n\n\n\n\n\nProvide an efficient and user-friendly unstructured PDE discretization\n  component for the future \nexascale software ecosystem\n.\n\n\n\n\n\n\nCEED is a research partnership involving \n30+ computational scientists\n\nfrom two DOE labs and and five universities.\n\n\nExascale Discretizations\n\n\nCEED's research and development efforts are centered around the following\ncomputational motifs and their performance on exascale hardware:\n\n\nPDE-based simulations on unstructured grids\n\n\n\n\nGeneral finite element algorithms on triangular, quadrilateral, tetrahedral and hexahedral meshes in 1D, 2D and 3D.\n\n\nSupport for the whole de Rham complex: H1, H(curl), H(div) and L2/DG spaces and discretizations.\n\n\nConforming and non-conforming unstructured AMR.\n\n\nOptimized assembly support for low-order methods.\n\n\n\n\nHigh-order/spectral finite elements\n\n\n\n\nAny order space on any order mesh.\n\n\nHigh-order conforming and non-conforming curved meshes.\n\n\nEfficient matrix-free operator evaluation on all mesh geometries for all spaces in the de Rham complex.\n\n\n\n\nWhat's New?\n\n\nRecent activities of the center are listed below. For additional updates, see the \nNews\n page.\n\n\n\n\nFirst CEED annual meeting to be held at LLNL\n\n(Aug 15-17, 2017)\n\n\nWorkshop on batched BLAS\n (Feb 23-25, 2017)\n\n\nSoftware release: MFEM v3.3\n (Jan 28, 2017)\n\n\nCEED co-design center announced\n (Nov 11, 2016)\n\n\n\n\n\n\n\n\n\n\nSoftware\n\n\nThe CEED team includes members of the \nNek5000\n, \nMFEM\n, \nMAGMA\n, \nOCCA\n and \nPETSc\n projects.\nBuilding on these efforts, the co-design center will produce a range of software products, including:\n\n\n\n\n\n\nNext-generation \nfinite element\n \ndiscretization\n \nlibraries\n that enable unstructured PDE-based applications to take\n   full advantage of exascale resources. These libraries will cover the full spectrum of discretizations, from assembled\n   low-order to matrix-free high-order methods.\n\n\n\n\n\n\nMiniapps\n combining applications-relevant physics with key high-order kernels that use matrix-free forms\n   for efficient performance. CEED will also develop element-level kernels and \nbenchmark problems\n.\n\n\n\n\n\n\nBroadly applicable technologies, including extensions of dense linear algebra libraries to support\n \nfast tensor contractions\n, scalable matrix-free \nlinear solvers\n and programming models for \nperformance portability\n.\n\n\n\n\n\n\nR\nD Thrusts\n\n\nCEED's scientists work closely with hardware vendors, algorithm and software developers,\nand collaborate with application scientists to meet their needs. These co-design efforts\nare organized in four interconnected R\nD thrusts:\n\n\nApplications Thrust (AP)\n:\nMaintain a close connection with ECP apps. Reach out to low-order and non-ECP apps.\nDerive requirements for CEED\u2019s miniapps and software technologies work.\n\n\nHardware Thrust (HW)\n:\nWork with the vendors to take advantage of existing hardware to impact applications.\nProvide them with relevant miniapps and hardware/software requirements for high-order apps.\nFollow and evaluate HW trends towards exascale.\n\n\nSoftware Thrust (SW)\n:\nParticipate in the development of software libraries and frameworks of general interest to the scientific computing community.\nEnable integration into and/or interoperability with overall ECP ST stack.\n\n\nFinite Elements Thrust (FE)\n:\nContinue to improve the state-of-the-art FE algorithms and kernels in CEED software for exascale efficiency.\nLead the development of discretization libraries and miniapps.\n\n\nContact\n\n\nYou can reach us by emailing \nceed-users@llnl.gov\n or by leaving a comment in\nthe \nCEED user forum\n.\n\n\nFor additional information, see the \nAbout\n page.", 
            "title": "Home"
        }, 
        {
            "location": "/#ecp-co-design", 
            "text": "The Center for Efficient Exascale Discretizations (CEED) is an  Exascale\nComputing Project (ECP)  co-design center focused on\nthe following goals:    Help applications leverage future architectures by providing them with\n   state-of-the-art discretization algorithms  that better exploit\n  the hardware and deliver a significant performance gain over conventional\n  methods.    Learn from and educate hardware vendors and software technologies projects\n  about efficient finite element algorithms through CEED-developed  proxies and\n  miniapps .    Provide an efficient and user-friendly unstructured PDE discretization\n  component for the future  exascale software ecosystem .    CEED is a research partnership involving  30+ computational scientists \nfrom two DOE labs and and five universities.", 
            "title": "ECP Co-Design"
        }, 
        {
            "location": "/#exascale-discretizations", 
            "text": "CEED's research and development efforts are centered around the following\ncomputational motifs and their performance on exascale hardware:  PDE-based simulations on unstructured grids   General finite element algorithms on triangular, quadrilateral, tetrahedral and hexahedral meshes in 1D, 2D and 3D.  Support for the whole de Rham complex: H1, H(curl), H(div) and L2/DG spaces and discretizations.  Conforming and non-conforming unstructured AMR.  Optimized assembly support for low-order methods.   High-order/spectral finite elements   Any order space on any order mesh.  High-order conforming and non-conforming curved meshes.  Efficient matrix-free operator evaluation on all mesh geometries for all spaces in the de Rham complex.", 
            "title": "Exascale Discretizations"
        }, 
        {
            "location": "/#whats-new", 
            "text": "Recent activities of the center are listed below. For additional updates, see the  News  page.   First CEED annual meeting to be held at LLNL \n(Aug 15-17, 2017)  Workshop on batched BLAS  (Feb 23-25, 2017)  Software release: MFEM v3.3  (Jan 28, 2017)  CEED co-design center announced  (Nov 11, 2016)", 
            "title": "What's New?"
        }, 
        {
            "location": "/#software", 
            "text": "The CEED team includes members of the  Nek5000 ,  MFEM ,  MAGMA ,  OCCA  and  PETSc  projects.\nBuilding on these efforts, the co-design center will produce a range of software products, including:    Next-generation  finite element   discretization   libraries  that enable unstructured PDE-based applications to take\n   full advantage of exascale resources. These libraries will cover the full spectrum of discretizations, from assembled\n   low-order to matrix-free high-order methods.    Miniapps  combining applications-relevant physics with key high-order kernels that use matrix-free forms\n   for efficient performance. CEED will also develop element-level kernels and  benchmark problems .    Broadly applicable technologies, including extensions of dense linear algebra libraries to support\n  fast tensor contractions , scalable matrix-free  linear solvers  and programming models for  performance portability .", 
            "title": "Software"
        }, 
        {
            "location": "/#rd-thrusts", 
            "text": "CEED's scientists work closely with hardware vendors, algorithm and software developers,\nand collaborate with application scientists to meet their needs. These co-design efforts\nare organized in four interconnected R D thrusts:  Applications Thrust (AP) :\nMaintain a close connection with ECP apps. Reach out to low-order and non-ECP apps.\nDerive requirements for CEED\u2019s miniapps and software technologies work.  Hardware Thrust (HW) :\nWork with the vendors to take advantage of existing hardware to impact applications.\nProvide them with relevant miniapps and hardware/software requirements for high-order apps.\nFollow and evaluate HW trends towards exascale.  Software Thrust (SW) :\nParticipate in the development of software libraries and frameworks of general interest to the scientific computing community.\nEnable integration into and/or interoperability with overall ECP ST stack.  Finite Elements Thrust (FE) :\nContinue to improve the state-of-the-art FE algorithms and kernels in CEED software for exascale efficiency.\nLead the development of discretization libraries and miniapps.", 
            "title": "R&amp;D Thrusts"
        }, 
        {
            "location": "/#contact", 
            "text": "You can reach us by emailing  ceed-users@llnl.gov  or by leaving a comment in\nthe  CEED user forum .  For additional information, see the  About  page.", 
            "title": "Contact"
        }, 
        {
            "location": "/news/", 
            "text": "News\n\n\n\n\nFirst CEED annual meeting to be held at LLNL\n\n\nCEED will hold its first annual meeting August 15-17, 2017 at the \nHPC\nInnovation Center\n\nof Lawrence Livermore National Laboratory.\n\n\nThe goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with hardware vendors and other ECP\nprojects, plan project activities and brainstorm/work as a group to make\ntechnical progress.\n\n\nIn addition to gathering together many of the CEED researchers (in-person or by\nvideoconferencing), the meeting will include representatives of the ECP\nmanagement, hardware vendors, software technology and other interested projects,\nat least for part of the meeting.\n\n\n\n\nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\n\nThe second \nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\nwas held in Atlanta, GA on February 23-25, 2017.\n\n\nThe goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.\n\n\nOf particular interest to CEED is in the use of batched BLAS for tensor\ncontractions, and thus, in the establishment of a batched BLAS standard,\nhighly-optimized implementations, and support from vendors on various\narchitectures.\n\n\nThis is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found \nhere\n.\n\n\nSoftware release: MFEM v3.3\n\n\nVersion 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods, was released on January 28, 2017 at \nhttp://mfem.org\n\n\nThe goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.\n\n\nIt has many features, including:\n\n\n\n\n2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.\n\n\nParallel version scalable to hundreds of thousands of MPI cores.\n\n\nConforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefenement and parallel load balancing.\n\n\nGalerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.\n\n\nSupport for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.\n\n\nScalable algebraic multigrid, time integrators, and eigensolvers.\n\n\nLightweight interactive OpenGL visualization with the MFEM-based \nGLVis\n tool.\n\n\n\n\nSome of the \nnew additions in version 3.3\n are:\n\n\n\n\nComprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the \nPETSc\n and \nSUNDIALS\n suites.\n\n\nLinear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.\n\n\nGeneral quadrature and nodal finite element basis types.\n\n\nScalable parallel mesh format.\n\n\nThirty six new integrators for common families of operators.\n\n\nSixteen new serial and parallel example codes.\n\n\nSupport for CMake, on-the-fly compression of file streams, and HDF5-based output following the \nConduit\n mesh blueprint specification.\n\n\n\n\nMFEM is being developed in \nCASC\n, \nLLNL\n and is freely available under LGPL 2.1.\nFor more details, see the \ninteractive documentation\n and the full \nCHANGELOG\n.\n\n\n\n\nCEED co-design center announced\n\n\nThe \nExascale Computing Project (ECP)\n announced on November 11, 2016 its selection\nof four \nco-design centers\n, including CEED: the Center for\nEfficient Exascale Discretizations, which is a \nresearch partnership\n between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).\n\n\nAdditional news coverage can be found in \nLLNL Newsline\n\nand the \nANL press release\n.", 
            "title": "News"
        }, 
        {
            "location": "/news/#news", 
            "text": "First CEED annual meeting to be held at LLNL  CEED will hold its first annual meeting August 15-17, 2017 at the  HPC\nInnovation Center \nof Lawrence Livermore National Laboratory.  The goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with hardware vendors and other ECP\nprojects, plan project activities and brainstorm/work as a group to make\ntechnical progress.  In addition to gathering together many of the CEED researchers (in-person or by\nvideoconferencing), the meeting will include representatives of the ECP\nmanagement, hardware vendors, software technology and other interested projects,\nat least for part of the meeting.   Workshop on Batched, Reproducible, and Reduced Precision BLAS  The second  Workshop on Batched, Reproducible, and Reduced Precision BLAS \nwas held in Atlanta, GA on February 23-25, 2017.  The goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.  Of particular interest to CEED is in the use of batched BLAS for tensor\ncontractions, and thus, in the establishment of a batched BLAS standard,\nhighly-optimized implementations, and support from vendors on various\narchitectures.  This is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found  here .  Software release: MFEM v3.3  Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods, was released on January 28, 2017 at  http://mfem.org  The goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.  It has many features, including:   2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.  Parallel version scalable to hundreds of thousands of MPI cores.  Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefenement and parallel load balancing.  Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.  Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.  Scalable algebraic multigrid, time integrators, and eigensolvers.  Lightweight interactive OpenGL visualization with the MFEM-based  GLVis  tool.   Some of the  new additions in version 3.3  are:   Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the  PETSc  and  SUNDIALS  suites.  Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.  General quadrature and nodal finite element basis types.  Scalable parallel mesh format.  Thirty six new integrators for common families of operators.  Sixteen new serial and parallel example codes.  Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the  Conduit  mesh blueprint specification.   MFEM is being developed in  CASC ,  LLNL  and is freely available under LGPL 2.1.\nFor more details, see the  interactive documentation  and the full  CHANGELOG .   CEED co-design center announced  The  Exascale Computing Project (ECP)  announced on November 11, 2016 its selection\nof four  co-design centers , including CEED: the Center for\nEfficient Exascale Discretizations, which is a  research partnership  between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).  Additional news coverage can be found in  LLNL Newsline \nand the  ANL press release .", 
            "title": "News"
        }, 
        {
            "location": "/bps/", 
            "text": "CEED Bake-off Problems (Benchmarks)\n\n\nThis page is under construction\n\n\n\n\nBake off problems specifications\n\n\nDatabase of results from CEED codes\n\n\nReference implementation in libCEED\n\n\nTarget audience is: ECP vendors, ST projects, other high-order codes\n\n\n\n\nSome terminology and notation\n\n\nVector representation/storage categories:\n\n\n\n\nTrue degrees of freedom/unknowns, \nT-vector\n:\n\n\neach unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$\n\n\nthis is a non-overlapping vector decomposition\n\n\nusually includes any essential (fixed) dofs.\n\n\n\n\n\n\n\nLocal true degrees of freedom/unknowns, \nL-vector\n:\n\n\neach unknown $i$ has exactly one copy on each processor that owns an\n  element contaning $i$\n\n\nthis is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor\n\n\nthe shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n  \n\n\n\n\n\n\nPer element decomposition, \nE-vector\n:\n\n\neach unknown $i$ has as many copies as the number of elements that contain\n  $i$\n\n\nusually, the copies of the unknowns are grouped by the element they belong\n  to.\n  \n\n\n\n\n\n\nIn the case of hanging nodes (giving rise to hanging dofs):\n\n\nthere is another represention similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs, \nH-vector\n\n\nthe additional hanging dofs are duplicated when they are shared by\n  multiple processors.\n\n\n\n\n\n\nIn the case of variable order spaces:\n\n\nthe dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case\n\n\nhave both L- and H-vector representations.\n\n\n\n\n\n\nQuadrature point vector, \nQ-vector\n:\n\n\nthis is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.\n\n\n\n\n\n\nIn many cases it is useful to distinguish two types of vectors:\n\n\nX-vector, or \nprimal\n X-vector, and X'-vector, or \ndual\n X-vector\n\n\nhere X can be any of the T, L, H, E, or Q categories\n\n\nfor example, the mass matrix operator maps a T-vector to a T'-vector\n\n\nthe solutions vector is a T-vector, and the RHS vector is a T'-vector\n\n\nusing the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.\n\n\n\n\n\n\n\n\nOperator representation/storage/action categories:\n\n\n\n\nFull true-dof parallel assembly, \nTA\n, or \nA\n:\n\n\nParCSR or similar format\n\n\nthe T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.\n\n\n\n\n\n\nFull local assembly, \nLA\n:\n\n\nCSR matrix on each rank\n\n\nthe parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action\n\n\nnote that $P$ is the operator mapping T-vectors to L-vectors.\n\n\n\n\n\n\nElement matrix assembly, \nEA\n:\n\n\neach element matrix is stored as a dense matrix\n\n\noptimized element and parallel prolongation operators\n\n\nnote that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.\n\n\n\n\n\n\nQuadrature-point/partial assembly, \nQA\n or \nPA\n:\n\n\nprecompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements\n\n\nthe stored data can be viewed as a Q-vector.\n\n\n\n\n\n\nUnassembled option,  \nUA\n or \nU\n:\n\n\nno assembly step\n\n\nthe action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).\n\n\n\n\n\n\n\n\nBake-off problem 1\n\n\nSetup:\n\n\n\n\nMass matrix\n\n\nCoefficient: constant $1$\n\n\n3D hex mesh\n\n\nNo essential boundary conditions\n\n\nSolution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?\n\n\nQuadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the qudrature order is $2q-1$\n\n\nAlso, consider $q=2$, for $p=1$, and $q=3$, for $p=2$\n\n\nNodal basis using $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension\n\n\nConsider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$\n\n\nFocus on the QA/PA operator representation\n\n\nMeshes: ...\n\n\nNumber of MPI tasks, and number of MPI tasks per node:\n\n\nOn ALCF's BG/Q, \ncetus\n: $2^{14}$ tasks total, with $2^5$ tasks per node\n\n\n\n\n\n\n...\n\n\n\n\nReport:\n\n\n\n\nNumber of mesh elements, $E$\n\n\nPolynomial degree, $N$, or $p$\n\n\nTotal number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$\n\n\nTime per iteration = total CG time $/$ number of CG iterations\n\n\nNumber of iterations to reach relative residual reduction of $10^{-6}$\n\n\nTime for quadrature-point/partial assembly\n\n\nTime is measured as maximum over all MPI ranks; using \nMPI_Wtime()\n or other\n\n\n...\n\n\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/bps/#ceed-bake-off-problems-benchmarks", 
            "text": "This page is under construction   Bake off problems specifications  Database of results from CEED codes  Reference implementation in libCEED  Target audience is: ECP vendors, ST projects, other high-order codes   Some terminology and notation  Vector representation/storage categories:   True degrees of freedom/unknowns,  T-vector :  each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$  this is a non-overlapping vector decomposition  usually includes any essential (fixed) dofs.    Local true degrees of freedom/unknowns,  L-vector :  each unknown $i$ has exactly one copy on each processor that owns an\n  element contaning $i$  this is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor  the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n      Per element decomposition,  E-vector :  each unknown $i$ has as many copies as the number of elements that contain\n  $i$  usually, the copies of the unknowns are grouped by the element they belong\n  to.\n      In the case of hanging nodes (giving rise to hanging dofs):  there is another represention similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs,  H-vector  the additional hanging dofs are duplicated when they are shared by\n  multiple processors.    In the case of variable order spaces:  the dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case  have both L- and H-vector representations.    Quadrature point vector,  Q-vector :  this is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.    In many cases it is useful to distinguish two types of vectors:  X-vector, or  primal  X-vector, and X'-vector, or  dual  X-vector  here X can be any of the T, L, H, E, or Q categories  for example, the mass matrix operator maps a T-vector to a T'-vector  the solutions vector is a T-vector, and the RHS vector is a T'-vector  using the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.     Operator representation/storage/action categories:   Full true-dof parallel assembly,  TA , or  A :  ParCSR or similar format  the T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.    Full local assembly,  LA :  CSR matrix on each rank  the parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action  note that $P$ is the operator mapping T-vectors to L-vectors.    Element matrix assembly,  EA :  each element matrix is stored as a dense matrix  optimized element and parallel prolongation operators  note that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.    Quadrature-point/partial assembly,  QA  or  PA :  precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements  the stored data can be viewed as a Q-vector.    Unassembled option,   UA  or  U :  no assembly step  the action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).     Bake-off problem 1  Setup:   Mass matrix  Coefficient: constant $1$  3D hex mesh  No essential boundary conditions  Solution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?  Quadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the qudrature order is $2q-1$  Also, consider $q=2$, for $p=1$, and $q=3$, for $p=2$  Nodal basis using $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension  Consider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$  Focus on the QA/PA operator representation  Meshes: ...  Number of MPI tasks, and number of MPI tasks per node:  On ALCF's BG/Q,  cetus : $2^{14}$ tasks total, with $2^5$ tasks per node    ...   Report:   Number of mesh elements, $E$  Polynomial degree, $N$, or $p$  Total number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$  Time per iteration = total CG time $/$ number of CG iterations  Number of iterations to reach relative residual reduction of $10^{-6}$  Time for quadrature-point/partial assembly  Time is measured as maximum over all MPI ranks; using  MPI_Wtime()  or other  ...   MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED Bake-off Problems (Benchmarks)"
        }, 
        {
            "location": "/miniapps/", 
            "text": "CEED Miniapps\n\n\nThis page is under construction\n\n\n\n\nList of miniapps, with documentation and downloads\n\n\n\n\nNekbone:\n\n\n\n\nNek5000 miniapp, solving Poisson equation with conjugate gradident\n\n\nDocumentation can be found in Http://withub.com/Nek5000/NekDoc\n\n\nDownload from github http://github.com/Nek5000/Nekbone\n\n\n\n\n\n\n\n\nDatabase of results/expectations\n\n\n\n\nTarget audience is: ECP vendors, ST projects", 
            "title": "Miniapps"
        }, 
        {
            "location": "/miniapps/#ceed-miniapps", 
            "text": "This page is under construction   List of miniapps, with documentation and downloads   Nekbone:   Nek5000 miniapp, solving Poisson equation with conjugate gradident  Documentation can be found in Http://withub.com/Nek5000/NekDoc  Download from github http://github.com/Nek5000/Nekbone     Database of results/expectations   Target audience is: ECP vendors, ST projects", 
            "title": "CEED Miniapps"
        }, 
        {
            "location": "/ap/", 
            "text": "CEED Applications Thrust\n\n\nThis page is under construction\n\n\n\n\nMisun Min leads the AP thrust.\n\n\nThe goal of this thrust is to impact a wide range of ECP application teams through\n  focused one-on-one interactions, facilitated by CEED application liaisons, as well as\n  through one-to-many interactions, based on the development of easy-to-use discretization\n  libraries for high-order finite element methods.\n\n\nSome of the thrust activities (current and planned) are:\n\n\nOpenACC- and CUDA-based GPU version implementation for Nek5000 and Nek miniapps.\n\n\nI/O performance improvement for meshes with more than 10 millions of elements.\n\n\nExplore potential improvement in performance with lightweight MPI and neighborhood collective MPI.\n\n\n\n\n\n\nList ECP apps and external project with which we are collaborating on these topics:\n\n\nCoupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors - SMRs (ORNL).\n\n\nNext-gen Multi-physics Simulation Code - MARBL (LLNL).\n\n\nMultiscale Coupled Urban System (ANL).\n\n\nTransforming Combustion Science and Technology with Exascale Simulations (SNL).\n\n\nCloud-Resolving Climate Modeling of the Earths Water Cycle (SNL).", 
            "title": "Applications"
        }, 
        {
            "location": "/ap/#ceed-applications-thrust", 
            "text": "This page is under construction   Misun Min leads the AP thrust.  The goal of this thrust is to impact a wide range of ECP application teams through\n  focused one-on-one interactions, facilitated by CEED application liaisons, as well as\n  through one-to-many interactions, based on the development of easy-to-use discretization\n  libraries for high-order finite element methods.  Some of the thrust activities (current and planned) are:  OpenACC- and CUDA-based GPU version implementation for Nek5000 and Nek miniapps.  I/O performance improvement for meshes with more than 10 millions of elements.  Explore potential improvement in performance with lightweight MPI and neighborhood collective MPI.    List ECP apps and external project with which we are collaborating on these topics:  Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors - SMRs (ORNL).  Next-gen Multi-physics Simulation Code - MARBL (LLNL).  Multiscale Coupled Urban System (ANL).  Transforming Combustion Science and Technology with Exascale Simulations (SNL).  Cloud-Resolving Climate Modeling of the Earths Water Cycle (SNL).", 
            "title": "CEED Applications Thrust"
        }, 
        {
            "location": "/hw/", 
            "text": "CEED Hardware Thrust\n\n\nThis page is under construction\n\n\n\n\nJack Dongarra leads the HW thrust\n\n\nThe goal of this thrust is to build a two-way (\npull-and-push\n) collaboration with vendors, where the CEED team will develop hardware-aware technologies (\npull\n) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek (\npush\n) impact and improve hardware designs within the ECP scope.\n\n\nSome of the thrust activities (current and planned) are:\n\n\nMini-Applications Design and Development;\n\n\nOptimal Data Locality and Motion;\n\n\nEnhanced Scalability and Parallelism.\n\n\n\n\n\n\nList vendors and other ECP AD, ST and external project with which we are collaborating on these topics:\n\n\nVendors: Intel, Nvidia, IBM, ARM, and AMD;\n\n\nECP collaborators: OpenMPI, MPICH, KokkosKernels, SNLApp.", 
            "title": "Hardware"
        }, 
        {
            "location": "/hw/#ceed-hardware-thrust", 
            "text": "This page is under construction   Jack Dongarra leads the HW thrust  The goal of this thrust is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope.  Some of the thrust activities (current and planned) are:  Mini-Applications Design and Development;  Optimal Data Locality and Motion;  Enhanced Scalability and Parallelism.    List vendors and other ECP AD, ST and external project with which we are collaborating on these topics:  Vendors: Intel, Nvidia, IBM, ARM, and AMD;  ECP collaborators: OpenMPI, MPICH, KokkosKernels, SNLApp.", 
            "title": "CEED Hardware Thrust"
        }, 
        {
            "location": "/sw/", 
            "text": "CEED Software Thrust\n\n\nThis page is under construction\n\n\n\n\nJed Brown leads the SW thrust\n\n\nThe goal of this thrust is to facilitate collaboration between CEED software packages, streamline developer and user workflows,\n  maintain testing and benchmarking infrastructure, and coordinate CEED releases.\n\n\nSome of the thrust activities (current and planned) are\n\n\nDevelop continuous integration and performance regression testing\n\n\nIdentify common kernels and their regimes of relevance\n\n\nDevelop benchmarking suite for bake-off problems\n\n\nSpack support\n\n\nCoordinate libCEED API design\n\n\n\n\n\n\nThe SW thrust collaborates with the following projects\n\n\nECP ST: PETSc, MPICH, Open MPI", 
            "title": "Software"
        }, 
        {
            "location": "/sw/#ceed-software-thrust", 
            "text": "This page is under construction   Jed Brown leads the SW thrust  The goal of this thrust is to facilitate collaboration between CEED software packages, streamline developer and user workflows,\n  maintain testing and benchmarking infrastructure, and coordinate CEED releases.  Some of the thrust activities (current and planned) are  Develop continuous integration and performance regression testing  Identify common kernels and their regimes of relevance  Develop benchmarking suite for bake-off problems  Spack support  Coordinate libCEED API design    The SW thrust collaborates with the following projects  ECP ST: PETSc, MPICH, Open MPI", 
            "title": "CEED Software Thrust"
        }, 
        {
            "location": "/partners/", 
            "text": "CEED Partners\n\n\nThis page is under construction\n\n\nECP Partners\n\n\n\n\nSMRs\n\n\nMARBL\n\n\nACME\n\n\nUrban\n\n\nGEOS\n\n\nMPICH\n\n\nOpenMPI\n\n\nhypre\n\n\nSTRUMPACK\n\n\nALPINE/VTK-m\n\n\n...\n\n\n\n\nExternal Partners\n\n\n\n\nKAUST\n\n\nIntelligent Light\n\n\n...\n\n\n\n\nOther High-Order Projects\n\n\n\n\nNektar++\n\n\ndeal.ii", 
            "title": "Partners"
        }, 
        {
            "location": "/partners/#ceed-partners", 
            "text": "This page is under construction  ECP Partners   SMRs  MARBL  ACME  Urban  GEOS  MPICH  OpenMPI  hypre  STRUMPACK  ALPINE/VTK-m  ...   External Partners   KAUST  Intelligent Light  ...   Other High-Order Projects   Nektar++  deal.ii", 
            "title": "CEED Partners"
        }, 
        {
            "location": "/ceed-code/", 
            "text": "CEED APIs\n\n\nCEED will build on the efforts of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.\n\n\nHigh level API\n\n\nThe CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.\n\n\nGiven such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.\n\n\nThe global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nLow level API\n\n\nThe CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:\n\n\n\nWe take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as \nsum\nfactorization\n.\nIn the case where we pre-compute and store the $D$ matrix, we call the algorithm\n\npartial assembly\n.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED"
        }, 
        {
            "location": "/ceed-code/#ceed-apis", 
            "text": "CEED will build on the efforts of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.  High level API  The CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.  Given such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.  The global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.  This API is currently under development. Stay tuned for more details...  Low level API  The CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:  We take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as  sum\nfactorization .\nIn the case where we pre-compute and store the $D$ matrix, we call the algorithm partial assembly .  This API is currently under development. Stay tuned for more details...  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/nek/", 
            "text": "Nek\n\n\nNek5000/NekCEM is open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.\n\n\nIt features state-of-the-art, scalable, high-order spectral-element based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.\n\n\nNek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories, and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.\n\n\n\n\nReactor analysis\n\n\nAerospace application\n\n\nEngine application\n\n\nOcean modeling\n\n\nTurbulence modeling\n\n\nElectromagnetics modeling\n\n\nDrift-diffusion  modeling\n\n\n\n\nNek is freely available under a BSD license.\n\n\nFor more information, see the Nek5000 website: \nhttps://nek5000.mcs.anl.gov\n.", 
            "title": "Nek"
        }, 
        {
            "location": "/nek/#nek", 
            "text": "Nek5000/NekCEM is open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.  It features state-of-the-art, scalable, high-order spectral-element based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.  Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories, and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.   Reactor analysis  Aerospace application  Engine application  Ocean modeling  Turbulence modeling  Electromagnetics modeling  Drift-diffusion  modeling   Nek is freely available under a BSD license.  For more information, see the Nek5000 website:  https://nek5000.mcs.anl.gov .", 
            "title": "Nek"
        }, 
        {
            "location": "/mfem/", 
            "text": "MFEM\n \n\n\nMFEM is a free, lightweight, scalable C++ library for finite element methods.\n\n\nIts goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.\n\n\nIts features include:\n\n\n\n\nArbitrary high-order finite element \nmeshes\n\nand \nspaces\n.\n\n\nWide variety\n of finite element discretization approaches.\n\n\nConforming and nonconforming \nadaptive mesh refinement\n.\n\n\nScalable to \nhundreds of thousands\n of cores.\n\n\n... and \nmany more\n.\n\n\n\n\nMFEM is being developed in \nCASC\n,\n\nLLNL\n and is freely available under LGPL 2.1.\n\n\nFor more information, see the MFEM website: \nhttp://mfem.org\n.", 
            "title": "MFEM"
        }, 
        {
            "location": "/mfem/#mfem-wzxhzdk0", 
            "text": "MFEM is a free, lightweight, scalable C++ library for finite element methods.  Its goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.  Its features include:   Arbitrary high-order finite element  meshes \nand  spaces .  Wide variety  of finite element discretization approaches.  Conforming and nonconforming  adaptive mesh refinement .  Scalable to  hundreds of thousands  of cores.  ... and  many more .   MFEM is being developed in  CASC , LLNL  and is freely available under LGPL 2.1.  For more information, see the MFEM website:  http://mfem.org .", 
            "title": "MFEM "
        }, 
        {
            "location": "/magma/", 
            "text": "MAGMA\n \n\n\nThis page is under construction\n\n\n\nMAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures. MAGMA allows applications to\nfully exploit the power of current heterogeneous systems of multi/many-core CPUs\nand multi-GPUs to deliver the fastest possible time to accurate solution within\ngiven energy constraints.\n\n\nHybrid Algorithms\n\n\n\nMAGMA uses a hybridization methodology where algorithms of interest are split into tasks of\nvarying granularity and their execution scheduled over the available hardware components.\nScheduling can be static or dynamic. In either case, small non-parallelizable tasks, often on the\ncritical path, are scheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.\n\n\nPerformance and Energy Efficiency\n\n\n\n\nMAGMA Batched\n\n\n\n\nMAGMA Sparse\n\n\nMAGMA Development\n\n\nFor more information, see the MAGMA website: \nhttp://icl.cs.utk.edu/magma/\n.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#magma-wzxhzdk0", 
            "text": "This page is under construction  \nMAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures. MAGMA allows applications to\nfully exploit the power of current heterogeneous systems of multi/many-core CPUs\nand multi-GPUs to deliver the fastest possible time to accurate solution within\ngiven energy constraints.  Hybrid Algorithms  \nMAGMA uses a hybridization methodology where algorithms of interest are split into tasks of\nvarying granularity and their execution scheduled over the available hardware components.\nScheduling can be static or dynamic. In either case, small non-parallelizable tasks, often on the\ncritical path, are scheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.  Performance and Energy Efficiency   MAGMA Batched   MAGMA Sparse  MAGMA Development  For more information, see the MAGMA website:  http://icl.cs.utk.edu/magma/ .", 
            "title": "MAGMA "
        }, 
        {
            "location": "/occa/", 
            "text": "OCCA\n \n\n\nOCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and let the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.\n\n\nOCCA abstracts the device programming languages into one kernel language, the\nOCCA kernel language (OKL). OKL minimally extends C and restricts the user to\nwrite parallel code that is JIT compiled.\n\n\nOCCA is freely available under an MIT license.\n\n\nFor more information, see the OCCA website: \nhttp://libocca.org\n.", 
            "title": "OCCA"
        }, 
        {
            "location": "/occa/#occa-wzxhzdk0", 
            "text": "OCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and let the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.  OCCA abstracts the device programming languages into one kernel language, the\nOCCA kernel language (OKL). OKL minimally extends C and restricts the user to\nwrite parallel code that is JIT compiled.  OCCA is freely available under an MIT license.  For more information, see the OCCA website:  http://libocca.org .", 
            "title": "OCCA "
        }, 
        {
            "location": "/holmes/", 
            "text": "Holmes\n\n\nThis page is under construction", 
            "title": "Holmes"
        }, 
        {
            "location": "/holmes/#holmes", 
            "text": "This page is under construction", 
            "title": "Holmes"
        }, 
        {
            "location": "/petsc/", 
            "text": "PETSc\n\n\nPETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.\n\n\nAs part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.\n\n\nPETSc is freely available under a BSD license.\n\n\nFor more information, see the PETSc website: \nhttps://www.mcs.anl.gov/petsc/\n.", 
            "title": "PETSc"
        }, 
        {
            "location": "/petsc/#petsc", 
            "text": "PETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.  As part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.  PETSc is freely available under a BSD license.  For more information, see the PETSc website:  https://www.mcs.anl.gov/petsc/ .", 
            "title": "PETSc"
        }, 
        {
            "location": "/pumi/", 
            "text": "PUMI\n \n\n\nPUMI\n is an unstructured, distributed mesh data management system designed for massively parallel computing.\n\n\nPUMI supports a full range of operations on unstructured meshes on\nmassively parallel computers consisting of five libraries:\n\n\n\n\nPCU\n for phased message passing and thread management.\n\n\nGMI\n for geometric model interface.\n\n\nMDS\n for unstructured mesh representation.\n\n\nAPF Mesh\n for partition model and distributed mesh management.\n\n\nAPF_Field\n for field management.\n\n\n\n\nPUMI is being developed at RPI's \nScientific Computation Research Center\n and is currently being used on projects sponsored by the DOE, NSF, Army, NASA, IBM and several companies.\n\n\nFor more information, see the \nPUMI documents\n.", 
            "title": "PUMI"
        }, 
        {
            "location": "/pumi/#pumi-wzxhzdk0", 
            "text": "PUMI  is an unstructured, distributed mesh data management system designed for massively parallel computing.  PUMI supports a full range of operations on unstructured meshes on\nmassively parallel computers consisting of five libraries:   PCU  for phased message passing and thread management.  GMI  for geometric model interface.  MDS  for unstructured mesh representation.  APF Mesh  for partition model and distributed mesh management.  APF_Field  for field management.   PUMI is being developed at RPI's  Scientific Computation Research Center  and is currently being used on projects sponsored by the DOE, NSF, Army, NASA, IBM and several companies.  For more information, see the  PUMI documents .", 
            "title": "PUMI "
        }, 
        {
            "location": "/gslib/", 
            "text": "gslib\n\n\nThis page is under construction\n\n\nA library for Gather/Scatter-type nearest neighbor data exchanges for parallel spectral element computations.\n\n\nAdditional features include:\n\n\n\n\nXXT solver (parallel direct solver)\n\n\nAMG solver\n\n\nRobust spectral element interpolation for a given set of points\n\n\n\n\ngslib's applications include \nNek5000\n and \nNektar++\n.\n\n\nFor more information, see the gslib GitHub repository: \nhttps://github.com/gslib/gslib\n.", 
            "title": "gslib"
        }, 
        {
            "location": "/gslib/#gslib", 
            "text": "This page is under construction  A library for Gather/Scatter-type nearest neighbor data exchanges for parallel spectral element computations.  Additional features include:   XXT solver (parallel direct solver)  AMG solver  Robust spectral element interpolation for a given set of points   gslib's applications include  Nek5000  and  Nektar++ .  For more information, see the gslib GitHub repository:  https://github.com/gslib/gslib .", 
            "title": "gslib"
        }, 
        {
            "location": "/fe/", 
            "text": "CEED Finite Element Thrust\n\n\nThis page is under construction\n\n\n\n\nVeselin Dobrev leads the FE thrust\n\n\nThe goal of this thrust is...\n\n\nSome of the thrust activities (current and planned) are...\n\n\nList ECP AD, ST and external project with which we are collaborating on these topics\n\n\n\"Why high-order?\" manifesto", 
            "title": "Finite Elements"
        }, 
        {
            "location": "/fe/#ceed-finite-element-thrust", 
            "text": "This page is under construction   Veselin Dobrev leads the FE thrust  The goal of this thrust is...  Some of the thrust activities (current and planned) are...  List ECP AD, ST and external project with which we are collaborating on these topics  \"Why high-order?\" manifesto", 
            "title": "CEED Finite Element Thrust"
        }, 
        {
            "location": "/formats/", 
            "text": "Formats\n\n\nThis page is under construction\n\n\n\n\nproposed format for HO mesh/data description for vis/data exchange\n\n\nlibCEED proposed format for efficient HO operator descriptions", 
            "title": "Formats"
        }, 
        {
            "location": "/formats/#formats", 
            "text": "This page is under construction   proposed format for HO mesh/data description for vis/data exchange  libCEED proposed format for efficient HO operator descriptions", 
            "title": "Formats"
        }, 
        {
            "location": "/pubs/", 
            "text": "CEED Documents\n\n\n\n\nHigh-order benchmarks: \nbake-off problems\n.\n\n\nHigh-order \nminiapps\n.\n\n\nHigh-order \nformats\n.\n\n\nActivities in the \nApplications\n, \nHardware\n, \nSoftware\n and \nFinite Element\n thrusts.\n\n\n\n\nPublications\n\n\n\n\nA. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, \nFactorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures\n, \nProceedings of the 2017 International Conference on Computational Science, ICCS'17\n, Z\u00fcrich, Switzerland, June 12-14, \nProcedia Computer Science\n, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, \nSmall Tensor Operations on Advanced Architectures for High-order Applications\n, Technical report UT-EECS-17-749, EECS Department, Univerity of Tennessee, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nHigh-performance Tensor Contractions for GPUs\n, \nProcedia Computer Science\n, Volume 80, Pages 108-118, ISSN 1877-0509, \n2016\n.\n\n\nM. B.E., Y. Peet, P. Fischer, and J. Lottes, \nA spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations\n, \nJ. Comp. Phys.\n, 307:60\u201393, \n2016\n.\n\n\nM. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, \nAn MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication\n, \nThe International Journal of High Performance Computing Application\n, 30(3):320\u2013334, \n2016\n.\n\n\nJ. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, \nNekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations\n, \nSpecial issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing\n, \n2016\n.\n\n\nP. Fischer, K. Heisey, and M. Min, \nScaling limits for PDE-based simulation\n, \nIn 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation\n, AIAA 2015-3049, \n2015\n.\n\n\nA. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, \nErosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches\n, volume 2, pp. 1448\u20131461. \nAmerican Nuclear Society\n, \n2015\n.\n\n\nE. Merzari, P. Fischer, and J. Walker, \nLarge-scale simulation of rod bundles: Coherent structure recognition and stability analysis\n, volume 1. \nAmerican Society of Mechanical Engineers\n, \n2015\n.\n\n\nM. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, \nEntanglement of two, three and four plasmonically coupled quantum dots\n, \nPhysical Review B\n, 92:125432, \n2015\n.\n\n\nD. A. May, J. Brown, and L. Le Pourhiet. \npTatin3D: High-performance methods for long-term lithospheric dynamics\n, In Proceedings of \nSC14: International Conference for High Performance Computing, Networking, Storage and Analysis\n. ACM, \n2014\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, \nMonotonicity in high-order curvilinear finite element ALE remap\n, \nInt. J. Numer. Meth. Fluids\n, 77(5), pp. 249\u2013273, \n2014\n.\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG solver for H(div) problems\n,  \nSIAM J. Sci. Comp.\n, 34, pp. A3079\u2013A3098, \n2012\n.\n\n\nV. Dobrev, Tz. Kolev and R. Rieben, \nHigh-order curvilinear finite element methods for Lagrangian hydrodynamics\n, \nSIAM J. Sci. Comp.\n, 34, pp. B606\u2013B641, \n2012\n.\n\n\nJ. Brown, \nEfficient nonlinear solvers for nodal high-order finite elements in 3D\n, \nJournal of Scientific Computing\n, 45:48\u201363, \n2010\n. doi:10.1007/s10915-010-9396-8\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG for H(curl) problems\n, \nJ. Comput. Math.\n, 27, pp. 604-623, \n2009\n.\n\n\n\n\nPresentations\n\n\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nAccelerating Tensor Contractions in High-Order FEM with MAGMA Batched\n, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nP. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nM. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\n\n\nOther Resources\n\n\n\n\nNews coverage of CEED announcement in  \nLLNL Newsline\n\nand the \nANL press release\n.\n\n\nLLNL's \nexascale computing website\n.", 
            "title": "Publications"
        }, 
        {
            "location": "/pubs/#ceed-documents", 
            "text": "High-order benchmarks:  bake-off problems .  High-order  miniapps .  High-order  formats .  Activities in the  Applications ,  Hardware ,  Software  and  Finite Element  thrusts.", 
            "title": "CEED Documents"
        }, 
        {
            "location": "/pubs/#publications", 
            "text": "A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra,  Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures ,  Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14,  Procedia Computer Science ,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov,  Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, Univerity of Tennessee,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  High-performance Tensor Contractions for GPUs ,  Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509,  2016 .  M. B.E., Y. Peet, P. Fischer, and J. Lottes,  A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations ,  J. Comp. Phys. , 307:60\u201393,  2016 .  M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min,  An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication ,  The International Journal of High Performance Computing Application , 30(3):320\u2013334,  2016 .  J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min,  Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations ,  Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing ,  2016 .  P. Fischer, K. Heisey, and M. Min,  Scaling limits for PDE-based simulation ,  In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049,  2015 .  A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer,  Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461.  American Nuclear Society ,  2015 .  E. Merzari, P. Fischer, and J. Walker,  Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1.  American Society of Mechanical Engineers ,  2015 .  M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray,  Entanglement of two, three and four plasmonically coupled quantum dots ,  Physical Review B , 92:125432,  2015 .  D. A. May, J. Brown, and L. Le Pourhiet.  pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of  SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM,  2014 .  R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben,  Monotonicity in high-order curvilinear finite element ALE remap ,  Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273,  2014 .  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG solver for H(div) problems ,   SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098,  2012 .  V. Dobrev, Tz. Kolev and R. Rieben,  High-order curvilinear finite element methods for Lagrangian hydrodynamics ,  SIAM J. Sci. Comp. , 34, pp. B606\u2013B641,  2012 .  J. Brown,  Efficient nonlinear solvers for nodal high-order finite elements in 3D ,  Journal of Scientific Computing , 45:48\u201363,  2010 . doi:10.1007/s10915-010-9396-8  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG for H(curl) problems ,  J. Comput. Math. , 27, pp. 604-623,  2009 .", 
            "title": "Publications"
        }, 
        {
            "location": "/pubs/#presentations", 
            "text": "A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched ,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .", 
            "title": "Presentations"
        }, 
        {
            "location": "/pubs/#other-resources", 
            "text": "News coverage of CEED announcement in   LLNL Newsline \nand the  ANL press release .  LLNL's  exascale computing website .", 
            "title": "Other Resources"
        }, 
        {
            "location": "/about/", 
            "text": "About CEED\n\n\nThe Center for Efficient Exascale Discretizations is a research partnership\nbetween two DOE labs and five universities:\n\n\n\n\nArgonne National Laboratory\n\n\nUniversity of Colorado Boulder\n\n\nUniversity of Illinois Urbana-Champaign\n\n\nLawrence Livermore National Laboratory\n\n\nRensselaer Polytechnic Institute\n\n\nThe University of Tennessee, Knoxville\n\n\nVirginia Tech\n\n\n\n\n\n\nThis research was supported by the \nExascale Computing Project\n (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a capable exascale ecosystem, including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s exascale computing imperative.\n\n\n\n\nOur Team\n\n\n\n\nAhmad Abdelfattah\n\n\nAleks Obabko\n\n\nAli Karakus\n\n\nAndrew Siegel\n\n\nAzzam Haidar\n\n\nBarry Smith\n\n\nCameron Smith\n\n\nDavid Beckingsale\n\n\nDavid Medina\n\n\nIan Karlin\n\n\nJack Dongarra\n \n Lead for the \nHardware\n thrust\n\n\nJed Brown\n \n Lead for the \nSoftware\n thrust\n\n\nKatie Heisey\n\n\nKazem Kamran\n\n\nMark Shepard\n\n\nMatt Otten\n\n\nMisun Min\n \n Lead for the \nApplications\n thrust\n\n\nPanayot Vassilevski\n\n\nPaul Fischer\n \n Deputy Director of CEED\n\n\nRobert Rieben\n\n\nRon Rahaman\n\n\nScott Parker\n\n\nStanimire Tomov\n\n\nStefan Kerkemeier\n\n\nSom Dutta\n\n\nThilina Ratnayake\n\n\nTim Warburton\n\n\nTim Moon\n\n\nTzanio Kolev\n \n Director of CEED\n\n\nVeselin Dobrev\n \n Lead for the \nFinite Element\n thrust\n\n\nVladimir Tomov\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nBootstrap\n\nand \nBootswatch\n. Hosted on \nGitHub\n.\n\n\nLLNL-WEB-XXXXXX.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-ceed", 
            "text": "The Center for Efficient Exascale Discretizations is a research partnership\nbetween two DOE labs and five universities:   Argonne National Laboratory  University of Colorado Boulder  University of Illinois Urbana-Champaign  Lawrence Livermore National Laboratory  Rensselaer Polytechnic Institute  The University of Tennessee, Knoxville  Virginia Tech    This research was supported by the  Exascale Computing Project  (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a capable exascale ecosystem, including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s exascale computing imperative.", 
            "title": "About CEED"
        }, 
        {
            "location": "/about/#our-team", 
            "text": "Ahmad Abdelfattah  Aleks Obabko  Ali Karakus  Andrew Siegel  Azzam Haidar  Barry Smith  Cameron Smith  David Beckingsale  David Medina  Ian Karlin  Jack Dongarra    Lead for the  Hardware  thrust  Jed Brown    Lead for the  Software  thrust  Katie Heisey  Kazem Kamran  Mark Shepard  Matt Otten  Misun Min    Lead for the  Applications  thrust  Panayot Vassilevski  Paul Fischer    Deputy Director of CEED  Robert Rieben  Ron Rahaman  Scott Parker  Stanimire Tomov  Stefan Kerkemeier  Som Dutta  Thilina Ratnayake  Tim Warburton  Tim Moon  Tzanio Kolev    Director of CEED  Veselin Dobrev    Lead for the  Finite Element  thrust  Vladimir Tomov    Website built with  MkDocs ,  Bootstrap \nand  Bootswatch . Hosted on  GitHub .  LLNL-WEB-XXXXXX.", 
            "title": "Our Team"
        }
    ]
}