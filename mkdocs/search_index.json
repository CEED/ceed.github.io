{
    "docs": [
        {
            "location": "/", 
            "text": "Exascale Co-Design\n\n\nThe \nCenter for Efficient Exascale Discretizations (CEED)\n is a co-design center within the\nU.S. Department of Energy (DOE) \nExascale Computing Project (ECP)\n\nwith the following goals:\n\n\n\n\n\n\nHelp applications leverage future architectures by providing them with\n  \nstate-of-the-art discretization algorithms\n that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.\n\n\n\n\n\n\nCollaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed \nproxies and miniapps\n.\n\n\n\n\n\n\nProvide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming \nexascale software ecosystem\n.\n\n\n\n\n\n\nCEED is a research partnership involving \n30+ computational scientists\n from\ntwo DOE labs and five universities, including members of the \nNek5000\n,\n\nMFEM\n, \nMAGMA\n, \nOCCA\n and \nPETSc\n projects.\nYou can reach us by emailing \nceed-users@llnl.gov\n\nor by leaving a comment in the \nCEED user forum\n.\n\n\nThe center's co-design efforts are organized in \nfour interconnected R\nD\nthrusts\n, focused on the following computational motifs and their\nperformance on \nexascale hardware\n.\nSee also our \npublications\n.\n\n\n\n\nPDE-BASED SIMULATIONS ON UNSTRUCTURED GRIDS\n\n\nCEED is producing a range of \nsoftware products\n supporting general\n\nfinite element\n algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H\n1\n, H(curl), H(div) and L\n2\n/DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).\n\n\n\n\nHIGH-ORDER/SPECTRAL FINITE ELEMENTS\n\n\nOur algorithms and software come with comprehensive high-order support: we provide\n\nefficient matrix-free operator evaluation\n for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "Home"
        }, 
        {
            "location": "/#exascale-co-design", 
            "text": "The  Center for Efficient Exascale Discretizations (CEED)  is a co-design center within the\nU.S. Department of Energy (DOE)  Exascale Computing Project (ECP) \nwith the following goals:    Help applications leverage future architectures by providing them with\n   state-of-the-art discretization algorithms  that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.    Collaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed  proxies and miniapps .    Provide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming  exascale software ecosystem .    CEED is a research partnership involving  30+ computational scientists  from\ntwo DOE labs and five universities, including members of the  Nek5000 , MFEM ,  MAGMA ,  OCCA  and  PETSc  projects.\nYou can reach us by emailing  ceed-users@llnl.gov \nor by leaving a comment in the  CEED user forum .  The center's co-design efforts are organized in  four interconnected R D\nthrusts , focused on the following computational motifs and their\nperformance on  exascale hardware .\nSee also our  publications .", 
            "title": "Exascale Co-Design"
        }, 
        {
            "location": "/#pde-based-simulations-on-unstructured-grids", 
            "text": "CEED is producing a range of  software products  supporting general finite element  algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H 1 , H(curl), H(div) and L 2 /DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).", 
            "title": "PDE-BASED SIMULATIONS ON UNSTRUCTURED GRIDS"
        }, 
        {
            "location": "/#high-orderspectral-finite-elements", 
            "text": "Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation  for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "HIGH-ORDER/SPECTRAL FINITE ELEMENTS"
        }, 
        {
            "location": "/news/", 
            "text": "News\n\n\n\n\nFirst CEED annual meeting to be held at LLNL\n\n\nCEED will hold its first annual meeting in August, 2017 at the \nHPC Innovation\nCenter\n of\nLawrence Livermore National Laboratory.\n\n\nThe goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.\n\n\nIn addition to gathering together many of the CEED researchers, the meeting will\ninclude representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.\n\n\nPlease \ncontact the CEED team\n if you are interested\nin attending.\n\n\n\n\nGPU Hackathon 2017\n\n\nNek/CEED team will participate in the \nGPU Hackathon 2017\n\nto be held in Brookhaven National Laboratory on June 5-9, 2017.\n\n\nOur team will focus on performing and tuning GPU-enabled \nNek5000/Nekbone/NekCEM\n\nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.\n\n\n\n\nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\n\nThe second \nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED \nMAGMA\n team.\n\n\nThe goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.\n\n\nOf particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.\n\n\nThis is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found \nhere\n.\n\n\nSoftware release: MFEM v3.3\n\n\nVersion 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at \nhttp://mfem.org\n\n\nThe goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.\n\n\nIt has many features, including:\n\n\n\n\n2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.\n\n\nParallel version scalable to hundreds of thousands of MPI cores.\n\n\nConforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.\n\n\nGalerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.\n\n\nSupport for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.\n\n\nScalable algebraic multigrid, time integrators, and eigensolvers.\n\n\nLightweight interactive OpenGL visualization with the MFEM-based \nGLVis\n tool.\n\n\n\n\nSome of the \nnew additions in version 3.3\n are:\n\n\n\n\nComprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the \nPETSc\n and \nSUNDIALS\n suites.\n\n\nLinear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.\n\n\nGeneral quadrature and nodal finite element basis types.\n\n\nScalable parallel mesh format.\n\n\nThirty six new integrators for common families of operators.\n\n\nSixteen new serial and parallel example codes.\n\n\nSupport for CMake, on-the-fly compression of file streams, and HDF5-based output following the \nConduit\n mesh blueprint specification.\n\n\n\n\nMFEM is being developed in \nCASC\n, \nLLNL\n and is freely available under LGPL 2.1.\nFor more details, see the \ninteractive documentation\n and the full \nCHANGELOG\n.\n\n\n\n\nCEED co-design center announced\n\n\nThe \nExascale Computing Project (ECP)\n announced on November 11, 2016 its selection\nof four \nco-design centers\n, including CEED: the Center for\nEfficient Exascale Discretizations, which is a \nresearch partnership\n between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).\n\n\nAdditional news coverage can be found in \nLLNL Newsline\n\nand the \nANL press release\n.", 
            "title": "News"
        }, 
        {
            "location": "/news/#news", 
            "text": "", 
            "title": "News"
        }, 
        {
            "location": "/news/#first-ceed-annual-meeting-to-be-held-at-llnl", 
            "text": "CEED will hold its first annual meeting in August, 2017 at the  HPC Innovation\nCenter  of\nLawrence Livermore National Laboratory.  The goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.  In addition to gathering together many of the CEED researchers, the meeting will\ninclude representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.  Please  contact the CEED team  if you are interested\nin attending.", 
            "title": "First CEED annual meeting to be held at LLNL"
        }, 
        {
            "location": "/news/#gpu-hackathon-2017", 
            "text": "Nek/CEED team will participate in the  GPU Hackathon 2017 \nto be held in Brookhaven National Laboratory on June 5-9, 2017.  Our team will focus on performing and tuning GPU-enabled  Nek5000/Nekbone/NekCEM \nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.", 
            "title": "GPU Hackathon 2017"
        }, 
        {
            "location": "/news/#workshop-on-batched-reproducible-and-reduced-precision-blas", 
            "text": "The second  Workshop on Batched, Reproducible, and Reduced Precision BLAS \nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED  MAGMA  team.  The goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.  Of particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.  This is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found  here .", 
            "title": "Workshop on Batched, Reproducible, and Reduced Precision BLAS"
        }, 
        {
            "location": "/news/#software-release-mfem-v33", 
            "text": "Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at  http://mfem.org  The goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.  It has many features, including:   2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.  Parallel version scalable to hundreds of thousands of MPI cores.  Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.  Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.  Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.  Scalable algebraic multigrid, time integrators, and eigensolvers.  Lightweight interactive OpenGL visualization with the MFEM-based  GLVis  tool.   Some of the  new additions in version 3.3  are:   Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the  PETSc  and  SUNDIALS  suites.  Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.  General quadrature and nodal finite element basis types.  Scalable parallel mesh format.  Thirty six new integrators for common families of operators.  Sixteen new serial and parallel example codes.  Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the  Conduit  mesh blueprint specification.   MFEM is being developed in  CASC ,  LLNL  and is freely available under LGPL 2.1.\nFor more details, see the  interactive documentation  and the full  CHANGELOG .", 
            "title": "Software release: MFEM v3.3"
        }, 
        {
            "location": "/news/#ceed-co-design-center-announced", 
            "text": "The  Exascale Computing Project (ECP)  announced on November 11, 2016 its selection\nof four  co-design centers , including CEED: the Center for\nEfficient Exascale Discretizations, which is a  research partnership  between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).  Additional news coverage can be found in  LLNL Newsline \nand the  ANL press release .", 
            "title": "CEED co-design center announced"
        }, 
        {
            "location": "/codesign/", 
            "text": "Discretization Co-Design Approach\n\n\nCEED's co-design approach is based on close collaboration between its\n\nApplications\n, \nHardware\n, and \nSoftware\n thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's \nFinite Elements\n\nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.\n\n\nFor example, the development of \ndiscretization libraries\n in CEED\nis led by the \nFinite Elements\n thrust, but involves working closely\nwith vendors (\nHardware\n thrust) and software technology efforts\n(\nSoftware\n thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of and are successfully incorporated in\nECP applications is based on collaboration between the \nApplications\n and\n\nFinite Elements\n thrusts.\n\n\nIn addition to libraries of highly-performant kernels, a key product of the CEED\nproject will be a set of \nminiapps\n that will serve multiple roles:\n\n\n\n\nProvide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.\n\n\nServe as stand-alone unit-test drivers for the library kernels.\n\n\nProvide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).\n\n\nProvide test and demonstration cases for application scientists who are\nconsidering new formulations.\n\n\n\n\nThe \nminiapps\n developed in CEED, which we also refer to as\n\nCEEDlings\n, will range from local, element-level \nkernels\n, which can be run in\na simulator, to \nbake-off problems\n, that combine local and global\nkernels into model problem benchmarks for high-order computations, to \nproxy\napps\n which will include also application-relevant physics.\n\n\nThese encapsulated \nCEEDlings\n will be will be used in interactions with vendors\non \nemergent HPC technologies\n\nand \nECP software technologies\n\nprojects to highlight performance critical paths (e.g., size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of \nexascale architectures\n\nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Approach"
        }, 
        {
            "location": "/codesign/#discretization-co-design-approach", 
            "text": "CEED's co-design approach is based on close collaboration between its Applications ,  Hardware , and  Software  thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's  Finite Elements \nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.  For example, the development of  discretization libraries  in CEED\nis led by the  Finite Elements  thrust, but involves working closely\nwith vendors ( Hardware  thrust) and software technology efforts\n( Software  thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of and are successfully incorporated in\nECP applications is based on collaboration between the  Applications  and Finite Elements  thrusts.  In addition to libraries of highly-performant kernels, a key product of the CEED\nproject will be a set of  miniapps  that will serve multiple roles:   Provide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.  Serve as stand-alone unit-test drivers for the library kernels.  Provide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).  Provide test and demonstration cases for application scientists who are\nconsidering new formulations.   The  miniapps  developed in CEED, which we also refer to as CEEDlings , will range from local, element-level  kernels , which can be run in\na simulator, to  bake-off problems , that combine local and global\nkernels into model problem benchmarks for high-order computations, to  proxy\napps  which will include also application-relevant physics.  These encapsulated  CEEDlings  will be will be used in interactions with vendors\non  emergent HPC technologies \nand  ECP software technologies \nprojects to highlight performance critical paths (e.g., size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of  exascale architectures \nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Discretization Co-Design Approach"
        }, 
        {
            "location": "/ap/", 
            "text": "CEED Applications Thrust \n\n\nThis page is under construction\n\n\n\n\nMisun Min leads the AP thrust.\n\n\nThe goal of this thrust is to impact a wide range of \nECP application teams\n through\n  focused one-on-one interactions, facilitated by CEED application liaisons, as well as\n  through one-to-many interactions, based on the development of easy-to-use discretization\n  libraries for high-order finite element methods.\n\n\nSome of the thrust activities (current and planned) are:\n\n\nOpenACC- and CUDA-based GPU version implementation for Nek5000 and Nek miniapps.\n\n\nI/O performance improvement for meshes with more than 10 millions of elements.\n\n\nExplore potential improvement in performance with lightweight MPI and neighborhood collective MPI.\n\n\n\n\n\n\nList ECP apps and external project with which we are collaborating on these topics:\n\n\nCoupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors - SMRs (ORNL).\n\n\nNext-gen Multi-physics Simulation Code - MARBL (LLNL).\n\n\nMultiscale Coupled Urban System (ANL).\n\n\nTransforming Combustion Science and Technology with Exascale Simulations (SNL).\n\n\nCloud-Resolving Climate Modeling of the Earths Water Cycle (SNL).", 
            "title": "Applications"
        }, 
        {
            "location": "/ap/#ceed-applications-thrust", 
            "text": "This page is under construction   Misun Min leads the AP thrust.  The goal of this thrust is to impact a wide range of  ECP application teams  through\n  focused one-on-one interactions, facilitated by CEED application liaisons, as well as\n  through one-to-many interactions, based on the development of easy-to-use discretization\n  libraries for high-order finite element methods.  Some of the thrust activities (current and planned) are:  OpenACC- and CUDA-based GPU version implementation for Nek5000 and Nek miniapps.  I/O performance improvement for meshes with more than 10 millions of elements.  Explore potential improvement in performance with lightweight MPI and neighborhood collective MPI.    List ECP apps and external project with which we are collaborating on these topics:  Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors - SMRs (ORNL).  Next-gen Multi-physics Simulation Code - MARBL (LLNL).  Multiscale Coupled Urban System (ANL).  Transforming Combustion Science and Technology with Exascale Simulations (SNL).  Cloud-Resolving Climate Modeling of the Earths Water Cycle (SNL).", 
            "title": "CEED Applications Thrust "
        }, 
        {
            "location": "/hw/", 
            "text": "CEED Hardware Thrust\n\n\nThis page is under construction\n\n\n\n\nJack Dongarra leads the HW thrust\n\n\nThe goal of this thrust is to build a two-way (\npull-and-push\n) collaboration with vendors, where the CEED team will develop hardware-aware technologies (\npull\n) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek (\npush\n) impact and improve hardware designs within the ECP scope.\n\n\nSome of the thrust activities (current and planned) are:\n\n\nMini-Applications Design and Development;\n\n\nOptimal Data Locality and Motion;\n\n\nEnhanced Scalability and Parallelism.\n\n\n\n\n\n\nList vendors and other ECP AD, ST and external project with which we are collaborating on these topics:\n\n\nVendors: Intel, Nvidia, IBM, ARM, and AMD;\n\n\nECP collaborators: OpenMPI, MPICH, KokkosKernels, SNLApp.", 
            "title": "Hardware"
        }, 
        {
            "location": "/hw/#ceed-hardware-thrust", 
            "text": "This page is under construction   Jack Dongarra leads the HW thrust  The goal of this thrust is to build a two-way ( pull-and-push ) collaboration with vendors, where the CEED team will develop hardware-aware technologies ( pull ) to understand performance bottlenecks and take advantage of inevitable hardware trends, and vendor interactions to seek ( push ) impact and improve hardware designs within the ECP scope.  Some of the thrust activities (current and planned) are:  Mini-Applications Design and Development;  Optimal Data Locality and Motion;  Enhanced Scalability and Parallelism.    List vendors and other ECP AD, ST and external project with which we are collaborating on these topics:  Vendors: Intel, Nvidia, IBM, ARM, and AMD;  ECP collaborators: OpenMPI, MPICH, KokkosKernels, SNLApp.", 
            "title": "CEED Hardware Thrust"
        }, 
        {
            "location": "/sw/", 
            "text": "CEED Software Thrust\n\n\nThis page is under construction\n\n\n\n\nJed Brown leads the SW thrust\n\n\nThe goal of this thrust is to facilitate collaboration between CEED software packages, streamline developer and user workflows,\n  maintain testing and benchmarking infrastructure, and coordinate CEED releases.\n\n\nSome of the thrust activities (current and planned) are\n\n\nDevelop continuous integration and performance regression testing\n\n\nIdentify common kernels and their regimes of relevance\n\n\nDevelop benchmarking suite for bake-off problems\n\n\nSpack support\n\n\nCoordinate libCEED API design\n\n\n\n\n\n\nThe SW thrust collaborates with the following projects\n\n\nECP ST: PETSc, MPICH, Open MPI", 
            "title": "Software"
        }, 
        {
            "location": "/sw/#ceed-software-thrust", 
            "text": "This page is under construction   Jed Brown leads the SW thrust  The goal of this thrust is to facilitate collaboration between CEED software packages, streamline developer and user workflows,\n  maintain testing and benchmarking infrastructure, and coordinate CEED releases.  Some of the thrust activities (current and planned) are  Develop continuous integration and performance regression testing  Identify common kernels and their regimes of relevance  Develop benchmarking suite for bake-off problems  Spack support  Coordinate libCEED API design    The SW thrust collaborates with the following projects  ECP ST: PETSc, MPICH, Open MPI", 
            "title": "CEED Software Thrust"
        }, 
        {
            "location": "/bps/", 
            "text": "CEED Bake-off Problems (Benchmarks)\n\n\nThis page is under construction\n\n\n\n\nBake off problems specifications\n\n\nDatabase of results from CEED codes\n\n\nReference implementation in libCEED\n\n\nTarget audience is: ECP vendors, ST projects, other high-order codes\n\n\n\n\nSome terminology and notation\n\n\nVector representation/storage categories:\n\n\n\n\nTrue degrees of freedom/unknowns, \nT-vector\n:\n\n\neach unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$\n\n\nthis is a non-overlapping vector decomposition\n\n\nusually includes any essential (fixed) dofs.\n\n\n\n\n\n\n\nLocal true degrees of freedom/unknowns, \nL-vector\n:\n\n\neach unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$\n\n\nthis is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor\n\n\nthe shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n  \n\n\n\n\n\n\nPer element decomposition, \nE-vector\n:\n\n\neach unknown $i$ has as many copies as the number of elements that contain\n  $i$\n\n\nusually, the copies of the unknowns are grouped by the element they belong\n  to.\n  \n\n\n\n\n\n\nIn the case of hanging nodes (giving rise to hanging dofs):\n\n\nthere is another representation similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs, \nH-vector\n\n\nthe additional hanging dofs are duplicated when they are shared by\n  multiple processors.\n\n\n\n\n\n\nIn the case of variable order spaces:\n\n\nthe dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case\n\n\nhave both L- and H-vector representations.\n\n\n\n\n\n\nQuadrature point vector, \nQ-vector\n:\n\n\nthis is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.\n\n\n\n\n\n\nIn many cases it is useful to distinguish two types of vectors:\n\n\nX-vector, or \nprimal\n X-vector, and X'-vector, or \ndual\n X-vector\n\n\nhere X can be any of the T, L, H, E, or Q categories\n\n\nfor example, the mass matrix operator maps a T-vector to a T'-vector\n\n\nthe solutions vector is a T-vector, and the RHS vector is a T'-vector\n\n\nusing the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.\n\n\n\n\n\n\n\n\nOperator representation/storage/action categories:\n\n\n\n\nFull true-dof parallel assembly, \nTA\n, or \nA\n:\n\n\nParCSR or similar format\n\n\nthe T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.\n\n\n\n\n\n\nFull local assembly, \nLA\n:\n\n\nCSR matrix on each rank\n\n\nthe parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action\n\n\nnote that $P$ is the operator mapping T-vectors to L-vectors.\n\n\n\n\n\n\nElement matrix assembly, \nEA\n:\n\n\neach element matrix is stored as a dense matrix\n\n\noptimized element and parallel prolongation operators\n\n\nnote that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.\n\n\n\n\n\n\nQuadrature-point/partial assembly, \nQA\n or \nPA\n:\n\n\nprecompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements\n\n\nthe stored data can be viewed as a Q-vector.\n\n\n\n\n\n\nUnassembled option,  \nUA\n or \nU\n:\n\n\nno assembly step\n\n\nthe action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).\n\n\n\n\n\n\n\n\nBake-off problem 1\n\n\nSetup:\n\n\n\n\nMass matrix\n\n\nCoefficient: constant $1$\n\n\n3D hex mesh\n\n\nNo essential boundary conditions, or essential boundary conditions on the\n  whole boundary\n\n\nSolution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?\n\n\nQuadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the quadrature order is $2q-1$\n\n\nAlso, consider $q=2$, for $p=1$, and $q=3$, for $p=2$\n\n\nUse nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension\n\n\nConsider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$\n\n\nFocus on the QA/PA operator representation\n\n\nMeshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$\n\n\nFor example:\n\n\nif $s=15$, then $s_1=s_2=s_3=5$\n\n\nif $s=16$, then $s_1=6$ and $s_2=s_3=5$\n\n\nif $s=17$, then $s_1=s_2=6$ and $s_3=5$\n\n\n\n\n\n\nConsider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh\n  into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$\n  are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$\n\n\nWhat are good partitioning algorithms for the\n  strong scaling limit? The problem is to generate well balanced partitions when\n  the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4\n  does not do well on this type of problems. What about METIS 5 and other\n  graph partitioners? Maybe we need to develop specialized algorithms?\n\n\nNumber of MPI tasks, and number of MPI tasks per node:\n\n\nOn ALCF's BG/Q, \nCetus\n: $2^{14}$ tasks total, with $2^5$ tasks per node\n\n\n\n\n\n\n...\n\n\n\n\nReport:\n\n\n\n\nNumber of mesh elements, $E$\n\n\nPolynomial degree, $N$, or $p$\n\n\nTotal number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$\n\n\nTime per iteration = total CG time $/$ number of CG iterations\n\n\nNumber of iterations to reach relative residual reduction of $10^{-6}$\n\n\nTime for quadrature-point/partial assembly\n\n\nTime is measured as maximum over all MPI ranks; using \nMPI_Wtime()\n or other\n  similar function\n\n\n...\n\n\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/bps/#ceed-bake-off-problems-benchmarks", 
            "text": "This page is under construction   Bake off problems specifications  Database of results from CEED codes  Reference implementation in libCEED  Target audience is: ECP vendors, ST projects, other high-order codes", 
            "title": "CEED Bake-off Problems (Benchmarks)"
        }, 
        {
            "location": "/bps/#some-terminology-and-notation", 
            "text": "Vector representation/storage categories:   True degrees of freedom/unknowns,  T-vector :  each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$  this is a non-overlapping vector decomposition  usually includes any essential (fixed) dofs.    Local true degrees of freedom/unknowns,  L-vector :  each unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$  this is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor  the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n      Per element decomposition,  E-vector :  each unknown $i$ has as many copies as the number of elements that contain\n  $i$  usually, the copies of the unknowns are grouped by the element they belong\n  to.\n      In the case of hanging nodes (giving rise to hanging dofs):  there is another representation similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs,  H-vector  the additional hanging dofs are duplicated when they are shared by\n  multiple processors.    In the case of variable order spaces:  the dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case  have both L- and H-vector representations.    Quadrature point vector,  Q-vector :  this is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.    In many cases it is useful to distinguish two types of vectors:  X-vector, or  primal  X-vector, and X'-vector, or  dual  X-vector  here X can be any of the T, L, H, E, or Q categories  for example, the mass matrix operator maps a T-vector to a T'-vector  the solutions vector is a T-vector, and the RHS vector is a T'-vector  using the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.     Operator representation/storage/action categories:   Full true-dof parallel assembly,  TA , or  A :  ParCSR or similar format  the T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.    Full local assembly,  LA :  CSR matrix on each rank  the parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action  note that $P$ is the operator mapping T-vectors to L-vectors.    Element matrix assembly,  EA :  each element matrix is stored as a dense matrix  optimized element and parallel prolongation operators  note that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.    Quadrature-point/partial assembly,  QA  or  PA :  precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements  the stored data can be viewed as a Q-vector.    Unassembled option,   UA  or  U :  no assembly step  the action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).", 
            "title": "Some terminology and notation"
        }, 
        {
            "location": "/bps/#bake-off-problem-1", 
            "text": "Setup:   Mass matrix  Coefficient: constant $1$  3D hex mesh  No essential boundary conditions, or essential boundary conditions on the\n  whole boundary  Solution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?  Quadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the quadrature order is $2q-1$  Also, consider $q=2$, for $p=1$, and $q=3$, for $p=2$  Use nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension  Consider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$  Focus on the QA/PA operator representation  Meshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$  For example:  if $s=15$, then $s_1=s_2=s_3=5$  if $s=16$, then $s_1=6$ and $s_2=s_3=5$  if $s=17$, then $s_1=s_2=6$ and $s_3=5$    Consider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh\n  into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$\n  are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$  What are good partitioning algorithms for the\n  strong scaling limit? The problem is to generate well balanced partitions when\n  the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4\n  does not do well on this type of problems. What about METIS 5 and other\n  graph partitioners? Maybe we need to develop specialized algorithms?  Number of MPI tasks, and number of MPI tasks per node:  On ALCF's BG/Q,  Cetus : $2^{14}$ tasks total, with $2^5$ tasks per node    ...   Report:   Number of mesh elements, $E$  Polynomial degree, $N$, or $p$  Total number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$  Time per iteration = total CG time $/$ number of CG iterations  Number of iterations to reach relative residual reduction of $10^{-6}$  Time for quadrature-point/partial assembly  Time is measured as maximum over all MPI ranks; using  MPI_Wtime()  or other\n  similar function  ...   MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Bake-off problem 1"
        }, 
        {
            "location": "/miniapps/", 
            "text": "CEED Miniapps\n\n\nThis page is under construction\n\n\n\n\nList of miniapps, with documentation and downloads\n\n\nNekbone:\n\n\nNek5000 miniapp, solving Poisson equation with conjugate gradient\n\n\nDocumentation can be found in http://github.com/Nek5000/NekDoc\n\n\nDownload from GitHub http://github.com/Nek5000/Nekbone\n\n\n\n\n\n\nNekCEM-ceedling:\n\n\nNekCEM miniapp, solving time-domain Maxwell equations\n\n\nDocumentation and Download to be available from github\n\n\n\n\n\n\nMFEM examples:\n\n\nminiapps/performance Ex1\n\n\nLagrangian hydrodynamics miniapp (under development)\n\n\nOther MFEM example codes and miniapps\n\n\n\n\n\n\nDatabase of results/expectations\n\n\nTarget audience is: ECP vendors, ST projects", 
            "title": "Miniapps"
        }, 
        {
            "location": "/miniapps/#ceed-miniapps", 
            "text": "This page is under construction   List of miniapps, with documentation and downloads  Nekbone:  Nek5000 miniapp, solving Poisson equation with conjugate gradient  Documentation can be found in http://github.com/Nek5000/NekDoc  Download from GitHub http://github.com/Nek5000/Nekbone    NekCEM-ceedling:  NekCEM miniapp, solving time-domain Maxwell equations  Documentation and Download to be available from github    MFEM examples:  miniapps/performance Ex1  Lagrangian hydrodynamics miniapp (under development)  Other MFEM example codes and miniapps    Database of results/expectations  Target audience is: ECP vendors, ST projects", 
            "title": "CEED Miniapps"
        }, 
        {
            "location": "/partners/", 
            "text": "CEED Partners\n\n\nThis page is under construction\n\n\nCEED is interested in reaching out to additional apps, including low-order\napplications.\n\n\nECP Partners\n\n\n\n\nSMRs\n\n\nMARBL\n\n\nACME\n\n\nUrban\n\n\nGEOS\n\n\nMPICH\n\n\nOpenMPI\n\n\nhypre\n\n\nSTRUMPACK\n\n\nALPINE/VTK-m\n\n\n...\n\n\n\n\nExternal Partners\n\n\n\n\nKAUST\n\n\nIntelligent Light\n\n\n...\n\n\n\n\nOther High-Order Projects\n\n\n\n\nNektar++\n\n\ndeal.ii", 
            "title": "Partners"
        }, 
        {
            "location": "/partners/#ceed-partners", 
            "text": "This page is under construction  CEED is interested in reaching out to additional apps, including low-order\napplications.", 
            "title": "CEED Partners"
        }, 
        {
            "location": "/partners/#ecp-partners", 
            "text": "SMRs  MARBL  ACME  Urban  GEOS  MPICH  OpenMPI  hypre  STRUMPACK  ALPINE/VTK-m  ...", 
            "title": "ECP Partners"
        }, 
        {
            "location": "/partners/#external-partners", 
            "text": "KAUST  Intelligent Light  ...", 
            "title": "External Partners"
        }, 
        {
            "location": "/partners/#other-high-order-projects", 
            "text": "Nektar++  deal.ii", 
            "title": "Other High-Order Projects"
        }, 
        {
            "location": "/software/", 
            "text": "Software Catalog\n\n\nThe CEED team includes members of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:\n\n\n\n\n\n\nNext-generation \nfinite element\n \ndiscretization\n\n  \nlibraries\n that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.\n\n\n\n\n\n\nMiniapps\n combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and \nbenchmark problems\n.\n\n\n\n\n\n\nBroadly applicable technologies, including extensions of dense linear algebra\n  libraries to support \nfast tensor contractions\n, scalable\n  matrix-free \nlinear solvers\n and programming models for \nperformance\n  portability\n.\n\n\n\n\n\n\nAll CEED software is open-source and publicly available at\n  \nhttps://github.com/ceed\n.", 
            "title": "Catalog"
        }, 
        {
            "location": "/software/#software-catalog", 
            "text": "The CEED team includes members of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:    Next-generation  finite element   discretization \n   libraries  that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.    Miniapps  combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and  benchmark problems .    Broadly applicable technologies, including extensions of dense linear algebra\n  libraries to support  fast tensor contractions , scalable\n  matrix-free  linear solvers  and programming models for  performance\n  portability .    All CEED software is open-source and publicly available at\n   https://github.com/ceed .", 
            "title": "Software Catalog"
        }, 
        {
            "location": "/ceed-code/", 
            "text": "CEED APIs\n\n\nCEED will build on the efforts of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.\n\n\nHigh level API\n\n\nThe CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.\n\n\nGiven such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.\n\n\nThe global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nLow level API\n\n\nThe CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:\n\n\n\nWe take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as \nsum\nfactorization\n.\nIn the case where we precompute and store the $D$ matrix, we call the algorithm\n\npartial assembly\n.\n\n\nThe low-level API can be used as the foundation for an efficient high-order\n\noperator format\n.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/ceed-code/#ceed-apis", 
            "text": "CEED will build on the efforts of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/ceed-code/#high-level-api", 
            "text": "The CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.  Given such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.  The global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.  This API is currently under development. Stay tuned for more details...", 
            "title": "High level API"
        }, 
        {
            "location": "/ceed-code/#low-level-api", 
            "text": "The CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:  We take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as  sum\nfactorization .\nIn the case where we precompute and store the $D$ matrix, we call the algorithm partial assembly .  The low-level API can be used as the foundation for an efficient high-order operator format .  This API is currently under development. Stay tuned for more details...  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Low level API"
        }, 
        {
            "location": "/nek/", 
            "text": "Nek\n \n\n\nNek5000/NekCEM is open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.\n\n\nIt features state-of-the-art, scalable, high-order spectral-element based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.\n\n\nNek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories, and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.\n\n\n\n\nReactor analysis\n\n\nAerospace application\n\n\nEngine application\n\n\nOcean modeling\n\n\nTurbulence modeling\n\n\nElectromagnetics modeling\n\n\nDrift-diffusion  modeling\n\n\n\n\nNek5000 and NekCEM are freely available under a BSD license.\n\n\nIn CEED, Nek is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the Nek5000 website: \nhttps://nek5000.mcs.anl.gov\n.", 
            "title": "Nek"
        }, 
        {
            "location": "/nek/#nek", 
            "text": "Nek5000/NekCEM is open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.  It features state-of-the-art, scalable, high-order spectral-element based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.  Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories, and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.   Reactor analysis  Aerospace application  Engine application  Ocean modeling  Turbulence modeling  Electromagnetics modeling  Drift-diffusion  modeling   Nek5000 and NekCEM are freely available under a BSD license.  In CEED, Nek is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the Nek5000 website:  https://nek5000.mcs.anl.gov .", 
            "title": "Nek "
        }, 
        {
            "location": "/mfem/", 
            "text": "MFEM\n \n\n\nMFEM is a free, lightweight, scalable C++ library for finite element methods.\n\n\nIts goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.\n\n\nIts features include:\n\n\n\n\nArbitrary high-order finite element \nmeshes\n\nand \nspaces\n.\n\n\nWide variety\n of finite element discretization approaches.\n\n\nConforming and nonconforming \nadaptive mesh refinement\n.\n\n\nScalable to \nhundreds of thousands\n of cores.\n\n\n... and \nmany more\n.\n\n\n\n\nMFEM is being developed in \nCASC\n,\n\nLLNL\n and is freely available under LGPL 2.1.\n\n\nIn CEED, MFEM is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MFEM website: \nhttp://mfem.org\n.", 
            "title": "MFEM"
        }, 
        {
            "location": "/mfem/#mfem", 
            "text": "MFEM is a free, lightweight, scalable C++ library for finite element methods.  Its goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.  Its features include:   Arbitrary high-order finite element  meshes \nand  spaces .  Wide variety  of finite element discretization approaches.  Conforming and nonconforming  adaptive mesh refinement .  Scalable to  hundreds of thousands  of cores.  ... and  many more .   MFEM is being developed in  CASC , LLNL  and is freely available under LGPL 2.1.  In CEED, MFEM is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the MFEM website:  http://mfem.org .", 
            "title": "MFEM "
        }, 
        {
            "location": "/magma/", 
            "text": "MAGMA\n\n\n\n\n\nMAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.\n\n\n\n\nMAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.\n\n\nHybrid Algorithms\n\n\nMAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.\n\n\n\n\nIn either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.\n\n\nPerformance and Energy Efficiency\n\n\nMAGMA solvers run close to the machine's peak performance. The LU factorization for \nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.\n\n\n\n\nMAGMA Batched\n\n\nMAGMA Batched targets small linear algebra operations. Small computational tasks are difficult \nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.\n\n\n\n\nMAGMA Sparse\n\n\nMAGMA Sparse targets the development of high-performance sparse linear algebra operations on \naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace \niterative solvers, eigensolvers, and preconditioners.\n\n\n\n\nMAGMA Development\n\n\nMAGMA is being developed at the \nInnovative Computing Laboratory\n\nof the \nUniversity of Tennessee\n.\n\n\nIn CEED, MAGMA is primarily involved in the efforts of the \nSoftware\n, \nHardware\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MAGMA website: \nhttp://icl.cs.utk.edu/magma\n.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#magma", 
            "text": "MAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.   MAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#hybrid-algorithms", 
            "text": "MAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.   In either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.", 
            "title": "Hybrid Algorithms"
        }, 
        {
            "location": "/magma/#performance-and-energy-efficiency", 
            "text": "MAGMA solvers run close to the machine's peak performance. The LU factorization for \nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.", 
            "title": "Performance and Energy Efficiency"
        }, 
        {
            "location": "/magma/#magma-batched", 
            "text": "MAGMA Batched targets small linear algebra operations. Small computational tasks are difficult \nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.", 
            "title": "MAGMA Batched"
        }, 
        {
            "location": "/magma/#magma-sparse", 
            "text": "MAGMA Sparse targets the development of high-performance sparse linear algebra operations on \naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace \niterative solvers, eigensolvers, and preconditioners.", 
            "title": "MAGMA Sparse"
        }, 
        {
            "location": "/magma/#magma-development", 
            "text": "MAGMA is being developed at the  Innovative Computing Laboratory \nof the  University of Tennessee .  In CEED, MAGMA is primarily involved in the efforts of the  Software ,  Hardware  and  Finite Element  thrusts.  For more information, see the MAGMA website:  http://icl.cs.utk.edu/magma .", 
            "title": "MAGMA Development"
        }, 
        {
            "location": "/occa/", 
            "text": "OCCA\n \n\n\nOCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and let the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.\n\n\nOCCA abstracts the device programming languages into one kernel language, the\nOCCA kernel language (OKL). OKL minimally extends C and restricts the user to\nwrite parallel code that is JIT compiled.\n\n\nOCCA is freely available under an MIT license.\n\n\nIn CEED, Holmes is primarily involved in the efforts of the \nSoftware\n and\n\nFinite Element\n thrusts.\n\n\nFor more information, see the OCCA website: \nhttp://libocca.org\n.", 
            "title": "OCCA"
        }, 
        {
            "location": "/occa/#occa", 
            "text": "OCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and let the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.  OCCA abstracts the device programming languages into one kernel language, the\nOCCA kernel language (OKL). OKL minimally extends C and restricts the user to\nwrite parallel code that is JIT compiled.  OCCA is freely available under an MIT license.  In CEED, Holmes is primarily involved in the efforts of the  Software  and Finite Element  thrusts.  For more information, see the OCCA website:  http://libocca.org .", 
            "title": "OCCA "
        }, 
        {
            "location": "/holmes/", 
            "text": "Holmes\n\n\nHolmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.\n\n\nIts features include:\n\n\n\n\nElements\n\n\nSpectral element quadrilateral and hexahedral elements\n\n\nWarp \n blend high-order nodal triangular and tetrahedral elements\n\n\n\n\n\n\nDiscretizations\n\n\nElliptic: continuous and discontinuous Galerkin discretization\n\n\nHyperbolic: discontinuous Galerkin time-domain\n\n\n\n\n\n\nSolvers\n\n\nTwo level overlapping additive Schwartz elliptic problem preconditioning\n\n\nFast approximate block preconditioning\n\n\nAlgebraic multigrid coarse solver with heterogeneous acceleration\n\n\nParallel direct solver XXT for the coarsest level solve.\n\n\n\n\n\n\n\n\n\n\nImplementation\n\n\nPortable on-node parallelism via the Open Concurrent Compute Abstraction, \nOCCA\n\n\nMPI communications for distributed computations\n\n\n\n\n\n\nDependencies\n\n\nThe XXT parallel direct solver for the coarsest AMG solve\n\n\nThe \ngslib\n for optimized gather-scatter operations\n\n\n\n\n\n\nExample CEEDling apps\n\n\nElliptic solvers\n\n\nBoltzmann flow solver\n\n\nAcoustics time-domain solver\n\n\n\n\n\n\n\n\nIn CEED, Holmes is primarily involved in the efforts of the \nFinite Element\n\nand \nSoftware\n thrusts.\n\n\nHolmes is being developed in \nTim Warburton's group\n\nat \nVirginia Tech\n.\n\n\nThe project GitHub repository will be made public in 2017. Stay tuned...", 
            "title": "Holmes"
        }, 
        {
            "location": "/holmes/#holmes", 
            "text": "Holmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.  Its features include:   Elements  Spectral element quadrilateral and hexahedral elements  Warp   blend high-order nodal triangular and tetrahedral elements    Discretizations  Elliptic: continuous and discontinuous Galerkin discretization  Hyperbolic: discontinuous Galerkin time-domain    Solvers  Two level overlapping additive Schwartz elliptic problem preconditioning  Fast approximate block preconditioning  Algebraic multigrid coarse solver with heterogeneous acceleration  Parallel direct solver XXT for the coarsest level solve.      Implementation  Portable on-node parallelism via the Open Concurrent Compute Abstraction,  OCCA  MPI communications for distributed computations    Dependencies  The XXT parallel direct solver for the coarsest AMG solve  The  gslib  for optimized gather-scatter operations    Example CEEDling apps  Elliptic solvers  Boltzmann flow solver  Acoustics time-domain solver     In CEED, Holmes is primarily involved in the efforts of the  Finite Element \nand  Software  thrusts.  Holmes is being developed in  Tim Warburton's group \nat  Virginia Tech .  The project GitHub repository will be made public in 2017. Stay tuned...", 
            "title": "Holmes"
        }, 
        {
            "location": "/petsc/", 
            "text": "PETSc\n\n\nPETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.\n\n\nAs part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.\n\n\nPETSc is freely available under a BSD license.\n\n\nIn CEED, PETSc is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the PETSc website: \nhttps://www.mcs.anl.gov/petsc/\n.", 
            "title": "PETSc"
        }, 
        {
            "location": "/petsc/#petsc", 
            "text": "PETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.  As part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.  PETSc is freely available under a BSD license.  In CEED, PETSc is primarily involved in the efforts of the  Software  thrust.  For more information, see the PETSc website:  https://www.mcs.anl.gov/petsc/ .", 
            "title": "PETSc"
        }, 
        {
            "location": "/pumi/", 
            "text": "PUMI\n \n\n\nPUMI\n is an unstructured, distributed mesh data management system designed for\n massively parallel computing.\n\n\nPUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:\n\n\n\n\nPCU\n for phased message passing and thread management.\n\n\nGMI\n for geometric model interface.\n\n\nMDS\n for unstructured mesh representation.\n\n\nAPF Mesh\n for partition model and distributed mesh management.\n\n\nAPF_Field\n for field management.\n\n\n\n\nPUMI is being developed at RPI's \nScientific Computation Research\nCenter\n and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.\n\n\nIn CEED, PUMI is primarily involved in the efforts of the \nFinite Element\n\nand \nSoftware\n thrusts.\n\n\nFor more information, see the \nPUMI documents\n.", 
            "title": "PUMI"
        }, 
        {
            "location": "/pumi/#pumi", 
            "text": "PUMI  is an unstructured, distributed mesh data management system designed for\n massively parallel computing.  PUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:   PCU  for phased message passing and thread management.  GMI  for geometric model interface.  MDS  for unstructured mesh representation.  APF Mesh  for partition model and distributed mesh management.  APF_Field  for field management.   PUMI is being developed at RPI's  Scientific Computation Research\nCenter  and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.  In CEED, PUMI is primarily involved in the efforts of the  Finite Element \nand  Software  thrusts.  For more information, see the  PUMI documents .", 
            "title": "PUMI "
        }, 
        {
            "location": "/gslib/", 
            "text": "gslib\n\n\nGslib is a library for Gather/Scatter-type nearest neighbor data exchanges for parallel spectral element computations.\n\n\nAdditional features include:\n\n\n\n\nXXT solver (parallel direct solver)\n\n\nAMG solver\n\n\nRobust spectral element interpolation for a given set of points\n\n\n\n\ngslib's applications include \nNek5000\n and \nNektar++\n.\n\n\nIn CEED, gslib is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the gslib GitHub repository: \nhttps://github.com/gslib/gslib\n.", 
            "title": "gslib"
        }, 
        {
            "location": "/gslib/#gslib", 
            "text": "Gslib is a library for Gather/Scatter-type nearest neighbor data exchanges for parallel spectral element computations.  Additional features include:   XXT solver (parallel direct solver)  AMG solver  Robust spectral element interpolation for a given set of points   gslib's applications include  Nek5000  and  Nektar++ .  In CEED, gslib is primarily involved in the efforts of the  Software  thrust.  For more information, see the gslib GitHub repository:  https://github.com/gslib/gslib .", 
            "title": "gslib"
        }, 
        {
            "location": "/thrusts/", 
            "text": "R\nD Thrusts\n\n\nCEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R\nD thrusts\nfocused on these customers and tied together by the foundational \nfinite element\nthrust\n.\n\n\n\n\nThe specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the \nOutreach page\n.\n\n\n\n\nApplications Thrust (AP)\n\n\nThe goal of the AP thrust is to maintain a close connection with ECP apps. Reach\nout to low-order and non-ECP apps.  Derive requirements for CEED\u2019s miniapps and\nsoftware technologies work.\n\n\n\n\nHardware Thrust (HW)\n\n\nThe goal of the HW thrust is to work with the vendors to take advantage of\nexisting hardware to impact applications.  Provide them with relevant miniapps\nand hardware/software requirements for high-order apps.  Follow and evaluate\nhardware trends towards exascale.\n\n\n\n\nSoftware Thrust (SW)\n\n\nThe goal of the SW thrust is to participate in the development of software\nlibraries and frameworks of general interest to the scientific computing\ncommunity.  Enable integration into and/or interoperability with overall ECP\nsoftware technologies stack.\n\n\n\n\nFinite Elements Thrust (FE)\n\n\nThe goal of the FE thrust is to continue to improve the state-of-the-art finite\nelement algorithms and kernels in CEED software for exascale efficiency.  Lead\nthe development of discretization libraries and miniapps.", 
            "title": "R&D Thrusts"
        }, 
        {
            "location": "/thrusts/#rd-thrusts", 
            "text": "CEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R D thrusts\nfocused on these customers and tied together by the foundational  finite element\nthrust .   The specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the  Outreach page .", 
            "title": "R&amp;D Thrusts"
        }, 
        {
            "location": "/thrusts/#applications-thrust-ap", 
            "text": "The goal of the AP thrust is to maintain a close connection with ECP apps. Reach\nout to low-order and non-ECP apps.  Derive requirements for CEED\u2019s miniapps and\nsoftware technologies work.", 
            "title": "Applications Thrust (AP)"
        }, 
        {
            "location": "/thrusts/#hardware-thrust-hw", 
            "text": "The goal of the HW thrust is to work with the vendors to take advantage of\nexisting hardware to impact applications.  Provide them with relevant miniapps\nand hardware/software requirements for high-order apps.  Follow and evaluate\nhardware trends towards exascale.", 
            "title": "Hardware Thrust (HW)"
        }, 
        {
            "location": "/thrusts/#software-thrust-sw", 
            "text": "The goal of the SW thrust is to participate in the development of software\nlibraries and frameworks of general interest to the scientific computing\ncommunity.  Enable integration into and/or interoperability with overall ECP\nsoftware technologies stack.", 
            "title": "Software Thrust (SW)"
        }, 
        {
            "location": "/thrusts/#finite-elements-thrust-fe", 
            "text": "The goal of the FE thrust is to continue to improve the state-of-the-art finite\nelement algorithms and kernels in CEED software for exascale efficiency.  Lead\nthe development of discretization libraries and miniapps.", 
            "title": "Finite Elements Thrust (FE)"
        }, 
        {
            "location": "/fe/", 
            "text": "CEED Finite Element Thrust\n\n\nThis page is under construction\n\n\n\n\nVeselin Dobrev leads the FE thrust\n\n\nThe goal of this thrust is...\n\n\nSome of the thrust activities (current and planned) are...\n\n\nList ECP AD, ST and external project with which we are collaborating on these topics\n\n\n\"Why high-order?\" manifesto", 
            "title": "Finite Elements"
        }, 
        {
            "location": "/fe/#ceed-finite-element-thrust", 
            "text": "This page is under construction   Veselin Dobrev leads the FE thrust  The goal of this thrust is...  Some of the thrust activities (current and planned) are...  List ECP AD, ST and external project with which we are collaborating on these topics  \"Why high-order?\" manifesto", 
            "title": "CEED Finite Element Thrust"
        }, 
        {
            "location": "/formats/", 
            "text": "Data Formats\n\n\nCEED's data formats are currently under development. Stay tuned for more details...", 
            "title": "_Formats"
        }, 
        {
            "location": "/formats/#data-formats", 
            "text": "CEED's data formats are currently under development. Stay tuned for more details...", 
            "title": "Data Formats"
        }, 
        {
            "location": "/pubs/", 
            "text": "Publications and Outreach\n\n\n\n\nCEED Documents\n\n\n\n\nCEED's high-order \nBenchmarks\n and \nMiniapps\n.\n\n\nActivities in the \nApplications\n, \nHardware\n, \nSoftware\n and \nFinite Element\n thrusts.\n\n\nCEED-proposed high-order \nData Formats\n.\n\n\n\n\n\n\nPublications\n\n\n2017\n\n\n\n\nA. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, \nFactorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures\n, \nProceedings of the 2017 International Conference on Computational Science, ICCS'17\n, Z\u00fcrich, Switzerland, June 12-14, \nProcedia Computer Science\n, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, \nSmall Tensor Operations on Advanced Architectures for High-order Applications\n, Technical report UT-EECS-17-749, EECS Department, Univerity of Tennessee, \n2017\n.\n\n\n\n\n\n\n2016\n\n\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nHigh-performance Tensor Contractions for GPUs\n, \nProcedia Computer Science\n, Volume 80, Pages 108-118, ISSN 1877-0509, \n2016\n.\n\n\nM. B.E., Y. Peet, P. Fischer, and J. Lottes, \nA spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations\n, \nJ. Comp. Phys.\n, 307:60\u201393, \n2016\n.\n\n\nM. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, \nAn MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication\n, \nThe International Journal of High Performance Computing Application\n, 30(3):320\u2013334, \n2016\n.\n\n\nJ. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, \nNekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations\n, \nSpecial issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing\n, \n2016\n.\n\n\n\n\n\n\n2015 and earlier\n\n\n\n\nP. Fischer, K. Heisey, and M. Min, \nScaling limits for PDE-based simulation\n, \nIn 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation\n, AIAA 2015-3049, \n2015\n.\n\n\nA. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, \nErosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches\n, volume 2, pp. 1448\u20131461. \nAmerican Nuclear Society\n, \n2015\n.\n\n\nE. Merzari, P. Fischer, and J. Walker, \nLarge-scale simulation of rod bundles: Coherent structure recognition and stability analysis\n, volume 1. \nAmerican Society of Mechanical Engineers\n, \n2015\n.\n\n\nM. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, \nEntanglement of two, three and four plasmonically coupled quantum dots\n, \nPhysical Review B\n, 92:125432, \n2015\n.\n\n\nD. A. May, J. Brown, and L. Le Pourhiet. \npTatin3D: High-performance methods for long-term lithospheric dynamics\n, In Proceedings of \nSC14: International Conference for High Performance Computing, Networking, Storage and Analysis\n. ACM, \n2014\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, \nMonotonicity in high-order curvilinear finite element ALE remap\n, \nInt. J. Numer. Meth. Fluids\n, 77(5), pp. 249\u2013273, \n2014\n.\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG solver for H(div) problems\n,  \nSIAM J. Sci. Comp.\n, 34, pp. A3079\u2013A3098, \n2012\n.\n\n\nV. Dobrev, Tz. Kolev and R. Rieben, \nHigh-order curvilinear finite element methods for Lagrangian hydrodynamics\n, \nSIAM J. Sci. Comp.\n, 34, pp. B606\u2013B641, \n2012\n.\n\n\nJ. Brown, \nEfficient nonlinear solvers for nodal high-order finite elements in 3D\n, \nJournal of Scientific Computing\n, 45:48\u201363, \n2010\n. doi:10.1007/s10915-010-9396-8\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG for H(curl) problems\n, \nJ. Comput. Math.\n, 27, pp. 604-623, \n2009\n.\n\n\n\n\n\n\nPresentations\n\n\n2017\n\n\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nAccelerating Tensor Contractions in High-Order FEM with MAGMA Batched\n, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nP. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nM. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\n\n\n\n\nOther Resources\n\n\n\n\nLLNL's \nexascale computing website\n.\n\n\nNews coverage of CEED announcement in  \nLLNL Newsline\n\nand the \nANL press release\n.\n\n\nU.S. Department of Energy \nExascale Initiative\n.", 
            "title": "Outreach"
        }, 
        {
            "location": "/pubs/#publications-and-outreach", 
            "text": "", 
            "title": "Publications and Outreach"
        }, 
        {
            "location": "/pubs/#ceed-documents", 
            "text": "CEED's high-order  Benchmarks  and  Miniapps .  Activities in the  Applications ,  Hardware ,  Software  and  Finite Element  thrusts.  CEED-proposed high-order  Data Formats .", 
            "title": "CEED Documents"
        }, 
        {
            "location": "/pubs/#publications", 
            "text": "", 
            "title": "Publications"
        }, 
        {
            "location": "/pubs/#2017", 
            "text": "A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra,  Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures ,  Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14,  Procedia Computer Science ,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov,  Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, Univerity of Tennessee,  2017 .", 
            "title": "2017"
        }, 
        {
            "location": "/pubs/#2016", 
            "text": "A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  High-performance Tensor Contractions for GPUs ,  Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509,  2016 .  M. B.E., Y. Peet, P. Fischer, and J. Lottes,  A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations ,  J. Comp. Phys. , 307:60\u201393,  2016 .  M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min,  An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication ,  The International Journal of High Performance Computing Application , 30(3):320\u2013334,  2016 .  J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min,  Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations ,  Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing ,  2016 .", 
            "title": "2016"
        }, 
        {
            "location": "/pubs/#2015-and-earlier", 
            "text": "P. Fischer, K. Heisey, and M. Min,  Scaling limits for PDE-based simulation ,  In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049,  2015 .  A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer,  Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461.  American Nuclear Society ,  2015 .  E. Merzari, P. Fischer, and J. Walker,  Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1.  American Society of Mechanical Engineers ,  2015 .  M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray,  Entanglement of two, three and four plasmonically coupled quantum dots ,  Physical Review B , 92:125432,  2015 .  D. A. May, J. Brown, and L. Le Pourhiet.  pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of  SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM,  2014 .  R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben,  Monotonicity in high-order curvilinear finite element ALE remap ,  Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273,  2014 .  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG solver for H(div) problems ,   SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098,  2012 .  V. Dobrev, Tz. Kolev and R. Rieben,  High-order curvilinear finite element methods for Lagrangian hydrodynamics ,  SIAM J. Sci. Comp. , 34, pp. B606\u2013B641,  2012 .  J. Brown,  Efficient nonlinear solvers for nodal high-order finite elements in 3D ,  Journal of Scientific Computing , 45:48\u201363,  2010 . doi:10.1007/s10915-010-9396-8  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG for H(curl) problems ,  J. Comput. Math. , 27, pp. 604-623,  2009 .", 
            "title": "2015 and earlier"
        }, 
        {
            "location": "/pubs/#presentations", 
            "text": "", 
            "title": "Presentations"
        }, 
        {
            "location": "/pubs/#2017_1", 
            "text": "A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched ,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .", 
            "title": "2017"
        }, 
        {
            "location": "/pubs/#other-resources", 
            "text": "LLNL's  exascale computing website .  News coverage of CEED announcement in   LLNL Newsline \nand the  ANL press release .  U.S. Department of Energy  Exascale Initiative .", 
            "title": "Other Resources"
        }, 
        {
            "location": "/about/", 
            "text": "About CEED\n\n\nThe Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:\n\n\n\n\nArgonne National Laboratory\n\n\nUniversity of Colorado Boulder\n\n\nUniversity of Illinois Urbana-Champaign\n\n\nLawrence Livermore National Laboratory\n\n\nRensselaer Polytechnic Institute\n\n\nThe University of Tennessee, Knoxville\n\n\nVirginia Tech\n\n\n\n\nYou can reach us by emailing \nceed-users@llnl.gov\n or by leaving a comment in\nthe \nCEED user forum\n.\n\n\n\n\n\n\n\nThis research was supported by the \nExascale Computing Project\n (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a \ncapable exascale ecosystem\n, including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s exascale computing imperative.\n\n\n\n\nOur Team\n\n\n\n\nAhmad Abdelfattah\n\n\nAleks Obabko\n\n\nAli Karakus\n\n\nAndrew Siegel\n\n\nAzzam Haidar\n\n\nBarry Smith\n\n\nCameron Smith\n\n\nDavid Beckingsale\n\n\nDavid Medina\n\n\nIan Karlin\n\n\nJack Dongarra\n \n Lead for the \nHardware\n thrust\n\n\nJed Brown\n \n Lead for the \nSoftware\n thrust\n\n\nKatie Heisey\n\n\nKazem Kamran\n\n\nMark Shepard\n\n\nMatt Otten\n\n\nMisun Min\n \n Lead for the \nApplications\n thrust\n\n\nNoel Chalmers\n\n\nPanayot Vassilevski\n\n\nPaul Fischer\n \n Deputy Director of CEED\n\n\nPedro Bello-Maldonado\n\n\nRobert Rieben\n\n\nRon Rahaman\n\n\nScott Parker\n\n\nSom Dutta\n\n\nStanimire Tomov\n\n\nStefan Kerkemeier\n\n\nThilina Ratnayake\n\n\nTim Moon\n\n\nTim Warburton\n\n\nTzanio Kolev\n \n Director of CEED\n\n\nVeselin Dobrev\n \n Lead for the \nFinite Element\n thrust\n\n\nVladimir Tomov\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nBootstrap\n\nand \nBootswatch\n. Hosted on \nGitHub\n.\n\n\nLLNL-WEB-732668.\n\nPrivacy \n Legal Notice\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-ceed", 
            "text": "The Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:   Argonne National Laboratory  University of Colorado Boulder  University of Illinois Urbana-Champaign  Lawrence Livermore National Laboratory  Rensselaer Polytechnic Institute  The University of Tennessee, Knoxville  Virginia Tech   You can reach us by emailing  ceed-users@llnl.gov  or by leaving a comment in\nthe  CEED user forum .    This research was supported by the  Exascale Computing Project  (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a  capable exascale ecosystem , including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s exascale computing imperative.", 
            "title": "About CEED"
        }, 
        {
            "location": "/about/#our-team", 
            "text": "Ahmad Abdelfattah  Aleks Obabko  Ali Karakus  Andrew Siegel  Azzam Haidar  Barry Smith  Cameron Smith  David Beckingsale  David Medina  Ian Karlin  Jack Dongarra    Lead for the  Hardware  thrust  Jed Brown    Lead for the  Software  thrust  Katie Heisey  Kazem Kamran  Mark Shepard  Matt Otten  Misun Min    Lead for the  Applications  thrust  Noel Chalmers  Panayot Vassilevski  Paul Fischer    Deputy Director of CEED  Pedro Bello-Maldonado  Robert Rieben  Ron Rahaman  Scott Parker  Som Dutta  Stanimire Tomov  Stefan Kerkemeier  Thilina Ratnayake  Tim Moon  Tim Warburton  Tzanio Kolev    Director of CEED  Veselin Dobrev    Lead for the  Finite Element  thrust  Vladimir Tomov    Website built with  MkDocs ,  Bootstrap \nand  Bootswatch . Hosted on  GitHub .  LLNL-WEB-732668. Privacy   Legal Notice .", 
            "title": "Our Team"
        }
    ]
}