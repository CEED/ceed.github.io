{
    "docs": [
        {
            "location": "/", 
            "text": "Exascale Co-Design\n\n\nThe \nCenter for Efficient Exascale Discretizations (CEED)\n is a co-design center within the\nU.S. Department of Energy (DOE) \nExascale Computing Project (ECP)\n\nwith the following goals:\n\n\n\n\n\n\nHelp applications leverage future architectures by providing them with\n  \nstate-of-the-art discretization algorithms\n that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.\n\n\n\n\n\n\nCollaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed \nproxies and miniapps\n.\n\n\n\n\n\n\nProvide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming \nexascale software ecosystem\n.\n\n\n\n\n\n\nCEED is a research partnership involving \n30+ computational scientists\n from\ntwo DOE labs and five universities, including members of the \nNek5000\n,\n\nMFEM\n, \nMAGMA\n, \nOCCA\n and \nPETSc\n projects.\nYou can reach us by emailing \nceed-users@llnl.gov\n\nor by leaving a comment in the \nCEED user forum\n.\n\n\nThe center's co-design efforts are organized in \nfour interconnected R\nD\nthrusts\n, focused on the following computational motifs and their\nperformance on \nexascale hardware\n.\nSee also our \npublications\n.\n\n\n\n\nPDE-based simulations on unstructured grids\n\n\nCEED is producing a range of \nsoftware products\n supporting general\n\nfinite element\n algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H\n1\n, H(curl), H(div) and L\n2\n/DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).\n\n\n\n\nHigh-order/spectral finite elements\n\n\nOur algorithms and software come with comprehensive high-order support: we provide\n\nefficient matrix-free operator evaluation\n for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "Home"
        }, 
        {
            "location": "/#exascale-co-design", 
            "text": "The  Center for Efficient Exascale Discretizations (CEED)  is a co-design center within the\nU.S. Department of Energy (DOE)  Exascale Computing Project (ECP) \nwith the following goals:    Help applications leverage future architectures by providing them with\n   state-of-the-art discretization algorithms  that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.    Collaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed  proxies and miniapps .    Provide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming  exascale software ecosystem .    CEED is a research partnership involving  30+ computational scientists  from\ntwo DOE labs and five universities, including members of the  Nek5000 , MFEM ,  MAGMA ,  OCCA  and  PETSc  projects.\nYou can reach us by emailing  ceed-users@llnl.gov \nor by leaving a comment in the  CEED user forum .  The center's co-design efforts are organized in  four interconnected R D\nthrusts , focused on the following computational motifs and their\nperformance on  exascale hardware .\nSee also our  publications .   PDE-based simulations on unstructured grids  CEED is producing a range of  software products  supporting general finite element  algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H 1 , H(curl), H(div) and L 2 /DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).   High-order/spectral finite elements  Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation  for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "Exascale Co-Design"
        }, 
        {
            "location": "/news/", 
            "text": "News\n\n\n\n\nFirst CEED annual meeting to be held at LLNL\n\n\nCEED will hold its first annual meeting in August, 2017 at the \nHPC Innovation\nCenter\n of\nLawrence Livermore National Laboratory.\n\n\nThe goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.\n\n\nIn addition to gathering together many of the CEED researchers, the meeting will\ninclude representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.\n\n\nPlease \ncontact the CEED team\n if you are interested\nin attending.\n\n\n\n\nGPU Hackathon 2017\n\n\nNek/CEED team participated the \nGPU Hackathon 2017\n\nthat was held in Brookhaven National Laboratory on June 5-9, 2017.\n\n\nOur team focused on performing and tuning GPU-enabled \nNek5000/Nekbone/NekCEM\n\nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.\n\n\n\n\nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\n\nThe second \nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED \nMAGMA\n team.\n\n\nThe goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.\n\n\nOf particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.\n\n\nThis is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found \nhere\n.\n\n\nSoftware release: MFEM v3.3\n\n\nVersion 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at \nhttp://mfem.org\n\n\nThe goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.\n\n\nIt has many features, including:\n\n\n\n\n2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.\n\n\nParallel version scalable to hundreds of thousands of MPI cores.\n\n\nConforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.\n\n\nGalerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.\n\n\nSupport for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.\n\n\nScalable algebraic multigrid, time integrators, and eigensolvers.\n\n\nLightweight interactive OpenGL visualization with the MFEM-based \nGLVis\n tool.\n\n\n\n\nSome of the \nnew additions in version 3.3\n are:\n\n\n\n\nComprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the \nPETSc\n and \nSUNDIALS\n suites.\n\n\nLinear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.\n\n\nGeneral quadrature and nodal finite element basis types.\n\n\nScalable parallel mesh format.\n\n\nThirty six new integrators for common families of operators.\n\n\nSixteen new serial and parallel example codes.\n\n\nSupport for CMake, on-the-fly compression of file streams, and HDF5-based output following the \nConduit\n mesh blueprint specification.\n\n\n\n\nMFEM is being developed in \nCASC\n, \nLLNL\n and is freely available under LGPL 2.1.\nFor more details, see the \ninteractive documentation\n and the full \nCHANGELOG\n.\n\n\n\n\nCEED co-design center announced\n\n\nThe \nExascale Computing Project (ECP)\n announced on November 11, 2016 its selection\nof four \nco-design centers\n, including CEED: the Center for\nEfficient Exascale Discretizations, which is a \nresearch partnership\n between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).\n\n\nAdditional news coverage can be found in \nLLNL Newsline\n\nand the \nANL press release\n.", 
            "title": "News"
        }, 
        {
            "location": "/news/#news", 
            "text": "First CEED annual meeting to be held at LLNL  CEED will hold its first annual meeting in August, 2017 at the  HPC Innovation\nCenter  of\nLawrence Livermore National Laboratory.  The goal of the meeting is to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.  In addition to gathering together many of the CEED researchers, the meeting will\ninclude representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.  Please  contact the CEED team  if you are interested\nin attending.   GPU Hackathon 2017  Nek/CEED team participated the  GPU Hackathon 2017 \nthat was held in Brookhaven National Laboratory on June 5-9, 2017.  Our team focused on performing and tuning GPU-enabled  Nek5000/Nekbone/NekCEM \nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.   Workshop on Batched, Reproducible, and Reduced Precision BLAS  The second  Workshop on Batched, Reproducible, and Reduced Precision BLAS \nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED  MAGMA  team.  The goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.  Of particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.  This is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found  here .  Software release: MFEM v3.3  Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at  http://mfem.org  The goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.  It has many features, including:   2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.  Parallel version scalable to hundreds of thousands of MPI cores.  Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.  Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.  Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.  Scalable algebraic multigrid, time integrators, and eigensolvers.  Lightweight interactive OpenGL visualization with the MFEM-based  GLVis  tool.   Some of the  new additions in version 3.3  are:   Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the  PETSc  and  SUNDIALS  suites.  Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.  General quadrature and nodal finite element basis types.  Scalable parallel mesh format.  Thirty six new integrators for common families of operators.  Sixteen new serial and parallel example codes.  Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the  Conduit  mesh blueprint specification.   MFEM is being developed in  CASC ,  LLNL  and is freely available under LGPL 2.1.\nFor more details, see the  interactive documentation  and the full  CHANGELOG .   CEED co-design center announced  The  Exascale Computing Project (ECP)  announced on November 11, 2016 its selection\nof four  co-design centers , including CEED: the Center for\nEfficient Exascale Discretizations, which is a  research partnership  between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).  Additional news coverage can be found in  LLNL Newsline \nand the  ANL press release .", 
            "title": "News"
        }, 
        {
            "location": "/codesign/", 
            "text": "Discretization Co-Design Approach\n\n\nCEED's co-design approach is based on close collaboration between its\n\nApplications\n, \nHardware\n, and \nSoftware\n thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's \nFinite Elements\n\nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.\n\n\nFor example, the development of \ndiscretization libraries\n in CEED\nis led by the \nFinite Elements\n thrust but involves working closely\nwith vendors (\nHardware\n thrust) and software technology efforts\n(\nSoftware\n thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of, and are successfully incorporated in,\nECP applications is based on collaboration between the \nApplications\n and\n\nFinite Elements\n thrusts.\n\n\nIn addition to libraries of highly performant kernels, a key product of the CEED\nproject will be a set of \nminiapps\n that will serve multiple roles:\n\n\n\n\nProvide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.\n\n\nServe as stand-alone unit-test drivers for the library kernels.\n\n\nProvide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).\n\n\nProvide test and demonstration cases for application scientists who are\nconsidering new formulations.\n\n\n\n\nThe \nminiapps\n developed in CEED, which we also refer to as\n\nCEEDlings\n, will range from local, element-level \nkernels\n, which can be run in\na simulator, to \nbake-off problems\n, which combine local and global\nkernels into model problem benchmarks for high-order computations, to \nproxy\napps\n, which will include application-relevant physics.\n\n\nThese encapsulated \nCEEDlings\n will be used in interactions with vendors\non \nemergent HPC technologies\n\nand \nECP software technologies\n\nprojects to highlight performance critical paths (e.g. size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of \nexascale architectures\n,\nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Approach"
        }, 
        {
            "location": "/codesign/#discretization-co-design-approach", 
            "text": "CEED's co-design approach is based on close collaboration between its Applications ,  Hardware , and  Software  thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's  Finite Elements \nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.  For example, the development of  discretization libraries  in CEED\nis led by the  Finite Elements  thrust but involves working closely\nwith vendors ( Hardware  thrust) and software technology efforts\n( Software  thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of, and are successfully incorporated in,\nECP applications is based on collaboration between the  Applications  and Finite Elements  thrusts.  In addition to libraries of highly performant kernels, a key product of the CEED\nproject will be a set of  miniapps  that will serve multiple roles:   Provide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.  Serve as stand-alone unit-test drivers for the library kernels.  Provide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).  Provide test and demonstration cases for application scientists who are\nconsidering new formulations.   The  miniapps  developed in CEED, which we also refer to as CEEDlings , will range from local, element-level  kernels , which can be run in\na simulator, to  bake-off problems , which combine local and global\nkernels into model problem benchmarks for high-order computations, to  proxy\napps , which will include application-relevant physics.  These encapsulated  CEEDlings  will be used in interactions with vendors\non  emergent HPC technologies \nand  ECP software technologies \nprojects to highlight performance critical paths (e.g. size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of  exascale architectures ,\nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Discretization Co-Design Approach"
        }, 
        {
            "location": "/ap/", 
            "text": "Applications Thrust \n\n\nThe goal of CEED's Applications (AP) thrust, led by \nMisun Min\n\nfrom \nArgonne National Laboratory\n, is to impact a wide range of\n\nECP application teams\n through focused one-on-one interactions,\nfacilitated by CEED application liaisons, as well as through one-to-many interactions, based on\nthe development of easy-to-use discretization libraries for high-order finite element methods.\n\n\nSome of our ECP application targets are:\n\n\n\n\nCoupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL)\n\n\nMulti-physics Simulation Code (LLNL)\n\n\nMultiscale Coupled Urban System (ANL)\n\n\nTransforming Combustion Science and Technology with Exascale Simulations (SNL)\n\n\nCloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL)\n\n\n\n\nIn addition to maintaining a close connection with these high-order ECP\napplications, the AP thrust is also reaching out to low-order and non-ECP\napparitions and using its interactions to derive requirements for CEED\u2019s\n\nminiapps\n and \nsoftware technologies\n work.\n\n\nThe AP thrust is actively involved with a variety of \nco-design\n\nactivities both within ECP applications and CEED miniapps, such as: OpenACC- and\nCUDA-based GPU implementations, I/O performance improvement for large meshes,\nexploration of lightweight MPI and neighborhood collective MPI, and many more.", 
            "title": "Applications"
        }, 
        {
            "location": "/ap/#applications-thrust-wzxhzdk0", 
            "text": "The goal of CEED's Applications (AP) thrust, led by  Misun Min \nfrom  Argonne National Laboratory , is to impact a wide range of ECP application teams  through focused one-on-one interactions,\nfacilitated by CEED application liaisons, as well as through one-to-many interactions, based on\nthe development of easy-to-use discretization libraries for high-order finite element methods.  Some of our ECP application targets are:   Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL)  Multi-physics Simulation Code (LLNL)  Multiscale Coupled Urban System (ANL)  Transforming Combustion Science and Technology with Exascale Simulations (SNL)  Cloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL)   In addition to maintaining a close connection with these high-order ECP\napplications, the AP thrust is also reaching out to low-order and non-ECP\napparitions and using its interactions to derive requirements for CEED\u2019s miniapps  and  software technologies  work.  The AP thrust is actively involved with a variety of  co-design \nactivities both within ECP applications and CEED miniapps, such as: OpenACC- and\nCUDA-based GPU implementations, I/O performance improvement for large meshes,\nexploration of lightweight MPI and neighborhood collective MPI, and many more.", 
            "title": "Applications Thrust "
        }, 
        {
            "location": "/hw/", 
            "text": "Hardware Thrust \n\n\nThe goal of CEED's Hardware (HW) thrust, led by \nJack Dongarra\n\nfrom \nThe University of Tennessee, Knoxville\n, is to build a two-way\n(\npull-and-push\n) collaboration with vendors, where the CEED team will develop\nhardware-aware technologies (\npull\n) to understand performance bottlenecks and\ntake advantage of inevitable hardware trends, and vendor interactions to seek\n(\npush\n) impact and improve hardware designs within the ECP scope.\n\n\nIn addition to maintaining a close connection with ECP vendors, the HW thrust is\na connection point for the MPI, OpenMP, and compiler-related work in the ECP.\nMembers of the HW team are also actively involved with the \nbatched\nBLAS\n standardization efforts.\n\n\nThe HW thrust participates in a variety of \nco-design\n\nactivities, both with ECP hardware vendors and within CEED miniapps, to explore\noptimal data locality and motion and to enhance the scalability and parallelism of\nhigh-order algorithms.", 
            "title": "Hardware"
        }, 
        {
            "location": "/hw/#hardware-thrust-wzxhzdk0", 
            "text": "The goal of CEED's Hardware (HW) thrust, led by  Jack Dongarra \nfrom  The University of Tennessee, Knoxville , is to build a two-way\n( pull-and-push ) collaboration with vendors, where the CEED team will develop\nhardware-aware technologies ( pull ) to understand performance bottlenecks and\ntake advantage of inevitable hardware trends, and vendor interactions to seek\n( push ) impact and improve hardware designs within the ECP scope.  In addition to maintaining a close connection with ECP vendors, the HW thrust is\na connection point for the MPI, OpenMP, and compiler-related work in the ECP.\nMembers of the HW team are also actively involved with the  batched\nBLAS  standardization efforts.  The HW thrust participates in a variety of  co-design \nactivities, both with ECP hardware vendors and within CEED miniapps, to explore\noptimal data locality and motion and to enhance the scalability and parallelism of\nhigh-order algorithms.", 
            "title": "Hardware Thrust "
        }, 
        {
            "location": "/sw/", 
            "text": "Software Thrust \n\n\nThe goal of CEED's Software (SW) thrust, led by \nJed Brown\n from \nUniversity of Colorado Boulder\n, is\nto participate in the development of software libraries and frameworks of\ngeneral interest to the scientific computing community, facilitate collaboration\nbetween CEED software packages, enable integration into and/or interoperability\nwith overall ECP software technologies stack, streamline developer and user\nworkflows, maintain testing and benchmarking infrastructure, and coordinate CEED\nsoftware releases.\n\n\nIn addition to maintaining a close connection with ECP software technologies\nprojects, the SW thrust develops continuous integration and performance\nregression testing for CEED, helps with the benchmarking suite and implements\nsupport for package managers, such as\n\nSpack\n.\nMembers of the SW team are also\nactively involved with the \nmatrix-free solvers\n work and the\nefforts in \ngeneral interpolation\n and \nvisualization of high-order\nmeshes and functions\n.\n\n\nThe SW thrust participates in a variety of \nco-design\n activities\nsuch as the coordination of the design of CEED's APIs and the identification of\ncommon kernels and their regimes of relevance relative to the parent\napplication.", 
            "title": "Software"
        }, 
        {
            "location": "/sw/#software-thrust-wzxhzdk0", 
            "text": "The goal of CEED's Software (SW) thrust, led by  Jed Brown  from  University of Colorado Boulder , is\nto participate in the development of software libraries and frameworks of\ngeneral interest to the scientific computing community, facilitate collaboration\nbetween CEED software packages, enable integration into and/or interoperability\nwith overall ECP software technologies stack, streamline developer and user\nworkflows, maintain testing and benchmarking infrastructure, and coordinate CEED\nsoftware releases.  In addition to maintaining a close connection with ECP software technologies\nprojects, the SW thrust develops continuous integration and performance\nregression testing for CEED, helps with the benchmarking suite and implements\nsupport for package managers, such as Spack .\nMembers of the SW team are also\nactively involved with the  matrix-free solvers  work and the\nefforts in  general interpolation  and  visualization of high-order\nmeshes and functions .  The SW thrust participates in a variety of  co-design  activities\nsuch as the coordination of the design of CEED's APIs and the identification of\ncommon kernels and their regimes of relevance relative to the parent\napplication.", 
            "title": "Software Thrust "
        }, 
        {
            "location": "/bps/", 
            "text": "CEED Bake-off Problems (Benchmarks)\n\n\nThis page contains the specifications of CEED's \nbake-off problems\n: high-order\nkernels/benchmarks designed to test and compare the performance of high-order\ncodes.\n\n\nCurrently available bake-off problems:\n\n\n\n\nBP1: Mass Matrix Evaluation\n\n\n\n\nCEED's bake-off problems are currently under development. Stay tuned for more details...\n\n\nTerminology and Notation\n\n\nVector representation/storage categories:\n\n\n\n\nTrue degrees of freedom/unknowns, \nT-vector\n:\n\n\neach unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$\n\n\nthis is a non-overlapping vector decomposition\n\n\nusually includes any essential (fixed) dofs.\n\n\n\n\n\n\n\nLocal true degrees of freedom/unknowns, \nL-vector\n:\n\n\neach unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$\n\n\nthis is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor\n\n\nthe shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n  \n\n\n\n\n\n\nPer element decomposition, \nE-vector\n:\n\n\neach unknown $i$ has as many copies as the number of elements that contain\n  $i$\n\n\nusually, the copies of the unknowns are grouped by the element they belong\n  to.\n  \n\n\n\n\n\n\nIn the case of hanging nodes (giving rise to hanging dofs):\n\n\nthere is another representation similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs, \nH-vector\n\n\nthe additional hanging dofs are duplicated when they are shared by\n  multiple processors.\n\n\n\n\n\n\nIn the case of variable order spaces:\n\n\nthe dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case\n\n\nhave both L- and H-vector representations.\n\n\n\n\n\n\nQuadrature point vector, \nQ-vector\n:\n\n\nthis is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.\n\n\n\n\n\n\nIn many cases it is useful to distinguish two types of vectors:\n\n\nX-vector, or \nprimal\n X-vector, and X'-vector, or \ndual\n X-vector\n\n\nhere X can be any of the T, L, H, E, or Q categories\n\n\nfor example, the mass matrix operator maps a T-vector to a T'-vector\n\n\nthe solutions vector is a T-vector, and the RHS vector is a T'-vector\n\n\nusing the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.\n\n\n\n\n\n\n\n\nOperator representation/storage/action categories:\n\n\n\n\nFull true-dof parallel assembly, \nTA\n, or \nA\n:\n\n\nParCSR or similar format\n\n\nthe T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.\n\n\n\n\n\n\nFull local assembly, \nLA\n:\n\n\nCSR matrix on each rank\n\n\nthe parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action\n\n\nnote that $P$ is the operator mapping T-vectors to L-vectors.\n\n\n\n\n\n\nElement matrix assembly, \nEA\n:\n\n\neach element matrix is stored as a dense matrix\n\n\noptimized element and parallel prolongation operators\n\n\nnote that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.\n\n\n\n\n\n\nQuadrature-point/partial assembly, \nQA\n or \nPA\n:\n\n\nprecompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements\n\n\nthe stored data can be viewed as a Q-vector.\n\n\n\n\n\n\nUnassembled option,  \nUA\n or \nU\n:\n\n\nno assembly step\n\n\nthe action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).\n\n\n\n\n\n\n\n\nBake-off Problem 1\n\n\nSetup:\n\n\n\n\nMass matrix\n\n\nCoefficient: constant $1$\n\n\n3D hex mesh\n\n\nNo essential boundary conditions, or essential boundary conditions on the\n  whole boundary\n\n\nSolution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?\n\n\nQuadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the quadrature order is $2q-1$\n\n\nAlso, consider $q=2$, for $p=1$, and $q=3$, for $p=2$\n\n\nUse nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension\n\n\nConsider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$\n\n\nFocus on the QA/PA operator representation\n\n\nMeshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$\n\n\nFor example:\n\n\nif $s=15$, then $s_1=s_2=s_3=5$\n\n\nif $s=16$, then $s_1=6$ and $s_2=s_3=5$\n\n\nif $s=17$, then $s_1=s_2=6$ and $s_3=5$\n\n\n\n\n\n\nConsider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh\n  into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$\n  are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$\n\n\nWhat are good partitioning algorithms for the\n  strong scaling limit? The problem is to generate well balanced partitions when\n  the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4\n  does not do well on this type of problems. What about METIS 5 and other\n  graph partitioners? Maybe we need to develop specialized algorithms?\n\n\nNumber of MPI tasks, and number of MPI tasks per node:\n\n\nOn ALCF's BG/Q, \nCetus\n: $2^{14}$ tasks total, with $2^5$ tasks per node\n\n\n\n\n\n\n...\n\n\n\n\nReport:\n\n\n\n\nNumber of mesh elements, $E$\n\n\nPolynomial degree, $N$, or $p$\n\n\nTotal number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$\n\n\nTime per iteration = total CG time $/$ number of CG iterations\n\n\nNumber of iterations to reach relative residual reduction of $10^{-6}$\n\n\nTime for quadrature-point/partial assembly\n\n\nTime is measured as maximum over all MPI ranks; using \nMPI_Wtime()\n or other\n  similar function\n\n\n...\n\n\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/bps/#ceed-bake-off-problems-benchmarks", 
            "text": "This page contains the specifications of CEED's  bake-off problems : high-order\nkernels/benchmarks designed to test and compare the performance of high-order\ncodes.  Currently available bake-off problems:   BP1: Mass Matrix Evaluation   CEED's bake-off problems are currently under development. Stay tuned for more details...  Terminology and Notation  Vector representation/storage categories:   True degrees of freedom/unknowns,  T-vector :  each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$  this is a non-overlapping vector decomposition  usually includes any essential (fixed) dofs.    Local true degrees of freedom/unknowns,  L-vector :  each unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$  this is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor  the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n      Per element decomposition,  E-vector :  each unknown $i$ has as many copies as the number of elements that contain\n  $i$  usually, the copies of the unknowns are grouped by the element they belong\n  to.\n      In the case of hanging nodes (giving rise to hanging dofs):  there is another representation similar to L-vector which stores the\n  hanging/dependent dofs in addition to the true dofs,  H-vector  the additional hanging dofs are duplicated when they are shared by\n  multiple processors.    In the case of variable order spaces:  the dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case  have both L- and H-vector representations.    Quadrature point vector,  Q-vector :  this is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.    In many cases it is useful to distinguish two types of vectors:  X-vector, or  primal  X-vector, and X'-vector, or  dual  X-vector  here X can be any of the T, L, H, E, or Q categories  for example, the mass matrix operator maps a T-vector to a T'-vector  the solutions vector is a T-vector, and the RHS vector is a T'-vector  using the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.     Operator representation/storage/action categories:   Full true-dof parallel assembly,  TA , or  A :  ParCSR or similar format  the T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.    Full local assembly,  LA :  CSR matrix on each rank  the parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action  note that $P$ is the operator mapping T-vectors to L-vectors.    Element matrix assembly,  EA :  each element matrix is stored as a dense matrix  optimized element and parallel prolongation operators  note that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.    Quadrature-point/partial assembly,  QA  or  PA :  precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements  the stored data can be viewed as a Q-vector.    Unassembled option,   UA  or  U :  no assembly step  the action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).     Bake-off Problem 1  Setup:   Mass matrix  Coefficient: constant $1$  3D hex mesh  No essential boundary conditions, or essential boundary conditions on the\n  whole boundary  Solution space orders: $p=2,3,\\ldots,15$; also, consider $p=1$?  Quadrature: Gauss-Legendre (GL) with $q=p+2$ points in each spatial\n  dimension; the quadrature order is $2q-1$  Also, consider $q=2$, for $p=1$, and $q=3$, for $p=2$  Use nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each spatial\n  dimension  Consider mesh orders of $p_{mesh}=1$, and $p_{mesh}=p$  Focus on the QA/PA operator representation  Meshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$  For example:  if $s=15$, then $s_1=s_2=s_3=5$  if $s=16$, then $s_1=6$ and $s_2=s_3=5$  if $s=17$, then $s_1=s_2=6$ and $s_3=5$    Consider tests with $2^t$ processors, $0\\le t\\le s$, and partition the mesh\n  into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where $\\{t_i\\}$\n  are derived from $t$ the same way $\\{s_i\\}$ are derived from $s$  What are good partitioning algorithms for the\n  strong scaling limit? The problem is to generate well balanced partitions when\n  the ratio \"number of elements\" $/$ \"number of processors\" is small. METIS 4\n  does not do well on this type of problems. What about METIS 5 and other\n  graph partitioners? Maybe we need to develop specialized algorithms?  Number of MPI tasks, and number of MPI tasks per node:  On ALCF's BG/Q,  Cetus : $2^{14}$ tasks total, with $2^5$ tasks per node    ...   Report:   Number of mesh elements, $E$  Polynomial degree, $N$, or $p$  Total number of degrees of freedom, $n_T$ (size of a T-vector), or\n  approximately $n:=E N^3$  Time per iteration = total CG time $/$ number of CG iterations  Number of iterations to reach relative residual reduction of $10^{-6}$  Time for quadrature-point/partial assembly  Time is measured as maximum over all MPI ranks; using  MPI_Wtime()  or other\n  similar function  ...   MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED Bake-off Problems (Benchmarks)"
        }, 
        {
            "location": "/miniapps/", 
            "text": "CEED Miniapps\n\n\nCEED is developing a variety of miniapps encapsulating key physics and numerical\nkernels of high-order applications.  The miniapps are designed to be used in a\nvariety of \nco-design\n activities with ECP vendors, software\ntechnologies projects as well as external partners.\n\n\nCEED's current miniapps are documented below and can be found on\n\nGitHub\n. Please contact the CEED team if you have any\nquestions.\n\n\n\n\n\n\nNekbone\n\n\n\n\nNekbone solves a standard Poisson equation using a conjugate gradient iteration\nwith a simple or spectral element multigrid preconditioner on a block or linear\ngeometry.\n\n\nIt exposes the principal computational kernel to reveal the essential elements\nof the algorithmic-architectural coupling that is pertinent to \nNek5000\n,\nrelevant to large eddy simulation (LES) and direct numerical simulation (DNS) of\nturbulence in complex domains.\n\n\nNekbone-3.1 was released in Oct 2013 as part of the \nCESAR co-design\ncenter\n.\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\nDownload Nekbone\n\n\n\n\n\n\n\n\n\n\nLaghos \nNew\n\n\n\n\nLaghos solves the time-dependent Euler equation of compressible gas dynamics in\na moving Lagrangian frame using high-order finite element spatial discretization\nand explicit high-order time-stepping.\n\n\nIt exposes the principal computational kernels of shock-capturing compressible\nflow, including the FLOP-intensive definition of artificial viscosity at\nquadrature points.\n\n\nLaghos will be released soon as part of CEED. Stay tuned for more details...\n\n\n\n\n\n\nNekCEM CEEDling \nNew\n\n\n\n\nNekCEM CEEDling is a \nNekCEM\n miniapp, solving the time-domain Maxwell and drift-diffusion\nequations.\n\n\nNekCEM CEEDling will be released soon as part of CEED. Stay tuned for more details...\n\n\n\n\n\n\nHolmes \nNew\n\n\n\n\nHolmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.\n\n\nIts features include:\n\n\n\n\nElements\n\n\nSpectral element quadrilateral and hexahedral elements\n\n\nWarp \n blend high-order nodal triangular and tetrahedral elements\n\n\n\n\n\n\nDiscretizations\n\n\nElliptic: continuous and discontinuous Galerkin discretization\n\n\nHyperbolic: discontinuous Galerkin time-domain\n\n\n\n\n\n\nSolvers\n\n\nTwo level overlapping additive Schwartz elliptic problem preconditioning\n\n\nFast approximate block preconditioning\n\n\nAlgebraic multigrid coarse solver with heterogeneous acceleration\n\n\nParallel direct solver XXT for the coarsest level solve\n\n\n\n\n\n\nFull algebraic multigrid preconditioning\n\n\n\n\n\n\nImplementation\n\n\nPortable on-node parallelism via the Open Concurrent Compute Abstraction, \nOCCA\n\n\nMPI communications for distributed computations\n\n\n\n\n\n\nDependencies\n\n\nThe XXT parallel direct solver for the coarsest AMG solve\n\n\nGSLIB\n for optimized gather-scatter operations\n\n\n\n\n\n\nExample CEEDling apps\n\n\nIncompressible flow solvers\n\n\nElliptic solvers\n\n\nBoltzmann flow solver\n\n\nAcoustics time-domain solver\n\n\n\n\n\n\n\n\nHolmes is being developed in \nTim Warburton's group\n\nat \nVirginia Tech\n.\n\n\nThe project GitHub repository will be made public in 2017. Stay tuned for more details...\n\n\n\n\n\n\nHPGMG-FE\n\n\n\n\nThe High-Performance Geometric Multigrid Finite Element benchmark is a\nFull Multigrid solver for a third order accurate finite element\ndiscretization of an elliptic operator on mapped grids.  The benchmark\nis designed to give a picture of the \"performance spectrum\" across a\nrange of problem sizes and thus solution time, thereby giving a clear\npicture of both weak and strong scaling concerns.\n\n\nFor details, see the \nHPGMG website\n.\n\n\n\n\n\n\nMFEM Example Codes\n\n\n\n\nThe \nMFEM\n distribution includes a large number of simple example codes\nthat can be viewed as simple miniapps for model high-order physics:\n\n\n\n\nExample 1\n: nodal H1 FEM for the Laplace problem,\n\n\nExample 2\n: vector FEM for linear elasticity,\n\n\nExample 3\n: Nedelec H(curl) FEM for the definite Maxwell problem,\n\n\nExample 4\n: Raviart-Thomas H(div) FEM for the grad-div problem,\n\n\nExample 5\n: mixed pressure-velocity FEM for the Darcy problem,\n\n\nExample 6\n: non-conforming adaptive mesh refinement (AMR) for the Laplace problem,\n\n\nExample 7\n: Laplace problem on a surface (the unit sphere),\n\n\nExample 8\n: Discontinuous Petrov-Galerkin (DPG) for the Laplace problem,\n\n\nExample 9\n: Discontinuous Galerkin (DG) time-dependent advection,\n\n\nExample 10\n: time-dependent implicit nonlinear elasticity,\n\n\nExample 11\n: parallel Laplace eigensolver,\n\n\nExample 12\n: parallel linear elasticity eigensolver,\n\n\nExample 13\n: parallel Maxwell eigensolver,\n\n\nExample 14\n: Discontinuous Galerkin (DG) for the Laplace problem,\n\n\nExample 15\n: dynamic AMR for Laplace with prescribed time-dependent source,\n\n\nExample 16\n: time-dependent nonlinear heat equation,\n\n\nExample 17\n: Discontinuous Galerkin (DG) for linear elasticity.\n\n\n\n\nMost of the examples have a serial and a parallel version, illustrating the ease\nof transition and the minimal code changes between the two.\n\n\nOf particular relevance to CEED is the HPC versions of the example codes that\nuse a \nset of templated classes\n to efficiently\nimplement high-order operator evaluation.\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\nDownload MFEM", 
            "title": "Miniapps"
        }, 
        {
            "location": "/miniapps/#ceed-miniapps", 
            "text": "CEED is developing a variety of miniapps encapsulating key physics and numerical\nkernels of high-order applications.  The miniapps are designed to be used in a\nvariety of  co-design  activities with ECP vendors, software\ntechnologies projects as well as external partners.  CEED's current miniapps are documented below and can be found on GitHub . Please contact the CEED team if you have any\nquestions.    Nekbone   Nekbone solves a standard Poisson equation using a conjugate gradient iteration\nwith a simple or spectral element multigrid preconditioner on a block or linear\ngeometry.  It exposes the principal computational kernel to reveal the essential elements\nof the algorithmic-architectural coupling that is pertinent to  Nek5000 ,\nrelevant to large eddy simulation (LES) and direct numerical simulation (DNS) of\nturbulence in complex domains.  Nekbone-3.1 was released in Oct 2013 as part of the  CESAR co-design\ncenter .    Documentation     Download Nekbone      Laghos  New   Laghos solves the time-dependent Euler equation of compressible gas dynamics in\na moving Lagrangian frame using high-order finite element spatial discretization\nand explicit high-order time-stepping.  It exposes the principal computational kernels of shock-capturing compressible\nflow, including the FLOP-intensive definition of artificial viscosity at\nquadrature points.  Laghos will be released soon as part of CEED. Stay tuned for more details...    NekCEM CEEDling  New   NekCEM CEEDling is a  NekCEM  miniapp, solving the time-domain Maxwell and drift-diffusion\nequations.  NekCEM CEEDling will be released soon as part of CEED. Stay tuned for more details...    Holmes  New   Holmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.  Its features include:   Elements  Spectral element quadrilateral and hexahedral elements  Warp   blend high-order nodal triangular and tetrahedral elements    Discretizations  Elliptic: continuous and discontinuous Galerkin discretization  Hyperbolic: discontinuous Galerkin time-domain    Solvers  Two level overlapping additive Schwartz elliptic problem preconditioning  Fast approximate block preconditioning  Algebraic multigrid coarse solver with heterogeneous acceleration  Parallel direct solver XXT for the coarsest level solve    Full algebraic multigrid preconditioning    Implementation  Portable on-node parallelism via the Open Concurrent Compute Abstraction,  OCCA  MPI communications for distributed computations    Dependencies  The XXT parallel direct solver for the coarsest AMG solve  GSLIB  for optimized gather-scatter operations    Example CEEDling apps  Incompressible flow solvers  Elliptic solvers  Boltzmann flow solver  Acoustics time-domain solver     Holmes is being developed in  Tim Warburton's group \nat  Virginia Tech .  The project GitHub repository will be made public in 2017. Stay tuned for more details...    HPGMG-FE   The High-Performance Geometric Multigrid Finite Element benchmark is a\nFull Multigrid solver for a third order accurate finite element\ndiscretization of an elliptic operator on mapped grids.  The benchmark\nis designed to give a picture of the \"performance spectrum\" across a\nrange of problem sizes and thus solution time, thereby giving a clear\npicture of both weak and strong scaling concerns.  For details, see the  HPGMG website .    MFEM Example Codes   The  MFEM  distribution includes a large number of simple example codes\nthat can be viewed as simple miniapps for model high-order physics:   Example 1 : nodal H1 FEM for the Laplace problem,  Example 2 : vector FEM for linear elasticity,  Example 3 : Nedelec H(curl) FEM for the definite Maxwell problem,  Example 4 : Raviart-Thomas H(div) FEM for the grad-div problem,  Example 5 : mixed pressure-velocity FEM for the Darcy problem,  Example 6 : non-conforming adaptive mesh refinement (AMR) for the Laplace problem,  Example 7 : Laplace problem on a surface (the unit sphere),  Example 8 : Discontinuous Petrov-Galerkin (DPG) for the Laplace problem,  Example 9 : Discontinuous Galerkin (DG) time-dependent advection,  Example 10 : time-dependent implicit nonlinear elasticity,  Example 11 : parallel Laplace eigensolver,  Example 12 : parallel linear elasticity eigensolver,  Example 13 : parallel Maxwell eigensolver,  Example 14 : Discontinuous Galerkin (DG) for the Laplace problem,  Example 15 : dynamic AMR for Laplace with prescribed time-dependent source,  Example 16 : time-dependent nonlinear heat equation,  Example 17 : Discontinuous Galerkin (DG) for linear elasticity.   Most of the examples have a serial and a parallel version, illustrating the ease\nof transition and the minimal code changes between the two.  Of particular relevance to CEED is the HPC versions of the example codes that\nuse a  set of templated classes  to efficiently\nimplement high-order operator evaluation.    Documentation     Download MFEM", 
            "title": "CEED Miniapps"
        }, 
        {
            "location": "/software/", 
            "text": "Software Catalog\n\n\nThe CEED team includes members of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:\n\n\n\n\n\n\nNext-generation \nfinite element\n \ndiscretization\n\n  \nlibraries\n that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.\n\n\n\n\n\n\nMiniapps\n combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and \nbenchmark problems\n.\n\n\n\n\n\n\nBroadly applicable technologies, including extensions of dense linear algebra\n  libraries to support \nfast tensor contractions\n, scalable\n  matrix-free \nlinear solvers\n and programming models for \nperformance\n  portability\n.\n\n\n\n\n\n\nThe CEED software is open-source and publicly available at\n  \nhttps://github.com/ceed\n.", 
            "title": "Catalog"
        }, 
        {
            "location": "/software/#software-catalog", 
            "text": "The CEED team includes members of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:    Next-generation  finite element   discretization \n   libraries  that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.    Miniapps  combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and  benchmark problems .    Broadly applicable technologies, including extensions of dense linear algebra\n  libraries to support  fast tensor contractions , scalable\n  matrix-free  linear solvers  and programming models for  performance\n  portability .    The CEED software is open-source and publicly available at\n   https://github.com/ceed .", 
            "title": "Software Catalog"
        }, 
        {
            "location": "/ceed-code/", 
            "text": "CEED APIs\n\n\nCEED will build on the efforts of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.\n\n\nHigh level API\n\n\nThe CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.\n\n\nGiven such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.\n\n\nThe global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nLow level API\n\n\nThe CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:\n\n\n\nWe take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as \nsum\nfactorization\n.\nIn the case where we precompute and store the $D$ matrix, we call the algorithm\n\npartial assembly\n.\n\n\nThe low-level API can be used as the foundation for an efficient high-order\n\noperator format\n.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/ceed-code/#ceed-apis", 
            "text": "CEED will build on the efforts of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.  High level API  The CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.  Given such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.  The global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.  This API is currently under development. Stay tuned for more details...  Low level API  The CEED low-level API operates with the foundational components of finite\nelement operators, described by the following decomposition:  We take advantage of the tensor-product structure of both the finite element\nbasis and the quadrature rule to efficiently apply the action of $B$ without\nnecessarily computing its entries. This is generally know as  sum\nfactorization .\nIn the case where we precompute and store the $D$ matrix, we call the algorithm partial assembly .  The low-level API can be used as the foundation for an efficient high-order operator format .  This API is currently under development. Stay tuned for more details...  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/gslib/", 
            "text": "GSLIB\n\n\nGSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications.  It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers.\n\n\nAny global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix.  GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$.  Users simply provide the local-to-global map $j(i)$ on each processor.  GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms.  The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks.\n\n\nGSLIB supports\n\n\n\n\nStencils of arbitrary width\n\n\n64-bit indexing for global addressing\n\n\nshort / long ints, floats, doubles\n\n\narbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$\n\n\nGPUDirect communication (beta-version)\n\n\ngather-scatter on arbitrary m-tuples (vector fields)\n\n\n\n\nAdditional features include:\n\n\n\n\nXXT solver (parallel direct solver for coarse-grid problems)\n\n\nAMG solver (design for solving coarse-grid problems in parallel)\n\n\nRobust and efficient spectral/finite element interpolation in parallel.\n\n\n\n\nApplications and libraries using GSLIB include \nNek5000\n, \nNekCEM\n,\n\nNektar++\n, and \nMOAB\n.\n\n\nIn CEED, GSLIB is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the GSLIB GitHub repository: \nhttps://github.com/gslib/gslib\n.\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "GSLIB"
        }, 
        {
            "location": "/gslib/#gslibwzxhzdk2", 
            "text": "GSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications.  It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers.  Any global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix.  GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$.  Users simply provide the local-to-global map $j(i)$ on each processor.  GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms.  The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks.  GSLIB supports   Stencils of arbitrary width  64-bit indexing for global addressing  short / long ints, floats, doubles  arbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$  GPUDirect communication (beta-version)  gather-scatter on arbitrary m-tuples (vector fields)   Additional features include:   XXT solver (parallel direct solver for coarse-grid problems)  AMG solver (design for solving coarse-grid problems in parallel)  Robust and efficient spectral/finite element interpolation in parallel.   Applications and libraries using GSLIB include  Nek5000 ,  NekCEM , Nektar++ , and  MOAB .  In CEED, GSLIB is primarily involved in the efforts of the  Software  thrust.  For more information, see the GSLIB GitHub repository:  https://github.com/gslib/gslib .  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "GSLIB"
        }, 
        {
            "location": "/magma/", 
            "text": "MAGMA\n\n\n\n\n\nMAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.\n\n\n\n\nMAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.\n\n\nHybrid Algorithms\n\n\nMAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.\n\n\n\n\nIn either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.\n\n\nPerformance and Energy Efficiency\n\n\nMAGMA solvers run close to the machine's peak performance. The LU factorization for\nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.\n\n\n\n\nMAGMA Batched\n\n\nMAGMA Batched targets small linear algebra operations. Small computational tasks are difficult\nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.\n\n\n\n\nMAGMA Sparse\n\n\nMAGMA Sparse targets the development of high-performance sparse linear algebra operations on\naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace\niterative solvers, eigensolvers, and preconditioners.\n\n\n\n\nMAGMA Development\n\n\nMAGMA is being developed at the \nInnovative Computing Laboratory\n\nof the \nUniversity of Tennessee\n.\n\n\nIn CEED, MAGMA is primarily involved in the efforts of the \nSoftware\n, \nHardware\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MAGMA website: \nhttp://icl.cs.utk.edu/magma\n.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#magma", 
            "text": "MAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.   MAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.  Hybrid Algorithms  MAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.   In either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.  Performance and Energy Efficiency  MAGMA solvers run close to the machine's peak performance. The LU factorization for\nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.   MAGMA Batched  MAGMA Batched targets small linear algebra operations. Small computational tasks are difficult\nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.   MAGMA Sparse  MAGMA Sparse targets the development of high-performance sparse linear algebra operations on\naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace\niterative solvers, eigensolvers, and preconditioners.   MAGMA Development  MAGMA is being developed at the  Innovative Computing Laboratory \nof the  University of Tennessee .  In CEED, MAGMA is primarily involved in the efforts of the  Software ,  Hardware  and  Finite Element  thrusts.  For more information, see the MAGMA website:  http://icl.cs.utk.edu/magma .", 
            "title": "MAGMA"
        }, 
        {
            "location": "/mfem/", 
            "text": "MFEM\n \n\n\nMFEM is a free, lightweight, scalable C++ library for finite element methods.\n\n\nIts goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.\n\n\nIts features include:\n\n\n\n\nArbitrary high-order finite element \nmeshes\n\nand \nspaces\n.\n\n\nWide variety\n of finite element discretization approaches.\n\n\nConforming and nonconforming \nadaptive mesh refinement\n.\n\n\nScalable to \nhundreds of thousands\n of cores.\n\n\n... and \nmany more\n.\n\n\n\n\nMFEM is being developed in \nCASC\n,\n\nLLNL\n and is freely available under LGPL 2.1.\n\n\nIn CEED, MFEM is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MFEM website: \nhttp://mfem.org\n.", 
            "title": "MFEM"
        }, 
        {
            "location": "/mfem/#mfem-wzxhzdk0", 
            "text": "MFEM is a free, lightweight, scalable C++ library for finite element methods.  Its goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.  Its features include:   Arbitrary high-order finite element  meshes \nand  spaces .  Wide variety  of finite element discretization approaches.  Conforming and nonconforming  adaptive mesh refinement .  Scalable to  hundreds of thousands  of cores.  ... and  many more .   MFEM is being developed in  CASC , LLNL  and is freely available under LGPL 2.1.  In CEED, MFEM is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the MFEM website:  http://mfem.org .", 
            "title": "MFEM "
        }, 
        {
            "location": "/nek/", 
            "text": "Nek\n \n\n\nNek5000/NekCEM is an open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications, including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.\n\n\nIt features state-of-the-art, scalable, high-order spectral element-based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.\n\n\nNek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.\n\n\n\n\nReactor analysis\n\n\nAerospace application\n\n\nEngine application\n\n\nOcean modeling\n\n\nTurbulence modeling\n\n\nElectromagnetics modeling\n\n\nDrift-diffusion  modeling\n\n\n\n\nNek5000 and NekCEM are freely available under a BSD license.\n\n\nIn CEED, Nek is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the Nek5000 website: \nhttps://nek5000.mcs.anl.gov\n.", 
            "title": "Nek"
        }, 
        {
            "location": "/nek/#nek-wzxhzdk0", 
            "text": "Nek5000/NekCEM is an open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications, including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.  It features state-of-the-art, scalable, high-order spectral element-based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.  Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.   Reactor analysis  Aerospace application  Engine application  Ocean modeling  Turbulence modeling  Electromagnetics modeling  Drift-diffusion  modeling   Nek5000 and NekCEM are freely available under a BSD license.  In CEED, Nek is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the Nek5000 website:  https://nek5000.mcs.anl.gov .", 
            "title": "Nek "
        }, 
        {
            "location": "/occa/", 
            "text": "OCCA\n \n\n\nOCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and lets the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.\n\n\nOCCA abstracts the variety of device programming languages into one kernel\nlanguage, the OCCA kernel language (OKL). OKL minimally extends C and restricts\nthe user to write parallel code that is JIT compiled.\n\n\nOCCA is freely available under an MIT license.\n\n\nIn CEED, OCCA is primarily involved in the efforts of the \nSoftware\n and\n\nFinite Element\n thrusts.\n\n\nFor more information, see the OCCA website: \nhttp://libocca.org\n.", 
            "title": "OCCA"
        }, 
        {
            "location": "/occa/#occa-wzxhzdk0", 
            "text": "OCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and lets the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.  OCCA abstracts the variety of device programming languages into one kernel\nlanguage, the OCCA kernel language (OKL). OKL minimally extends C and restricts\nthe user to write parallel code that is JIT compiled.  OCCA is freely available under an MIT license.  In CEED, OCCA is primarily involved in the efforts of the  Software  and Finite Element  thrusts.  For more information, see the OCCA website:  http://libocca.org .", 
            "title": "OCCA "
        }, 
        {
            "location": "/petsc/", 
            "text": "PETSc\n\n\nPETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.\n\n\nAs part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.\n\n\nPETSc is freely available under a BSD license.\n\n\nIn CEED, PETSc is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the PETSc website: \nhttps://www.mcs.anl.gov/petsc/\n.", 
            "title": "PETSc"
        }, 
        {
            "location": "/petsc/#petsc", 
            "text": "PETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.  As part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.  PETSc is freely available under a BSD license.  In CEED, PETSc is primarily involved in the efforts of the  Software  thrust.  For more information, see the PETSc website:  https://www.mcs.anl.gov/petsc/ .", 
            "title": "PETSc"
        }, 
        {
            "location": "/pumi/", 
            "text": "PUMI\n \n\n\nPUMI\n is an unstructured, distributed mesh data management system designed for\n massively parallel computing.\n\n\nPUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:\n\n\n\n\nPCU\n for phased message passing and thread management.\n\n\nGMI\n for geometric model interface.\n\n\nMDS\n for unstructured mesh representation.\n\n\nAPF Mesh\n for partition model and distributed mesh management.\n\n\nAPF_Field\n for field management.\n\n\n\n\nPUMI is being developed at RPI's \nScientific Computation Research\nCenter\n and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.\n\n\nIn CEED, PUMI is primarily involved in the efforts of the \nFinite Element\n\nand \nSoftware\n thrusts.\n\n\nFor more information, see the \nPUMI documents\n.", 
            "title": "PUMI"
        }, 
        {
            "location": "/pumi/#pumi-wzxhzdk0", 
            "text": "PUMI  is an unstructured, distributed mesh data management system designed for\n massively parallel computing.  PUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:   PCU  for phased message passing and thread management.  GMI  for geometric model interface.  MDS  for unstructured mesh representation.  APF Mesh  for partition model and distributed mesh management.  APF_Field  for field management.   PUMI is being developed at RPI's  Scientific Computation Research\nCenter  and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.  In CEED, PUMI is primarily involved in the efforts of the  Finite Element \nand  Software  thrusts.  For more information, see the  PUMI documents .", 
            "title": "PUMI "
        }, 
        {
            "location": "/thrusts/", 
            "text": "R\nD Thrusts\n\n\nCEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R\nD thrusts\nfocused on these customers and tied together by the foundational \nfinite element\nthrust\n.\n\n\n\n\nThe specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the \nOutreach page\n.\n\n\n\n\nApplications Thrust (AP)\n\n\nThe goal of CEED's Applications thrust is to impact a wide range of \nECP\napplication teams\n through focused\none-on-one interactions, facilitated by CEED application liaisons, as well as\nthrough one-to-many interactions, based on the development of easy-to-use\ndiscretization libraries for high-order finite element methods.\n\n\n\n\nHardware Thrust (HW)\n\n\nThe goal of CEED's Hardware thrust is to build a two-way (\npull-and-push\n)\ncollaboration with vendors, where the CEED team will develop hardware-aware\ntechnologies (\npull\n) to understand performance bottlenecks and take advantage\nof inevitable hardware trends, and vendor interactions to seek (\npush\n) impact\nand improve hardware designs within the ECP scope.\n\n\n\n\nSoftware Thrust (SW)\n\n\nThe goal of CEED's Software thrust is to participate in the development of\nsoftware libraries and frameworks of general interest to the scientific\ncomputing community, facilitate collaboration between CEED software packages,\nenable integration into and/or interoperability with overall ECP software\ntechnologies stack, streamline developer and user workflows, maintain testing\nand benchmarking infrastructure, and coordinate CEED software releases.\n\n\n\n\nFinite Elements Thrust (FE)\n\n\nThe goal of CEED's Finite Element thrust is to continue to improve the\nstate-of-the-art spectral-element/high-order finite element algorithms and\nkernels in the CEED software targeting exascale architectures, connect and\ncontributes to the efforts of the other thrusts, and lead the development of\ndiscretization libraries, benchmarks and miniapps.", 
            "title": "R&D Thrusts"
        }, 
        {
            "location": "/thrusts/#rd-thrusts", 
            "text": "CEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R D thrusts\nfocused on these customers and tied together by the foundational  finite element\nthrust .   The specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the  Outreach page .   Applications Thrust (AP)  The goal of CEED's Applications thrust is to impact a wide range of  ECP\napplication teams  through focused\none-on-one interactions, facilitated by CEED application liaisons, as well as\nthrough one-to-many interactions, based on the development of easy-to-use\ndiscretization libraries for high-order finite element methods.   Hardware Thrust (HW)  The goal of CEED's Hardware thrust is to build a two-way ( pull-and-push )\ncollaboration with vendors, where the CEED team will develop hardware-aware\ntechnologies ( pull ) to understand performance bottlenecks and take advantage\nof inevitable hardware trends, and vendor interactions to seek ( push ) impact\nand improve hardware designs within the ECP scope.   Software Thrust (SW)  The goal of CEED's Software thrust is to participate in the development of\nsoftware libraries and frameworks of general interest to the scientific\ncomputing community, facilitate collaboration between CEED software packages,\nenable integration into and/or interoperability with overall ECP software\ntechnologies stack, streamline developer and user workflows, maintain testing\nand benchmarking infrastructure, and coordinate CEED software releases.   Finite Elements Thrust (FE)  The goal of CEED's Finite Element thrust is to continue to improve the\nstate-of-the-art spectral-element/high-order finite element algorithms and\nkernels in the CEED software targeting exascale architectures, connect and\ncontributes to the efforts of the other thrusts, and lead the development of\ndiscretization libraries, benchmarks and miniapps.", 
            "title": "R&amp;D Thrusts"
        }, 
        {
            "location": "/pubs/", 
            "text": "Publications and Outreach\n\n\n\n\nCEED Documents\n\n\n\n\nCEED's high-order \nBenchmarks\n and \nMiniapps\n.\n\n\nActivities in the \nApplications\n, \nHardware\n, \nSoftware\n and \nFinite Element\n thrusts.\n\n\nCEED-proposed high-order \nOperator\n and \nVisualization\n formats.\n\n\n\n\n\n\nPublications\n\n\n2017\n\n\n\n\nK. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji, \nWhy is MPI so slow? Analysing the fundamental limits in implementing MPI-3.1\n, accepted, \nSC'17\n, \n2017\n\n\nV. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov, \nSequential limiting in continuous and discontinuous Galerkin methods for the Euler equations\n, submitted, \n2017\n\n\nA. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, \nFactorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures\n, \nProceedings of the 2017 International Conference on Computational Science, ICCS'17\n, Z\u00fcrich, Switzerland, June 12-14, \nProcedia Computer Science\n, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, \nSmall Tensor Operations on Advanced Architectures for High-order Applications\n, Technical report UT-EECS-17-749, EECS Department, University of Tennessee, \n2017\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov,\n\nHigh-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation\n, Journal of Computational Physics, 334:102\u2013124, \n2017\n\n\n\n\n\n\n2016\n\n\n\n\nV. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, \nMulti-material closure model for high-order finite element Lagrangian hydrodynamics\n, \nInt. J. Numer. Meth. Fluids\n, 82(10), pp. 689\u2013706, \n2016\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nHigh-performance Tensor Contractions for GPUs\n, \nProcedia Computer Science\n, Volume 80, Pages 108-118, ISSN 1877-0509, \n2016\n.\n\n\nM. B.E., Y. Peet, P. Fischer, and J. Lottes, \nA spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations\n, \nJ. Comp. Phys.\n, 307:60\u201393, \n2016\n.\n\n\nM. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, \nAn MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication\n, \nThe International Journal of High Performance Computing Application\n, 30(3):320\u2013334, \n2016\n.\n\n\nJ. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, \nNekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations\n, \nSpecial issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing\n, \n2016\n.\n\n\n\n\n\n\n2015 and earlier\n\n\n\n\nP. Fischer, K. Heisey, and M. Min, \nScaling limits for PDE-based simulation\n, \nIn 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation\n, AIAA 2015-3049, \n2015\n.\n\n\nA. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, \nErosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches\n, volume 2, pp. 1448\u20131461. \nAmerican Nuclear Society\n, \n2015\n.\n\n\nE. Merzari, P. Fischer, and J. Walker, \nLarge-scale simulation of rod bundles: Coherent structure recognition and stability analysis\n, volume 1. \nAmerican Society of Mechanical Engineers\n, \n2015\n.\n\n\nM. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, \nEntanglement of two, three and four plasmonically coupled quantum dots\n, \nPhysical Review B\n, 92:125432, \n2015\n.\n\n\nD. A. May, J. Brown, and L. Le Pourhiet. \npTatin3D: High-performance methods for long-term lithospheric dynamics\n, In Proceedings of \nSC14: International Conference for High Performance Computing, Networking, Storage and Analysis\n. ACM, \n2014\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, \nMonotonicity in high-order curvilinear finite element ALE remap\n, \nInt. J. Numer. Meth. Fluids\n, 77(5), pp. 249\u2013273, \n2014\n.\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG solver for H(div) problems\n,  \nSIAM J. Sci. Comp.\n, 34, pp. A3079\u2013A3098, \n2012\n.\n\n\nV. Dobrev, Tz. Kolev and R. Rieben, \nHigh-order curvilinear finite element methods for Lagrangian hydrodynamics\n, \nSIAM J. Sci. Comp.\n, 34, pp. B606\u2013B641, \n2012\n.\n\n\nJ. Brown, \nEfficient nonlinear solvers for nodal high-order finite elements in 3D\n, \nJournal of Scientific Computing\n, 45:48\u201363, \n2010\n. doi:10.1007/s10915-010-9396-8\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG for H(curl) problems\n, \nJ. Comput. Math.\n, 27, pp. 604-623, \n2009\n.\n\n\n\n\n\n\nPresentations\n\n\n2017\n\n\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nAccelerating Tensor Contractions in High-Order FEM with MAGMA Batched\n, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nP. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\nM. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, February 26-March 3, \n2017\n.\n\n\n\n\n\n\nOther Resources\n\n\n\n\nLLNL's \nexascale computing website\n.\n\n\nNews coverage of CEED announcement in  \nLLNL Newsline\n\nand the \nANL press release\n.\n\n\nU.S. Department of Energy \nExascale Initiative\n.", 
            "title": "Outreach"
        }, 
        {
            "location": "/pubs/#publications-and-outreach", 
            "text": "CEED Documents   CEED's high-order  Benchmarks  and  Miniapps .  Activities in the  Applications ,  Hardware ,  Software  and  Finite Element  thrusts.  CEED-proposed high-order  Operator  and  Visualization  formats.    Publications  2017   K. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji,  Why is MPI so slow? Analysing the fundamental limits in implementing MPI-3.1 , accepted,  SC'17 ,  2017  V. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov,  Sequential limiting in continuous and discontinuous Galerkin methods for the Euler equations , submitted,  2017  A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra,  Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures ,  Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14,  Procedia Computer Science ,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov,  Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, University of Tennessee,  2017 .  R. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov, High-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation , Journal of Computational Physics, 334:102\u2013124,  2017    2016   V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov,  Multi-material closure model for high-order finite element Lagrangian hydrodynamics ,  Int. J. Numer. Meth. Fluids , 82(10), pp. 689\u2013706,  2016  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  High-performance Tensor Contractions for GPUs ,  Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509,  2016 .  M. B.E., Y. Peet, P. Fischer, and J. Lottes,  A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations ,  J. Comp. Phys. , 307:60\u201393,  2016 .  M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min,  An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication ,  The International Journal of High Performance Computing Application , 30(3):320\u2013334,  2016 .  J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min,  Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations ,  Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing ,  2016 .    2015 and earlier   P. Fischer, K. Heisey, and M. Min,  Scaling limits for PDE-based simulation ,  In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049,  2015 .  A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer,  Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461.  American Nuclear Society ,  2015 .  E. Merzari, P. Fischer, and J. Walker,  Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1.  American Society of Mechanical Engineers ,  2015 .  M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray,  Entanglement of two, three and four plasmonically coupled quantum dots ,  Physical Review B , 92:125432,  2015 .  D. A. May, J. Brown, and L. Le Pourhiet.  pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of  SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM,  2014 .  R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben,  Monotonicity in high-order curvilinear finite element ALE remap ,  Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273,  2014 .  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG solver for H(div) problems ,   SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098,  2012 .  V. Dobrev, Tz. Kolev and R. Rieben,  High-order curvilinear finite element methods for Lagrangian hydrodynamics ,  SIAM J. Sci. Comp. , 34, pp. B606\u2013B641,  2012 .  J. Brown,  Efficient nonlinear solvers for nodal high-order finite elements in 3D ,  Journal of Scientific Computing , 45:48\u201363,  2010 . doi:10.1007/s10915-010-9396-8  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG for H(curl) problems ,  J. Comput. Math. , 27, pp. 604-623,  2009 .    Presentations  2017   A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched ,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .  M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, February 26-March 3,  2017 .    Other Resources   LLNL's  exascale computing website .  News coverage of CEED announcement in   LLNL Newsline \nand the  ANL press release .  U.S. Department of Energy  Exascale Initiative .", 
            "title": "Publications and Outreach"
        }, 
        {
            "location": "/fe/", 
            "text": "Finite Element Thrust \n\n\nThe goal of CEED's Finite Element (FE) thrust, led by \nVeselin Dobrev\n\nfrom \nLawrence Livermore National Laboratory\n, is to continue to improve\nthe state-of-the-art spectral-element/high-order finite element algorithms and kernels in the\nCEED software targeting exascale architectures.\n\n\nWhy High-Order Finite Elements?\n\n\nEfficient exploitation of\n\nexascale architectures\n requires\na rethink of the numerical algorithms used in large-scale scientific applications.\nThese architectures favor algorithms that expose ultra-fine-grain parallelism\nand maximize the ratio of floating-point operations to energy-intensive data\nmovement.\n\n\nMany large-scale \napplications\n employ unstructured finite element\ndiscretization methods, where practical efficiency is measured by the accuracy\nachieved per unit computational time.  One of the few viable approaches to\nachieve high performance in this case is to use matrix-free high-order finite\nelement methods, since these methods can both increase the accuracy and/or lower\nthe computational time due to reduced data motion.\n\n\nTo achieve this efficiency, high-order methods use mesh elements that are mapped\nfrom canonical reference elements (hexes, wedges, pyramids, tetrahedra) and\nexploit, where possible, the tensor-product structure of the canonical mesh\nelements and finite element spaces. Through matrix-free partial assembly, the\nuse of canonical reference elements enables substantial cache efficiency and\nminimizes extraneous data movement in comparison to traditional low-order\napproaches.\n\n\nHigh-Order Benchmarks and Miniapps\n\n\nThe FE thrust works closely with the \nApplications\n and \nHardware\n\nthrusts on the development of CEED \nbenchmarks\n and\n\nminiapps\n, and coordinates the project software activities with\nthe \nSoftware\n thrust.\n\n\nEcosystem for High-Order Applications\n\n\nIn addition to performance improvements, the FE thrust is focusing on\ncommunity-wide challenges that are necessary for establishing a\nfull-fledged high-order application software ecosystem.\n\n\nThese efforts include research and development in the following areas:\n\n\n\n\nHigh-Order Meshes\n\n\nUnstructured Adaptive Mesh Refinement\n\n\nEfficient High-Order Operator Format / Representation\n\n\nBatched Dense Tensor Contractions\n\n\nScalable Matrix-Free Solvers\n\n\nGeneral Interpolation\n\n\nVisualization of High-Order Meshes and Functions", 
            "title": "Finite Elements"
        }, 
        {
            "location": "/fe/#finite-element-thrust-wzxhzdk0", 
            "text": "The goal of CEED's Finite Element (FE) thrust, led by  Veselin Dobrev \nfrom  Lawrence Livermore National Laboratory , is to continue to improve\nthe state-of-the-art spectral-element/high-order finite element algorithms and kernels in the\nCEED software targeting exascale architectures.  Why High-Order Finite Elements?  Efficient exploitation of exascale architectures  requires\na rethink of the numerical algorithms used in large-scale scientific applications.\nThese architectures favor algorithms that expose ultra-fine-grain parallelism\nand maximize the ratio of floating-point operations to energy-intensive data\nmovement.  Many large-scale  applications  employ unstructured finite element\ndiscretization methods, where practical efficiency is measured by the accuracy\nachieved per unit computational time.  One of the few viable approaches to\nachieve high performance in this case is to use matrix-free high-order finite\nelement methods, since these methods can both increase the accuracy and/or lower\nthe computational time due to reduced data motion.  To achieve this efficiency, high-order methods use mesh elements that are mapped\nfrom canonical reference elements (hexes, wedges, pyramids, tetrahedra) and\nexploit, where possible, the tensor-product structure of the canonical mesh\nelements and finite element spaces. Through matrix-free partial assembly, the\nuse of canonical reference elements enables substantial cache efficiency and\nminimizes extraneous data movement in comparison to traditional low-order\napproaches.  High-Order Benchmarks and Miniapps  The FE thrust works closely with the  Applications  and  Hardware \nthrusts on the development of CEED  benchmarks  and miniapps , and coordinates the project software activities with\nthe  Software  thrust.  Ecosystem for High-Order Applications  In addition to performance improvements, the FE thrust is focusing on\ncommunity-wide challenges that are necessary for establishing a\nfull-fledged high-order application software ecosystem.  These efforts include research and development in the following areas:   High-Order Meshes  Unstructured Adaptive Mesh Refinement  Efficient High-Order Operator Format / Representation  Batched Dense Tensor Contractions  Scalable Matrix-Free Solvers  General Interpolation  Visualization of High-Order Meshes and Functions", 
            "title": "Finite Element Thrust "
        }, 
        {
            "location": "/meshing/", 
            "text": "High-Order Meshes \n\n\nHigh-order (curved) meshes appear in many applications, e.g. due to curved\nmaterial interfaces or due to mesh motion in simulations with moving meshes\n(for example, ALE methods).\n\n\nThe use of high-order geometry representation can lead to better feature\nresolution, improved symmetry preservation, better solution adaptivity and\nincreased robustness for problems where the mesh is being deformed in time.\n\n\nThe CEED software supports arbitrary order quadrilateral, hexahedral, triangular\nand tetrahedral meshes and our team members are developing algorithms for\nhigh-order mesh optimization and solution transfer between deformed high-order\nmeshes (remap).\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nUnstructured AMR \n\n\nAdaptive Mesh Refinement (AMR) on unstructured grids is an important enabling\ntechnology for many applications.\n\n\nThe CEED software supports both conforming AMR on triangular and tetrahedral\nmeshes as well as general non-conforming AMR on quadrilateral and hexahedral\nmeshes. Both approaches work for low-order meshes with no performance penalty\nand support parallel dynamic load balancing.\n\n\nOne of the distinguishing characteristics of our AMR approach is that it is\nimplemented at the level of the discretization library, decoupled from the\nphysics, so it can easily be incorporated in a variety of applications.  We\nsupport spaces in the whole de Rham complex, as well as AMR on high-order\nmeshes.\n\n\nCEED scientists are working on practical error indicators and\nphysics-conservative AMR interpolations to meet application needs.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Meshing"
        }, 
        {
            "location": "/meshing/#high-order-meshes-wzxhzdk0", 
            "text": "High-order (curved) meshes appear in many applications, e.g. due to curved\nmaterial interfaces or due to mesh motion in simulations with moving meshes\n(for example, ALE methods).  The use of high-order geometry representation can lead to better feature\nresolution, improved symmetry preservation, better solution adaptivity and\nincreased robustness for problems where the mesh is being deformed in time.  The CEED software supports arbitrary order quadrilateral, hexahedral, triangular\nand tetrahedral meshes and our team members are developing algorithms for\nhigh-order mesh optimization and solution transfer between deformed high-order\nmeshes (remap).  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "High-Order Meshes "
        }, 
        {
            "location": "/meshing/#unstructured-amr-wzxhzdk1", 
            "text": "Adaptive Mesh Refinement (AMR) on unstructured grids is an important enabling\ntechnology for many applications.  The CEED software supports both conforming AMR on triangular and tetrahedral\nmeshes as well as general non-conforming AMR on quadrilateral and hexahedral\nmeshes. Both approaches work for low-order meshes with no performance penalty\nand support parallel dynamic load balancing.  One of the distinguishing characteristics of our AMR approach is that it is\nimplemented at the level of the discretization library, decoupled from the\nphysics, so it can easily be incorporated in a variety of applications.  We\nsupport spaces in the whole de Rham complex, as well as AMR on high-order\nmeshes.  CEED scientists are working on practical error indicators and\nphysics-conservative AMR interpolations to meet application needs.  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Unstructured AMR "
        }, 
        {
            "location": "/linalg/", 
            "text": "Batched Dense Tensor Contractions \n\n\nThe numerical kernels of efficient high-order operator evaluation reduce to many\nsmall dense tensor contractions, one for each element of the computational mesh.\nThese contractions can be performed in parallel over the elements and can be\nimplemented as a sequence of small matrix-matrix multiplications (dgemms).\n\n\nCEED is developing efficient algorithms for these numerical kernels on a variety\nof advanced hardware, both by directly working on the specifying tensor\ncontraction found in high-order finite element kernels, as well as by recasting\nto batched interface of LAPACK-like linear algebra libraries providing efficient\ndgemms for matrices of small-to-medium size.\n\n\nCEED scientists are also involved in the standardization efforts for batched\nversions of Basic Linear Algebra Software Library (BLAS) routines, which are the\nfoundations for LAPACK-type libraries.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nScalable Matrix-Free Solvers\n\n\nCEED is developing matrix-free linear solvers and preconditioners for \npartially\nassembled\n high-order operators utilizing expertise from solver\nlibraries like \nhypre\n and\n\nPETSc\n.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Linear Algebra"
        }, 
        {
            "location": "/linalg/#batched-dense-tensor-contractions-wzxhzdk0", 
            "text": "The numerical kernels of efficient high-order operator evaluation reduce to many\nsmall dense tensor contractions, one for each element of the computational mesh.\nThese contractions can be performed in parallel over the elements and can be\nimplemented as a sequence of small matrix-matrix multiplications (dgemms).  CEED is developing efficient algorithms for these numerical kernels on a variety\nof advanced hardware, both by directly working on the specifying tensor\ncontraction found in high-order finite element kernels, as well as by recasting\nto batched interface of LAPACK-like linear algebra libraries providing efficient\ndgemms for matrices of small-to-medium size.  CEED scientists are also involved in the standardization efforts for batched\nversions of Basic Linear Algebra Software Library (BLAS) routines, which are the\nfoundations for LAPACK-type libraries.  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Batched Dense Tensor Contractions "
        }, 
        {
            "location": "/linalg/#scalable-matrix-free-solvers", 
            "text": "CEED is developing matrix-free linear solvers and preconditioners for  partially\nassembled  high-order operators utilizing expertise from solver\nlibraries like  hypre  and PETSc .  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Scalable Matrix-Free Solvers"
        }, 
        {
            "location": "/discr/", 
            "text": "Efficient High-Order Operator Format \n\n\nWhile a global (parallel) sparse matrix is a good representation of a PDE\noperator discretized with low-order elements, a global parallel matrix is a bad\nchoice when discretizing with high-order elements, due to the large cost of both\nthe memory transfer and floating point operations.\n\n\nCEED is developing an alternative operator format, based on the CEED \nlow-level\nAPI\n, that allows efficient operator evaluation that is optimal in\nmemory and nearly-optimal in FLOPs cost.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...\n\n\nGeneral Interpolation of Solution Field Values\n\n\nParticle tracking, grid-to-grid transfer, and data analysis are typical\noperations that require off-grid function evaluation.\n\n\nCEED is developing a scalable interpolation routine for arbitrary-order\nhexahedral elements that uses a hash table to rapidly identify candidate\nelements/processors that might contain the point in question, followed by a\nNewton iteration to find the point in the reference domain.\n\n\nThe iteration is based on minimization, rather than root-finding, which is\nadvantageous when the interpolation point is on or near an element boundary\nwhere high-order interpolants tend to exhibit rapid variation.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Discretization"
        }, 
        {
            "location": "/discr/#efficient-high-order-operator-format-wzxhzdk0", 
            "text": "While a global (parallel) sparse matrix is a good representation of a PDE\noperator discretized with low-order elements, a global parallel matrix is a bad\nchoice when discretizing with high-order elements, due to the large cost of both\nthe memory transfer and floating point operations.  CEED is developing an alternative operator format, based on the CEED  low-level\nAPI , that allows efficient operator evaluation that is optimal in\nmemory and nearly-optimal in FLOPs cost.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Efficient High-Order Operator Format "
        }, 
        {
            "location": "/discr/#general-interpolation-of-solution-field-values", 
            "text": "Particle tracking, grid-to-grid transfer, and data analysis are typical\noperations that require off-grid function evaluation.  CEED is developing a scalable interpolation routine for arbitrary-order\nhexahedral elements that uses a hash table to rapidly identify candidate\nelements/processors that might contain the point in question, followed by a\nNewton iteration to find the point in the reference domain.  The iteration is based on minimization, rather than root-finding, which is\nadvantageous when the interpolation point is on or near an element boundary\nwhere high-order interpolants tend to exhibit rapid variation.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "General Interpolation of Solution Field Values"
        }, 
        {
            "location": "/vis/", 
            "text": "Visualization of High-Order Meshes and Functions \n\n\nAccurate visualization of general finite element meshes and functions in the de\nRham complex requires finite element knowledge that may not be present in\nvisualization tools employed by applications. The visualization needs to account\nfor the orders of the mesh and solution fields as well as the type of finite\nelement basis used for each of them.\n\n\nAn additional challenge for high-order meshes and functions is that there is no\ncommon community standard for the description of high-order data at arbitrary\nother.\n\n\nCEED is working with visualization and application teams to develop a\nstandard, that not only improves visualization capabilities but also enables\nconsistent data transfer between high-order applications.\n\n\nOur work is based on the current capabilities in \nMFEM\n, illustrated in\nits native \nGLVis\n visualization tool as well as in the\n\nVisIt\n visualization and data analysis application.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Visualization"
        }, 
        {
            "location": "/vis/#visualization-of-high-order-meshes-and-functions-wzxhzdk0", 
            "text": "Accurate visualization of general finite element meshes and functions in the de\nRham complex requires finite element knowledge that may not be present in\nvisualization tools employed by applications. The visualization needs to account\nfor the orders of the mesh and solution fields as well as the type of finite\nelement basis used for each of them.  An additional challenge for high-order meshes and functions is that there is no\ncommon community standard for the description of high-order data at arbitrary\nother.  CEED is working with visualization and application teams to develop a\nstandard, that not only improves visualization capabilities but also enables\nconsistent data transfer between high-order applications.  Our work is based on the current capabilities in  MFEM , illustrated in\nits native  GLVis  visualization tool as well as in the VisIt  visualization and data analysis application.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Visualization of High-Order Meshes and Functions "
        }, 
        {
            "location": "/about/", 
            "text": "About CEED\n\n\nThe Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:\n\n\n\n\nArgonne National Laboratory\n\n\nLawrence Livermore National Laboratory\n\n\nRensselaer Polytechnic Institute\n\n\nThe University of Tennessee, Knoxville\n\n\nUniversity of Colorado Boulder\n\n\nUniversity of Illinois Urbana-Champaign\n\n\nVirginia Tech\n\n\n\n\nYou can reach us by emailing \nceed-users@llnl.gov\n or by leaving a comment in\nthe \nCEED user forum\n.\n\n\n\n\n\n\n\nThis research is supported by the \nExascale Computing Project\n (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a \ncapable exascale ecosystem\n, including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s \nexascale computing imperative\n.\n\n\n\n\nOur Team\n\n\n\n\nAhmad Abdelfattah\n\n\nAleks Obabko\n\n\nAli Karakus\n\n\nAndrew Siegel\n\n\nAzzam Haidar\n\n\nBarry Smith\n\n\nCameron Smith\n\n\nDavid Beckingsale\n\n\nDavid Medina\n\n\nIan Karlin\n\n\nJack Dongarra\n \n Lead for the \nHardware\n thrust\n\n\nJed Brown\n \n Lead for the \nSoftware\n thrust\n\n\nKatie Heisey\n\n\nKazem Kamran\n\n\nMark Shepard\n\n\nMatt Otten\n\n\nMisun Min\n \n Lead for the \nApplications\n thrust\n\n\nNoel Chalmers\n\n\nPanayot Vassilevski\n\n\nPaul Fischer\n \n Deputy Director of CEED\n\n\nPedro Bello-Maldonado\n\n\nRobert Rieben\n\n\nRon Rahaman\n\n\nScott Parker\n\n\nSom Dutta\n\n\nStanimire Tomov\n\n\nStefan Kerkemeier\n\n\nThilina Ratnayake\n\n\nTim Moon\n\n\nTim Warburton\n\n\nTzanio Kolev\n \n Director of CEED\n\n\nVeselin Dobrev\n \n Lead for the \nFinite Element\n thrust\n\n\nVladimir Tomov\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nBootstrap\n\nand \nBootswatch\n. Hosted on \nGitHub\n.\n\n\nLLNL-WEB-732668.\n\nPrivacy \n Legal Notice\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-ceed", 
            "text": "The Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:   Argonne National Laboratory  Lawrence Livermore National Laboratory  Rensselaer Polytechnic Institute  The University of Tennessee, Knoxville  University of Colorado Boulder  University of Illinois Urbana-Champaign  Virginia Tech   You can reach us by emailing  ceed-users@llnl.gov  or by leaving a comment in\nthe  CEED user forum .    This research is supported by the  Exascale Computing Project  (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a  capable exascale ecosystem , including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s  exascale computing imperative .", 
            "title": "About CEED"
        }, 
        {
            "location": "/about/#our-team", 
            "text": "Ahmad Abdelfattah  Aleks Obabko  Ali Karakus  Andrew Siegel  Azzam Haidar  Barry Smith  Cameron Smith  David Beckingsale  David Medina  Ian Karlin  Jack Dongarra    Lead for the  Hardware  thrust  Jed Brown    Lead for the  Software  thrust  Katie Heisey  Kazem Kamran  Mark Shepard  Matt Otten  Misun Min    Lead for the  Applications  thrust  Noel Chalmers  Panayot Vassilevski  Paul Fischer    Deputy Director of CEED  Pedro Bello-Maldonado  Robert Rieben  Ron Rahaman  Scott Parker  Som Dutta  Stanimire Tomov  Stefan Kerkemeier  Thilina Ratnayake  Tim Moon  Tim Warburton  Tzanio Kolev    Director of CEED  Veselin Dobrev    Lead for the  Finite Element  thrust  Vladimir Tomov    Website built with  MkDocs ,  Bootstrap \nand  Bootswatch . Hosted on  GitHub .  LLNL-WEB-732668. Privacy   Legal Notice .", 
            "title": "Our Team"
        }
    ]
}