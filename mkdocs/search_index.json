{
    "docs": [
        {
            "location": "/", 
            "text": "Exascale Co-Design\n\n\nThe \nCenter for Efficient Exascale Discretizations (CEED)\n is a co-design center within the\nU.S. Department of Energy (DOE) \nExascale Computing Project (ECP)\n\nwith the following goals:\n\n\n\n\n\n\nHelp applications leverage future architectures by providing them with\n  \nstate-of-the-art discretization algorithms\n that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.\n\n\n\n\n\n\nCollaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed \nproxies and miniapps\n.\n\n\n\n\n\n\nProvide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming \nexascale software ecosystem\n.\n\n\n\n\n\n\nCEED is a research partnership involving \n30+ computational scientists\n from\ntwo DOE labs and five universities, including members of the \nNek5000\n,\n\nMFEM\n, \nMAGMA\n, \nOCCA\n and \nPETSc\n projects.\nYou can reach us by emailing \nceed-users@llnl.gov\n\nor by leaving a comment in the \nCEED user forum\n.\n\n\nThe center's co-design efforts are organized in \nfour interconnected R\nD\nthrusts\n, focused on the following computational motifs and their\nperformance on \nexascale hardware\n.\nSee also our \npublications\n.\n\n\n\n\nPDE-based simulations on unstructured grids\n\n\nCEED is producing a range of \nsoftware products\n supporting general\n\nfinite element\n algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H\n1\n, H(curl), H(div) and L\n2\n/DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).\n\n\n\n\nHigh-order/spectral finite elements\n\n\nOur algorithms and software come with comprehensive high-order support: we provide\n\nefficient matrix-free operator evaluation\n for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "Home"
        }, 
        {
            "location": "/#exascale-co-design", 
            "text": "The  Center for Efficient Exascale Discretizations (CEED)  is a co-design center within the\nU.S. Department of Energy (DOE)  Exascale Computing Project (ECP) \nwith the following goals:    Help applications leverage future architectures by providing them with\n   state-of-the-art discretization algorithms  that better exploit\n  the hardware and deliver a significant performance gain over conventional low-order\n  methods.    Collaborate with hardware vendors and software technologies projects to\n  utilize and impact the upcoming exascale hardware and its software stack through\n  CEED-developed  proxies and miniapps .    Provide an efficient and user-friendly unstructured PDE discretization\n  component for the upcoming  exascale software ecosystem .    CEED is a research partnership involving  30+ computational scientists  from\ntwo DOE labs and five universities, including members of the  Nek5000 , MFEM ,  MAGMA ,  OCCA  and  PETSc  projects.\nYou can reach us by emailing  ceed-users@llnl.gov \nor by leaving a comment in the  CEED user forum .  The center's co-design efforts are organized in  four interconnected R D\nthrusts , focused on the following computational motifs and their\nperformance on  exascale hardware .\nSee also our  publications .", 
            "title": "Exascale Co-Design"
        }, 
        {
            "location": "/#pde-based-simulations-on-unstructured-grids", 
            "text": "CEED is producing a range of  software products  supporting general finite element  algorithms on triangular, quadrilateral, tetrahedral and\nhexahedral meshes in 3D, 2D and 1D.  We target the\nwhole de Rham complex: H 1 , H(curl), H(div) and L 2 /DG spaces and discretizations,\nincluding conforming and non-conforming unstructured adaptive mesh refinement\n(AMR).", 
            "title": "PDE-based simulations on unstructured grids"
        }, 
        {
            "location": "/#high-orderspectral-finite-elements", 
            "text": "Our algorithms and software come with comprehensive high-order support: we provide efficient matrix-free operator evaluation  for any order space on any\norder mesh, including high-order curved meshes and all geometries in the de Rham complex.\nThe CEED software will also include optimized assembly support for low-order methods.", 
            "title": "High-order/spectral finite elements"
        }, 
        {
            "location": "/news/", 
            "text": "News\n\n\nNew website launched by the Virginia Tech CEED team\n\n\nA new website was recently launched by the Parallel Numerical Algorithms\nresearch group at Virginia Tech \nhere\n. The\nsite includes a blog that gives some practical computing tips related to\nhigh performance implementations of finite element\nmethods developed as part of the CEED project \nhere\n.\n\n\n\n\nInitial release of libCEED: The CEED API library\n\n\nThe initial version of \nlibCEED\n,\nthe CEED API library, was released in December 2017.\n\n\nlibCEED is a high-order API library, that for the first time provides a common\noperator description on \nalgebraic level\n,\nthat allows a wide variety of applications to take advantage of the efficient operator\nevaluation algorithms in the different CEED packages (from a single source).\n\n\nOur long-term vision for libCEED is to include a variety of back-end\nimplementations, ranging from simple reference kernels, to highly optimized\nkernels targeting specific devices (e.g. GPUs) or specific polynomial orders.\n\n\nFor more information visit\n\nhttps://github.com/CEED/libCEED\n.\n\n\n\n\nSoftware release: MFEM v3.3.2\n\n\nVersion 3.3.2 of \nMFEM\n was released on November 10, 2017. Some of the\nnew additions in this release are:\n\n\n\n\nSupport for high-order mesh optimization based on the target-matrix\n  optimization paradigm from the \nETHOS project\n.\n\n\nImplementation of the community policies in xSDK, the \nExtreme-scale\n  Scientific Software Development Kit\n.\n\n\nIntegration with the\n  \nSTRUMPACK\n parallel sparse\n  direct solver and preconditioner.\n\n\nSeveral new \nlinear interpolators\n, five new\n  examples and miniapps.\n\n\nVarious memory, performance, discretization and solver improvements, including\n  physical-to-reference space mapping capabilities.\n\n\nContinuous integration testing on Linux, Mac and Windows.\n\n\n\n\nFor more details, see the \ninteractive documentation\n and the\nfull \nCHANGELOG\n at\n\nhttp://mfem.org\n.\n\n\n\n\nCEED participates in xSDK and FASTMath\n\n\nMFEM joined xSDK, the \nExtreme-scale Scientific Software Development\nKit\n in ECP's software technologies focus area as of release\nxSDK-0.3.0, see \nhttps://xsdk.info/packages\n.\n\n\nMFEM\n and \nPUMI\n are also part of the \nFASTMath\n\ninstitute in the SciDAC program, see \nhttps://fastmath-scidac.llnl.gov/software-catalog.html\n.\n\n\n\n\nSoftware release: Nek5000 v17.0\n\n\nNek5000\n version 17.0 was released as a major upgrade to Nek5000.\nMajor features improvements include:\n\n\n\n\nRefactored build system.\n\n\nNew user-input parameter file format (\n.par\n replacing \n.rea\n).\n\n\nCharacteristics (large time-step) support for moving mesh problems.\n\n\nMoving mesh support for the $PN-PN$ formulation.\n\n\nImproved stability for $PN-PN$ with variable viscosity.\n\n\nSupport for mixed \nHelmholtz/CVODE\n solves.\n\n\nNew fast \nAMG setup\n tool based on HYPRE.\n\n\nNew \nEXODUSII\n mesh converter.\n\n\nNew interface to \nlibxsmm\n (fast MATMUL library).\n\n\nExtended \nlowMach\n solver for time varying thermodynamic pressure.\n\n\nAdded DG for scalars.\n\n\nReduced solver initialization time (parallel binary reader for all input files).\n\n\nAutomatic general mesh-to-mesh transfer for restarts.\n\n\nRefactored support for overlapping domains (NekNek).\n\n\nAdded high-pass filter relaxation (alternative to explicit filter).\n\n\nRefactored residual projection including support for coupled Helmholtz solves.\n\n\n\n\n\n\nNekbone and Laghos join proxy app suites\n\n\nThe Nekbone and Laghos \nminiapps\n developed in CEED were selected to be part of ECP's\ninitial \nProxy Applications Suite\n.\n\n\nBoth miniapps were also picked to be \nCORAL-2 benchmarks\n.\n\n\nLaghos was also selected as one of LLNL's \nASC co-design miniapps\n.\n\n\n\n\n6th Nek5000 User Meeting to be held at U Florida\n\n\nThe 6th Nek5000 User/Developer Meeting will be hosted by the DOE PSAAP-II\nCompressible Multiphase Turbulence (CMT) center in Tampa, FL, March 17-18, 2018.\n\n\n\n\nNek5000 hackathon at UIUC\n\n\nThe inaugural Nek5000 Hackathon was held at NCSA Building, University of\nIllinois, Urbana-Champaign (UIUC), IL on Nov 12-14, 2017.\n\n\nThe event was attended by researchers and Nek5000 developers to promote the\napplication of Nek5000 to new problems from industry, national laboratories, and\nacademia.  Twenty-five participants spent three days working on setting up new\nexamples, developing new features, and helping one another to get maximum\nperformance on their applications. Some of the more prominent exchanges of ideas\nincluded standardization of synthetic turbulent inflow techniques, use of CVODE\nfor pure advection-diffusion problems, and the use of the characteristics\nmethods for moving geometry applications.\n\n\nFor more details, see the \nNek5000 hackathon website\n.\n\n\n\n\nCEED organizing minisymposium at ICOSAHOM 2018\n\n\nCEED is organizing a minisymposium, \nEfficient High-Order Finite Element\nDiscretizations at Large Scale\n, at the \nInternational Conference on Spectral and\nHigh-Order Methods\n (ICOSAHOM 2018) in London UK,\nJul 9-13, 2018.\n\n\nThe goal of the minisymposium is to discuss the next-generation high-order\ndiscretization algorithm and software, based on finite/spectral element\napproaches that will enable a wide range of important scientific applications to\nrun efficiently on future architecture.\n\n\n\n\nBest Paper Award at NURETH-17\n\n\nCEED researchers (P. Fischer, E. Merzari, A. Obabko) won a Best Paper Award at\nthe \n17th International Topical Meeting on Nuclear Reactor Thermal\nHydraulics\n (NURETH-17), held in China in September\n2017, with a paper entitled \nHigh-Fidelity Simulation of Flow Induced\nVibrations in Helical Steam Generators for Small Modular Reactors\n.\n\n\n\n\nCEED attending Cray, AMD and Intel Deep-Dives\n\n\nCEED researchers and representatives of the \nNek\n and \nMFEM\n\nteams will attend the October 2017, ECP vendor deep-dive meetings:\n\n\n\n\nCray deep-dive in Bloomington, MN on Oct 18-19\n\n\nAMD deep-dive in Austin, TX on Oct 24-25\n\n\nIntel deep-dive in Hudson, MA on Oct 21-Nov 2\n\n\n\n\nTopics of discussion include advanced technology and memory design, strong\nscaling considerations and the porting and evaluation of CEED's \nbake-off\nproblems\n and \nminiapps\n (Nekbone, Laghos, NekCEM\nCEEDling).\n\n\n\n\nNew Nekbench repository\n\n\nNew \nNekbench repository\n has been\nreleased to provide scripts that simplify the benchmarking of \nNek5000\n.\n\n\nThe user provides ranges for important parameters ranges (e.g., processor counts\nand local problem size ranges) and a test type (e.g., scaling or ping-pong\ntest). Nekbench will run the given test in the given parameter space using a\nNek5000 case file which is also given by the user (in the ping-pong tests, the\ncase file is optional).\n\n\nNekbench is written using bash scripting language and runs any Unix-like\noperating system that supports bash. It has been successfully tested on Linux\nlaptops/desktops, ALCF Theta, NERSC Cori (KNL and Haswell), and NERSC Edison\nmachines for scaling tests.\n\n\nPlanned extensions for Nekbench include adding more machine types like ANL's\nCetus, additional support for the ping-pong test type, and automated plot\ngeneration (e.g., scaling study graphs) for each test run.\n\n\n\n\nGPU ports of Nek and Laghos\n\n\nGPU acceleration is a main focus of the \nperformance optimization\n\nefforts in CEED. Recent progress in this direction include GPU ports of CEED's\n\nNek5000\n application and the Nekbone and Laghos \nminiapps\n.\n\n\nFor Nek5000, an initial \nGPU-enabled version\n\nhas been developed based on OpenACC. For Nekbone a pure OpenACC implementation as well as a hybrid\nOpenACC/CUDA implementation with a CUDA kernel for matrix-vector multiplication\n\nhas been developed\n.\n\n\nFor Laghos, a \nGPU-enabled version of\n has been\nreleased using the \nOCCA\n interface. With this approach, the user is able to run Laghos\ndistributively using varying device types per MPI process, whether serial C++, OpenMP, or CUDA.\n\n\n\n\nFirst CEED annual meeting held at LLNL\n\n\n\n\nCEED held its first annual meeting in August, 2017 at the \nHPC Innovation\nCenter\n of\nLawrence Livermore National Laboratory.\n\n\nThe goal of the meeting was to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.\n\n\nIn addition to gathering together many of the CEED researchers, the meeting\nincluded representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.\n\n\n\n\nCEED researchers at ATPESC17\n\n\nSix CEED researchers presented at the 2017 edition of the \nArgonne Training Program\non Extreme-Scale Computing\n,\nnow part of the Exascale Computing Project.\n\n\nThe CEED presentations covered a wide variety of topics, from overview of Theta,\nto GPU programming, dense and sparse linear algebra, and high-order discretizations\non unstructured meshes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVideos of all 2017 talks are available on\n\nYouTube\n.\nCEED researchers have also participated in past editions of the meeting.\n\n\n\n\nCEED BPs and benchmarks repository released\n\n\nCEED released an initial set of \nbake-off (BP) problems\n, which are simple\nkernels designed to test and compare the performance of high-order codes, both\ninternally in CEED, as well as in the \nbroader high-order\ncommunity\n.\n\n\nIn addition to the benchmark descriptions on the \nCEED BPs page\n, a\n\nbenchmarks repository\n is publicly\navailable with several implementations of the CEED bake-off problems.\nCurrently, MFEM, Nek5000 and deal.ii are included, see directories\n\ntests/mfem_bps\n,\n\ntests/nek5000_bps\n\nand\n\ntests/dealii_bps\n\nrespectively.\n\n\n\n\nNew Laghos and NekCEM CEEDling miniapps released\n\n\nTwo new miniapps developed in CEED were released in June 2017: Laghos and NekCEM\nCEEDling.\n\n\nLaghos (LAGrangian High-Order Solver) is a new miniapp developed in CEED that\nsolves the time-dependent Euler equations of compressible gas dynamics in a\nmoving Lagrangian frame using unstructured high-order finite element spatial\ndiscretization and explicit high-order time-stepping. In CEED, Laghos serves as\na proxy for a sub-component of the MARBL/LLNLApp application.\n\n\nNekCEM CEEDling is a new NekCEM miniapp, solving the time-domain Maxwell\nequation for electromagnetic systems.\n\n\nFor more details, see the CEED \nminiapps page\n and the\n\nLaghos\n and \nNekCEM\nCEEDling\n repositories on GitHub.\n\n\n\n\nPaper with MPICH at SC17\n\n\nJoint paper with the MPICH group, \nWhy is MPI so Slow? Analyzing the fundamental limits in implementing MPI-3.1\n \naccepted in Supercomputing 2017\n.\nThe paper provides an in-depth analysis of the software overheads in the MPI\nperformance-critical path and exposes mandatory performance overheads that are\nunavoidable based on the MPI-3.1 specification.\n\n\n\n\nSTRUMPACK support in MFEM\n\n\nSupport for the sparse direct solver and preconditioner STRUMPACK \nhas been\nintegrated\n in MFEM.\n\n\nSTRUMPACK\n is being\ndeveloped at LBNL and is part of the ECP project \nFactorization Based Sparse\nSolvers and Preconditioners\n (Xiaoye Sherry Li and Pieter Ghysels). The\nSTRUMPACK solver is based on multifrontal sparse Gaussian elimination and uses\nhierarchically semi-separable matrices to compress fill-in. It can be used as an\nexact direct solver or as an algebraic, robust and parallel preconditioner for a\nrange of discretized PDE problems.\n\n\n\n\n2017 PETSc User Meeting\n\n\nOver 75 participants from all over the world attended the PETSc User Meeting,\nheld June 14-16 in Boulder, CO. Hosted by the University of Colorado Boulder, the\nevent consisted of a one-day tutorial on the solver library PETSc and showcased\nthe latest research enabled by the functionality available in PETSc. The\n\nmeeting agenda\n covered\na total of 15 talks, four posters, and two panels.\n\n\nThanks to generous support from Intel and Tech-X, 22 students received travel\ngrants and got to learn about the latest techniques on the large-scale numerical\nsolution of partial differential equations.\n\n\nPETSc is a suite of data structures and routines for the scalable (parallel)\nsolution of scientific applications modeled by partial differential\nequations. It has become one of the most widely used numerical software packages\nof its kind and has users in application areas ranging from acoustics and\narterial flow to seismology and semiconductors.\n\n\n\n\nGPU Hackathon at BNL\n\n\nNek/CEED team participated the \nGPU Hackathon 2017\n\nthat was held in Brookhaven National Laboratory on June 5-9, 2017.\nOur team focused on performing and tuning GPU-enabled \nNek5000/Nekbone/NekCEM\n\nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.\n\n\n\n\nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\n\nThe second \nWorkshop on Batched, Reproducible, and Reduced Precision BLAS\n\nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED \nMAGMA\n team.\n\n\nThe goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.\n\n\nOf particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.\n\n\nThis is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found \nhere\n.\n\n\n\n\nSoftware release: MFEM v3.3\n\n\nVersion 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at \nhttp://mfem.org\n\n\nThe goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.\n\n\nIt has many features, including:\n\n\n\n\n2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.\n\n\nParallel version scalable to hundreds of thousands of MPI cores.\n\n\nConforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.\n\n\nGalerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.\n\n\nSupport for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.\n\n\nScalable algebraic multigrid, time integrators, and eigensolvers.\n\n\nLightweight interactive OpenGL visualization with the MFEM-based \nGLVis\n tool.\n\n\n\n\nSome of the \nnew additions in version 3.3\n are:\n\n\n\n\nComprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the \nPETSc\n and \nSUNDIALS\n suites.\n\n\nLinear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.\n\n\nGeneral quadrature and nodal finite element basis types.\n\n\nScalable parallel mesh format.\n\n\nThirty six new integrators for common families of operators.\n\n\nSixteen new serial and parallel example codes.\n\n\nSupport for CMake, on-the-fly compression of file streams, and HDF5-based output following the \nConduit\n mesh blueprint specification.\n\n\n\n\nMFEM is being developed in \nCASC\n, \nLLNL\n and is freely available under LGPL 2.1.\nFor more details, see the \ninteractive documentation\n and the full \nCHANGELOG\n.\n\n\n\n\nCEED co-design center announced\n\n\nThe \nExascale Computing Project (ECP)\n announced on November 11, 2016 its selection\nof four \nco-design centers\n, including CEED: the Center for\nEfficient Exascale Discretizations, which is a \nresearch partnership\n between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).\n\n\nAdditional news coverage can be found in \nLLNL Newsline\n\nand the \nANL press release\n.\n\n\n\n\nR\nD 100 Award for NekCEM / Nek5000\n\n\nNekCEM/Nek5000: Scalable High-Order Simulation Codes received a 2016 R\nD 100\nAward, given by R\nD Magazine to 100 top new technologies for the year.\n\n\nThe \nR\nD 100 citation\n\nreads:\n\n\"NekCEM/Nek5000: Release 4.0: Scalable High-Order Simulation Codes is an\nopen-source simulation-software package that delivers highly accurate solutions\nfor a wide range of scientific applications including electromagnetics, quantum\noptics, fluid flow, thermal convection, combustion and magnetohydrodynamics. It\nfeatures state-of-the-art, scalable, high-order algorithms that are fast and\nefficient on platforms ranging from laptops to the world\u2019s fastest\ncomputers. The size of the physical phenomena that can be simulated with this\npackage ranges from quantum dots for nanoscale devices to accretion disks\nsurrounding black holes. NekCEM provides simulation capabilities for the\nanalysis of electromagnetic and quantum optical devices, such as particle\naccelerators and solar cells. Nek5000 provides turbulent flow simulation\ncapabilities for a variety of thermal-fluid problems including nuclear reactors,\ninternal combustion engines, vascular flows, and ocean currents.\"\n\n\nSee the \nANL press release\n\nfor more information.", 
            "title": "News"
        }, 
        {
            "location": "/news/#news", 
            "text": "", 
            "title": "News"
        }, 
        {
            "location": "/news/#new-website-launched-by-the-virginia-tech-ceed-team", 
            "text": "A new website was recently launched by the Parallel Numerical Algorithms\nresearch group at Virginia Tech  here . The\nsite includes a blog that gives some practical computing tips related to\nhigh performance implementations of finite element\nmethods developed as part of the CEED project  here .", 
            "title": "New website launched by the Virginia Tech CEED team"
        }, 
        {
            "location": "/news/#initial-release-of-libceed-the-ceed-api-library", 
            "text": "The initial version of  libCEED ,\nthe CEED API library, was released in December 2017.  libCEED is a high-order API library, that for the first time provides a common\noperator description on  algebraic level ,\nthat allows a wide variety of applications to take advantage of the efficient operator\nevaluation algorithms in the different CEED packages (from a single source).  Our long-term vision for libCEED is to include a variety of back-end\nimplementations, ranging from simple reference kernels, to highly optimized\nkernels targeting specific devices (e.g. GPUs) or specific polynomial orders.  For more information visit https://github.com/CEED/libCEED .", 
            "title": "Initial release of libCEED: The CEED API library"
        }, 
        {
            "location": "/news/#software-release-mfem-v332", 
            "text": "Version 3.3.2 of  MFEM  was released on November 10, 2017. Some of the\nnew additions in this release are:   Support for high-order mesh optimization based on the target-matrix\n  optimization paradigm from the  ETHOS project .  Implementation of the community policies in xSDK, the  Extreme-scale\n  Scientific Software Development Kit .  Integration with the\n   STRUMPACK  parallel sparse\n  direct solver and preconditioner.  Several new  linear interpolators , five new\n  examples and miniapps.  Various memory, performance, discretization and solver improvements, including\n  physical-to-reference space mapping capabilities.  Continuous integration testing on Linux, Mac and Windows.   For more details, see the  interactive documentation  and the\nfull  CHANGELOG  at http://mfem.org .", 
            "title": "Software release: MFEM v3.3.2"
        }, 
        {
            "location": "/news/#ceed-participates-in-xsdk-and-fastmath", 
            "text": "MFEM joined xSDK, the  Extreme-scale Scientific Software Development\nKit  in ECP's software technologies focus area as of release\nxSDK-0.3.0, see  https://xsdk.info/packages .  MFEM  and  PUMI  are also part of the  FASTMath \ninstitute in the SciDAC program, see  https://fastmath-scidac.llnl.gov/software-catalog.html .", 
            "title": "CEED participates in xSDK and FASTMath"
        }, 
        {
            "location": "/news/#software-release-nek5000-v170", 
            "text": "Nek5000  version 17.0 was released as a major upgrade to Nek5000.\nMajor features improvements include:   Refactored build system.  New user-input parameter file format ( .par  replacing  .rea ).  Characteristics (large time-step) support for moving mesh problems.  Moving mesh support for the $PN-PN$ formulation.  Improved stability for $PN-PN$ with variable viscosity.  Support for mixed  Helmholtz/CVODE  solves.  New fast  AMG setup  tool based on HYPRE.  New  EXODUSII  mesh converter.  New interface to  libxsmm  (fast MATMUL library).  Extended  lowMach  solver for time varying thermodynamic pressure.  Added DG for scalars.  Reduced solver initialization time (parallel binary reader for all input files).  Automatic general mesh-to-mesh transfer for restarts.  Refactored support for overlapping domains (NekNek).  Added high-pass filter relaxation (alternative to explicit filter).  Refactored residual projection including support for coupled Helmholtz solves.", 
            "title": "Software release: Nek5000 v17.0"
        }, 
        {
            "location": "/news/#nekbone-and-laghos-join-proxy-app-suites", 
            "text": "The Nekbone and Laghos  miniapps  developed in CEED were selected to be part of ECP's\ninitial  Proxy Applications Suite .  Both miniapps were also picked to be  CORAL-2 benchmarks .  Laghos was also selected as one of LLNL's  ASC co-design miniapps .", 
            "title": "Nekbone and Laghos join proxy app suites"
        }, 
        {
            "location": "/news/#6th-nek5000-user-meeting-to-be-held-at-u-florida", 
            "text": "The 6th Nek5000 User/Developer Meeting will be hosted by the DOE PSAAP-II\nCompressible Multiphase Turbulence (CMT) center in Tampa, FL, March 17-18, 2018.", 
            "title": "6th Nek5000 User Meeting to be held at U Florida"
        }, 
        {
            "location": "/news/#nek5000-hackathon-at-uiuc", 
            "text": "The inaugural Nek5000 Hackathon was held at NCSA Building, University of\nIllinois, Urbana-Champaign (UIUC), IL on Nov 12-14, 2017.  The event was attended by researchers and Nek5000 developers to promote the\napplication of Nek5000 to new problems from industry, national laboratories, and\nacademia.  Twenty-five participants spent three days working on setting up new\nexamples, developing new features, and helping one another to get maximum\nperformance on their applications. Some of the more prominent exchanges of ideas\nincluded standardization of synthetic turbulent inflow techniques, use of CVODE\nfor pure advection-diffusion problems, and the use of the characteristics\nmethods for moving geometry applications.  For more details, see the  Nek5000 hackathon website .", 
            "title": "Nek5000 hackathon at UIUC"
        }, 
        {
            "location": "/news/#ceed-organizing-minisymposium-at-icosahom-2018", 
            "text": "CEED is organizing a minisymposium,  Efficient High-Order Finite Element\nDiscretizations at Large Scale , at the  International Conference on Spectral and\nHigh-Order Methods  (ICOSAHOM 2018) in London UK,\nJul 9-13, 2018.  The goal of the minisymposium is to discuss the next-generation high-order\ndiscretization algorithm and software, based on finite/spectral element\napproaches that will enable a wide range of important scientific applications to\nrun efficiently on future architecture.", 
            "title": "CEED organizing minisymposium at ICOSAHOM 2018"
        }, 
        {
            "location": "/news/#best-paper-award-at-nureth-17", 
            "text": "CEED researchers (P. Fischer, E. Merzari, A. Obabko) won a Best Paper Award at\nthe  17th International Topical Meeting on Nuclear Reactor Thermal\nHydraulics  (NURETH-17), held in China in September\n2017, with a paper entitled  High-Fidelity Simulation of Flow Induced\nVibrations in Helical Steam Generators for Small Modular Reactors .", 
            "title": "Best Paper Award at NURETH-17"
        }, 
        {
            "location": "/news/#ceed-attending-cray-amd-and-intel-deep-dives", 
            "text": "CEED researchers and representatives of the  Nek  and  MFEM \nteams will attend the October 2017, ECP vendor deep-dive meetings:   Cray deep-dive in Bloomington, MN on Oct 18-19  AMD deep-dive in Austin, TX on Oct 24-25  Intel deep-dive in Hudson, MA on Oct 21-Nov 2   Topics of discussion include advanced technology and memory design, strong\nscaling considerations and the porting and evaluation of CEED's  bake-off\nproblems  and  miniapps  (Nekbone, Laghos, NekCEM\nCEEDling).", 
            "title": "CEED attending Cray, AMD and Intel Deep-Dives"
        }, 
        {
            "location": "/news/#new-nekbench-repository", 
            "text": "New  Nekbench repository  has been\nreleased to provide scripts that simplify the benchmarking of  Nek5000 .  The user provides ranges for important parameters ranges (e.g., processor counts\nand local problem size ranges) and a test type (e.g., scaling or ping-pong\ntest). Nekbench will run the given test in the given parameter space using a\nNek5000 case file which is also given by the user (in the ping-pong tests, the\ncase file is optional).  Nekbench is written using bash scripting language and runs any Unix-like\noperating system that supports bash. It has been successfully tested on Linux\nlaptops/desktops, ALCF Theta, NERSC Cori (KNL and Haswell), and NERSC Edison\nmachines for scaling tests.  Planned extensions for Nekbench include adding more machine types like ANL's\nCetus, additional support for the ping-pong test type, and automated plot\ngeneration (e.g., scaling study graphs) for each test run.", 
            "title": "New Nekbench repository"
        }, 
        {
            "location": "/news/#gpu-ports-of-nek-and-laghos", 
            "text": "GPU acceleration is a main focus of the  performance optimization \nefforts in CEED. Recent progress in this direction include GPU ports of CEED's Nek5000  application and the Nekbone and Laghos  miniapps .  For Nek5000, an initial  GPU-enabled version \nhas been developed based on OpenACC. For Nekbone a pure OpenACC implementation as well as a hybrid\nOpenACC/CUDA implementation with a CUDA kernel for matrix-vector multiplication has been developed .  For Laghos, a  GPU-enabled version of  has been\nreleased using the  OCCA  interface. With this approach, the user is able to run Laghos\ndistributively using varying device types per MPI process, whether serial C++, OpenMP, or CUDA.", 
            "title": "GPU ports of Nek and Laghos"
        }, 
        {
            "location": "/news/#first-ceed-annual-meeting-held-at-llnl", 
            "text": "CEED held its first annual meeting in August, 2017 at the  HPC Innovation\nCenter  of\nLawrence Livermore National Laboratory.  The goal of the meeting was to report on the progress in the center, deepen\nexisting and establish new connections with ECP hardware vendors, ECP software\ntechnologies projects and other collaborators, plan project activities and\nbrainstorm/work as a group to make technical progress.  In addition to gathering together many of the CEED researchers, the meeting\nincluded representatives of the ECP management, hardware vendors, software\ntechnology and other interested projects.", 
            "title": "First CEED annual meeting held at LLNL"
        }, 
        {
            "location": "/news/#ceed-researchers-at-atpesc17", 
            "text": "Six CEED researchers presented at the 2017 edition of the  Argonne Training Program\non Extreme-Scale Computing ,\nnow part of the Exascale Computing Project.  The CEED presentations covered a wide variety of topics, from overview of Theta,\nto GPU programming, dense and sparse linear algebra, and high-order discretizations\non unstructured meshes.        Videos of all 2017 talks are available on YouTube .\nCEED researchers have also participated in past editions of the meeting.", 
            "title": "CEED researchers at ATPESC17"
        }, 
        {
            "location": "/news/#ceed-bps-and-benchmarks-repository-released", 
            "text": "CEED released an initial set of  bake-off (BP) problems , which are simple\nkernels designed to test and compare the performance of high-order codes, both\ninternally in CEED, as well as in the  broader high-order\ncommunity .  In addition to the benchmark descriptions on the  CEED BPs page , a benchmarks repository  is publicly\navailable with several implementations of the CEED bake-off problems.\nCurrently, MFEM, Nek5000 and deal.ii are included, see directories tests/mfem_bps , tests/nek5000_bps \nand tests/dealii_bps \nrespectively.", 
            "title": "CEED BPs and benchmarks repository released"
        }, 
        {
            "location": "/news/#new-laghos-and-nekcem-ceedling-miniapps-released", 
            "text": "Two new miniapps developed in CEED were released in June 2017: Laghos and NekCEM\nCEEDling.  Laghos (LAGrangian High-Order Solver) is a new miniapp developed in CEED that\nsolves the time-dependent Euler equations of compressible gas dynamics in a\nmoving Lagrangian frame using unstructured high-order finite element spatial\ndiscretization and explicit high-order time-stepping. In CEED, Laghos serves as\na proxy for a sub-component of the MARBL/LLNLApp application.  NekCEM CEEDling is a new NekCEM miniapp, solving the time-domain Maxwell\nequation for electromagnetic systems.  For more details, see the CEED  miniapps page  and the Laghos  and  NekCEM\nCEEDling  repositories on GitHub.", 
            "title": "New Laghos and NekCEM CEEDling miniapps released"
        }, 
        {
            "location": "/news/#paper-with-mpich-at-sc17", 
            "text": "Joint paper with the MPICH group,  Why is MPI so Slow? Analyzing the fundamental limits in implementing MPI-3.1   accepted in Supercomputing 2017 .\nThe paper provides an in-depth analysis of the software overheads in the MPI\nperformance-critical path and exposes mandatory performance overheads that are\nunavoidable based on the MPI-3.1 specification.", 
            "title": "Paper with MPICH at SC17"
        }, 
        {
            "location": "/news/#strumpack-support-in-mfem", 
            "text": "Support for the sparse direct solver and preconditioner STRUMPACK  has been\nintegrated  in MFEM.  STRUMPACK  is being\ndeveloped at LBNL and is part of the ECP project  Factorization Based Sparse\nSolvers and Preconditioners  (Xiaoye Sherry Li and Pieter Ghysels). The\nSTRUMPACK solver is based on multifrontal sparse Gaussian elimination and uses\nhierarchically semi-separable matrices to compress fill-in. It can be used as an\nexact direct solver or as an algebraic, robust and parallel preconditioner for a\nrange of discretized PDE problems.", 
            "title": "STRUMPACK support in MFEM"
        }, 
        {
            "location": "/news/#2017-petsc-user-meeting", 
            "text": "Over 75 participants from all over the world attended the PETSc User Meeting,\nheld June 14-16 in Boulder, CO. Hosted by the University of Colorado Boulder, the\nevent consisted of a one-day tutorial on the solver library PETSc and showcased\nthe latest research enabled by the functionality available in PETSc. The meeting agenda  covered\na total of 15 talks, four posters, and two panels.  Thanks to generous support from Intel and Tech-X, 22 students received travel\ngrants and got to learn about the latest techniques on the large-scale numerical\nsolution of partial differential equations.  PETSc is a suite of data structures and routines for the scalable (parallel)\nsolution of scientific applications modeled by partial differential\nequations. It has become one of the most widely used numerical software packages\nof its kind and has users in application areas ranging from acoustics and\narterial flow to seismology and semiconductors.", 
            "title": "2017 PETSc User Meeting"
        }, 
        {
            "location": "/news/#gpu-hackathon-at-bnl", 
            "text": "Nek/CEED team participated the  GPU Hackathon 2017 \nthat was held in Brookhaven National Laboratory on June 5-9, 2017.\nOur team focused on performing and tuning GPU-enabled  Nek5000/Nekbone/NekCEM \nversion on large-scale GPU systems for small modular reactor, thermal fluids,\nand meta-materials modeling.", 
            "title": "GPU Hackathon at BNL"
        }, 
        {
            "location": "/news/#workshop-on-batched-reproducible-and-reduced-precision-blas", 
            "text": "The second  Workshop on Batched, Reproducible, and Reduced Precision BLAS \nwas held in Atlanta, GA on February 23-25, 2017 including many members of the CEED  MAGMA  team.  The goal of this workshop was to touch on extending the Basic Linear Algebra\nSoftware Library (BLAS).  The existing BLAS have proven to be very effective in\nassisting portable, efficient software for sequential and some of the current\nclass of high-performance computers. New computational needs in many\napplications have motivated the need to investigate the possibility of extending\nthe currently accepted standards to provide greater parallelism for small size\noperations, reproducibility, and reduced precision support.  Of particular interest to CEED is the use of batched BLAS for finite element\ntensor contractions, and thus our team is interested in the establishment of a\nbatched BLAS standard, highly-optimized implementations, and support from\nvendors on various architectures.  This is the second workshop of an open forum to discuss and formalize details\nrelated to batched, reproducible, and reduced precision BLAS. The agenda and the\ntalks from the first workshop can be found  here .", 
            "title": "Workshop on Batched, Reproducible, and Reduced Precision BLAS"
        }, 
        {
            "location": "/news/#software-release-mfem-v33", 
            "text": "Version 3.3 of MFEM, a lightweight, general, scalable C++ library for finite element\nmethods and a main partner in CEED, was released on January 28, 2017 at  http://mfem.org  The goal of MFEM is to enable high-performance scalable finite element\ndiscretization research and application development on a wide variety of\nplatforms, ranging from laptops to exascale supercomputers.  It has many features, including:   2D and 3D, arbitrary order H1, H(curl), H(div), L2, NURBS elements.  Parallel version scalable to hundreds of thousands of MPI cores.  Conforming/nonconforming adaptive mesh refinement (AMR), including anisotropic refinement, derefinement and parallel load balancing.  Galerkin, mixed, isogeometric, discontinuous Galerkin, hybridized, and DPG discretizations.  Support for triangular, quadrilateral, tetrahedral and hexahedral elements, including arbitrary order curvilinear meshes.  Scalable algebraic multigrid, time integrators, and eigensolvers.  Lightweight interactive OpenGL visualization with the MFEM-based  GLVis  tool.   Some of the  new additions in version 3.3  are:   Comprehensive support for the linear and nonlinear solvers, preconditioners, time integrators and other features\n  from the  PETSc  and  SUNDIALS  suites.  Linear system interface for action-only linear operators including support for matrix-free preconditioning and low-order-refined spaces.  General quadrature and nodal finite element basis types.  Scalable parallel mesh format.  Thirty six new integrators for common families of operators.  Sixteen new serial and parallel example codes.  Support for CMake, on-the-fly compression of file streams, and HDF5-based output following the  Conduit  mesh blueprint specification.   MFEM is being developed in  CASC ,  LLNL  and is freely available under LGPL 2.1.\nFor more details, see the  interactive documentation  and the full  CHANGELOG .", 
            "title": "Software release: MFEM v3.3"
        }, 
        {
            "location": "/news/#ceed-co-design-center-announced", 
            "text": "The  Exascale Computing Project (ECP)  announced on November 11, 2016 its selection\nof four  co-design centers , including CEED: the Center for\nEfficient Exascale Discretizations, which is a  research partnership  between Lawrence Livermore National Laboratory;\nArgonne National Laboratory; the University of Illinois Urbana-Champaign; Virginia Tech; University of Tennessee,\nKnoxville; Colorado University, Boulder; and the Rensselaer Polytechnic Institute (RPI).  Additional news coverage can be found in  LLNL Newsline \nand the  ANL press release .", 
            "title": "CEED co-design center announced"
        }, 
        {
            "location": "/news/#rd-100-award-for-nekcem-nek5000", 
            "text": "NekCEM/Nek5000: Scalable High-Order Simulation Codes received a 2016 R D 100\nAward, given by R D Magazine to 100 top new technologies for the year.  The  R D 100 citation \nreads: \"NekCEM/Nek5000: Release 4.0: Scalable High-Order Simulation Codes is an\nopen-source simulation-software package that delivers highly accurate solutions\nfor a wide range of scientific applications including electromagnetics, quantum\noptics, fluid flow, thermal convection, combustion and magnetohydrodynamics. It\nfeatures state-of-the-art, scalable, high-order algorithms that are fast and\nefficient on platforms ranging from laptops to the world\u2019s fastest\ncomputers. The size of the physical phenomena that can be simulated with this\npackage ranges from quantum dots for nanoscale devices to accretion disks\nsurrounding black holes. NekCEM provides simulation capabilities for the\nanalysis of electromagnetic and quantum optical devices, such as particle\naccelerators and solar cells. Nek5000 provides turbulent flow simulation\ncapabilities for a variety of thermal-fluid problems including nuclear reactors,\ninternal combustion engines, vascular flows, and ocean currents.\"  See the  ANL press release \nfor more information.", 
            "title": "R&amp;D 100 Award for NekCEM / Nek5000"
        }, 
        {
            "location": "/codesign/", 
            "text": "Discretization Co-Design Approach\n\n\nCEED's co-design approach is based on close collaboration between its\n\nApplications\n, \nHardware\n, and \nSoftware\n thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's \nFinite Elements\n\nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.\n\n\nFor example, the development of \ndiscretization libraries\n in CEED\nis led by the \nFinite Elements\n thrust but involves working closely\nwith vendors (\nHardware\n thrust) and software technology efforts\n(\nSoftware\n thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of, and are successfully incorporated in,\nECP applications is based on collaboration between the \nApplications\n and\n\nFinite Elements\n thrusts.\n\n\nIn addition to libraries of highly performant kernels, a key product of the CEED\nproject will be a set of \nminiapps\n that will serve multiple roles:\n\n\n\n\nProvide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.\n\n\nServe as stand-alone unit-test drivers for the library kernels.\n\n\nProvide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).\n\n\nProvide test and demonstration cases for application scientists who are\nconsidering new formulations.\n\n\n\n\nThe \nminiapps\n developed in CEED, which we also refer to as\n\nCEEDlings\n, will range from local, element-level \nkernels\n, which can be run in\na simulator, to \nbake-off problems\n, which combine local and global\nkernels into model problem benchmarks for high-order computations, to \nproxy\napps\n, which will include application-relevant physics.\n\n\nThese encapsulated \nCEEDlings\n will be used in interactions with vendors\non \nemergent HPC technologies\n\nand \nECP software technologies\n\nprojects to highlight performance critical paths (e.g. size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of \nexascale architectures\n,\nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Approach"
        }, 
        {
            "location": "/codesign/#discretization-co-design-approach", 
            "text": "CEED's co-design approach is based on close collaboration between its Applications ,  Hardware , and  Software  thrusts, each of\nwhich has a two-way, push-and-pull relation with the external application,\nhardware and software technologies teams. CEED's  Finite Elements \nthrust serves as a central hub that ties together, coordinates and contributes\nto the efforts in all thrusts.  For example, the development of  discretization libraries  in CEED\nis led by the  Finite Elements  thrust but involves working closely\nwith vendors ( Hardware  thrust) and software technology efforts\n( Software  thrust) to take full advantage of exascale hardware. Making\nsure that these libraries meet the needs of, and are successfully incorporated in,\nECP applications is based on collaboration between the  Applications  and Finite Elements  thrusts.  In addition to libraries of highly performant kernels, a key product of the CEED\nproject will be a set of  miniapps  that will serve multiple roles:   Provide a mechanism to test and optimize across the breadth of\nimplementations already developed by team members for a variety of platforms.  Serve as stand-alone unit-test drivers for the library kernels.  Provide well-documented (implementation, usage, and performance) benchmarks\nto work with vendors, now and in the future (e.g. in system procurement).  Provide test and demonstration cases for application scientists who are\nconsidering new formulations.   The  miniapps  developed in CEED, which we also refer to as CEEDlings , will range from local, element-level  kernels , which can be run in\na simulator, to  bake-off problems , which combine local and global\nkernels into model problem benchmarks for high-order computations, to  proxy\napps , which will include application-relevant physics.  These encapsulated  CEEDlings  will be used in interactions with vendors\non  emergent HPC technologies \nand  ECP software technologies \nprojects to highlight performance critical paths (e.g. size of on package memory,\ninternode latency, hardware collectives) and provide simple examples of meaningful\nhigh-order computations.  One of the goals in these interactions will be to\nimpact the design of  exascale architectures ,\nand system and application software, for improved portability and performance of\nthe high-order algorithms.", 
            "title": "Discretization Co-Design Approach"
        }, 
        {
            "location": "/ap/", 
            "text": "Applications Thrust \n\n\nThe goal of CEED's Applications (AP) thrust, led by \nMisun Min\n\nfrom \nArgonne National Laboratory\n, is to impact a wide range of\n\nECP application teams\n through focused one-on-one interactions,\nfacilitated by CEED application liaisons, as well as through one-to-many interactions, based on\nthe development of easy-to-use discretization libraries for high-order finite element methods.\n\n\nSome of our ECP application targets are:\n\n\n\n\nCoupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL)\n\n\nMulti-physics Simulation Code (LLNL)\n\n\nMultiscale Coupled Urban System (ANL)\n\n\nTransforming Combustion Science and Technology with Exascale Simulations (SNL)\n\n\nCloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL)\n\n\n\n\nIn addition to maintaining a close connection with these high-order ECP\napplications, the AP thrust is also reaching out to low-order and non-ECP\napparitions and using its interactions to derive requirements for CEED\u2019s\n\nminiapps\n and \nsoftware technologies\n work.\n\n\nThe AP thrust is actively involved with a variety of \nco-design\n\nactivities both within ECP applications and CEED miniapps, such as: OpenACC- and\nCUDA-based GPU implementations, I/O performance improvement for large meshes,\nexploration of lightweight MPI and neighborhood collective MPI, and many more.", 
            "title": "Applications"
        }, 
        {
            "location": "/ap/#applications-thrust", 
            "text": "The goal of CEED's Applications (AP) thrust, led by  Misun Min \nfrom  Argonne National Laboratory , is to impact a wide range of ECP application teams  through focused one-on-one interactions,\nfacilitated by CEED application liaisons, as well as through one-to-many interactions, based on\nthe development of easy-to-use discretization libraries for high-order finite element methods.  Some of our ECP application targets are:   Coupled Monte Carlo Neutronics and Fluid Flow Simulation of Small Modular Reactors (ORNL)  Multi-physics Simulation Code (LLNL)  Multiscale Coupled Urban System (ANL)  Transforming Combustion Science and Technology with Exascale Simulations (SNL)  Cloud-Resolving Climate Modeling of the Earth's Water Cycle (SNL)   In addition to maintaining a close connection with these high-order ECP\napplications, the AP thrust is also reaching out to low-order and non-ECP\napparitions and using its interactions to derive requirements for CEED\u2019s miniapps  and  software technologies  work.  The AP thrust is actively involved with a variety of  co-design \nactivities both within ECP applications and CEED miniapps, such as: OpenACC- and\nCUDA-based GPU implementations, I/O performance improvement for large meshes,\nexploration of lightweight MPI and neighborhood collective MPI, and many more.", 
            "title": "Applications Thrust "
        }, 
        {
            "location": "/hw/", 
            "text": "Hardware Thrust \n\n\nThe goal of CEED's Hardware (HW) thrust, led by \nJack Dongarra\n\nfrom \nThe University of Tennessee, Knoxville\n, is to build a two-way\n(\npull-and-push\n) collaboration with vendors, where the CEED team will develop\nhardware-aware technologies (\npull\n) to understand performance bottlenecks and\ntake advantage of inevitable hardware trends, and vendor interactions to seek\n(\npush\n) impact and improve hardware designs within the ECP scope.\n\n\nIn addition to maintaining a close connection with ECP vendors, the HW thrust is\na connection point for the MPI, OpenMP, and compiler-related work in the ECP.\nMembers of the HW team are also actively involved with the \nbatched\nBLAS\n standardization efforts.\n\n\nThe HW thrust participates in a variety of \nco-design\n\nactivities, both with ECP hardware vendors and within CEED miniapps, to explore\noptimal data locality and motion and to enhance the scalability and parallelism of\nhigh-order algorithms.", 
            "title": "Hardware"
        }, 
        {
            "location": "/hw/#hardware-thrust", 
            "text": "The goal of CEED's Hardware (HW) thrust, led by  Jack Dongarra \nfrom  The University of Tennessee, Knoxville , is to build a two-way\n( pull-and-push ) collaboration with vendors, where the CEED team will develop\nhardware-aware technologies ( pull ) to understand performance bottlenecks and\ntake advantage of inevitable hardware trends, and vendor interactions to seek\n( push ) impact and improve hardware designs within the ECP scope.  In addition to maintaining a close connection with ECP vendors, the HW thrust is\na connection point for the MPI, OpenMP, and compiler-related work in the ECP.\nMembers of the HW team are also actively involved with the  batched\nBLAS  standardization efforts.  The HW thrust participates in a variety of  co-design \nactivities, both with ECP hardware vendors and within CEED miniapps, to explore\noptimal data locality and motion and to enhance the scalability and parallelism of\nhigh-order algorithms.", 
            "title": "Hardware Thrust "
        }, 
        {
            "location": "/sw/", 
            "text": "Software Thrust \n\n\nThe goal of CEED's Software (SW) thrust, led by \nJed Brown\n from \nUniversity of Colorado Boulder\n, is\nto participate in the development of software libraries and frameworks of\ngeneral interest to the scientific computing community, facilitate collaboration\nbetween CEED software packages, enable integration into and/or interoperability\nwith overall ECP software technologies stack, streamline developer and user\nworkflows, maintain testing and benchmarking infrastructure, and coordinate CEED\nsoftware releases.\n\n\nIn addition to maintaining a close connection with ECP software technologies\nprojects, the SW thrust develops continuous integration and performance\nregression testing for CEED, helps with the benchmarking suite and implements\nsupport for package managers, such as\n\nSpack\n.\nMembers of the SW team are also\nactively involved with the \nmatrix-free solvers\n work and the\nefforts in \ngeneral interpolation\n and \nvisualization of high-order\nmeshes and functions\n.\n\n\nThe SW thrust participates in a variety of \nco-design\n activities\nsuch as the coordination of the design of CEED's APIs and the identification of\ncommon kernels and their regimes of relevance relative to the parent\napplication.", 
            "title": "Software"
        }, 
        {
            "location": "/sw/#software-thrust", 
            "text": "The goal of CEED's Software (SW) thrust, led by  Jed Brown  from  University of Colorado Boulder , is\nto participate in the development of software libraries and frameworks of\ngeneral interest to the scientific computing community, facilitate collaboration\nbetween CEED software packages, enable integration into and/or interoperability\nwith overall ECP software technologies stack, streamline developer and user\nworkflows, maintain testing and benchmarking infrastructure, and coordinate CEED\nsoftware releases.  In addition to maintaining a close connection with ECP software technologies\nprojects, the SW thrust develops continuous integration and performance\nregression testing for CEED, helps with the benchmarking suite and implements\nsupport for package managers, such as Spack .\nMembers of the SW team are also\nactively involved with the  matrix-free solvers  work and the\nefforts in  general interpolation  and  visualization of high-order\nmeshes and functions .  The SW thrust participates in a variety of  co-design  activities\nsuch as the coordination of the design of CEED's APIs and the identification of\ncommon kernels and their regimes of relevance relative to the parent\napplication.", 
            "title": "Software Thrust "
        }, 
        {
            "location": "/bps/", 
            "text": "CEED Bake-off Problems (Benchmarks)\n\n\nThis page contains the specifications of CEED's \nbake-off problems\n: high-order\nkernels/benchmarks designed to test and compare the performance of high-order\ncodes.\n\n\nShort Summary\n\n\nThis section is a just a quick reference using the notation from the\n\nTerminology and Notation\n section below (cf.\nlibCEED \ndocumentation\n).\n\n\nFor more details and software, see the follow-on sections.\n\n\nBake-off Problems (BPs)\n\n\n\n\nBP1\n: scalar PCG with mass matrix, $q = p+2$\n\n\nBP2\n: vector PCG with mass matrix, $q = p+2$\n\n\nBP3\n: scalar PCG with stiffness matrix, $q = p+2$\n\n\nBP3.5\n: scalar PCG with stiffness matrix, $q = p+1$\n\n\nBP4\n: vector PCG with stiffness matrix, $q = p+2$\n\n\nBP4.5\n: vector PCG with stiffness matrix, $q = p+1$\n\n\n\n\nThese are all \nT-vector\n-to-\nT-vector\n and include parallel scatter +\nelement scatter + element evaluation kernel + element gather + parallel gather.\n\n\nBake-off Kernels (BKs)\n\n\n\n\nBK1\n: scalar \nE-vector\n-to-\nE-vector\n evaluation of mass matrix, $q = p+2$\n\n\nBK2\n: vector \nE-vector\n-to-\nE-vector\n evaluation of mass matrix, $q = p+2$\n\n\nBK3\n: scalar \nE-vector\n-to-\nE-vector\n evaluation of stiffness matrix, $q = p+2$\n\n\nBK3.5\n: scalar \nE-vector\n-to-\nE-vector\n evaluation of stiffness matrix, $q = p+1$\n\n\nBK4\n: vector  \nE-vector\n-to-\nE-vector\n evaluation of stiffness matrix, $q = p+2$\n\n\nBK4.5\n: vector  \nE-vector\n-to-\nE-vector\n evaluation of stiffness matrix, $q = p+1$\n\n\n\n\nThe BKs are parallel to the BPs, except they do not include parallel and element\nscatter/gather (the actions of \nP\n and \nG\n and their transposes).\n\n\nCEED Benchmarks Repository\n\n\nThe CEED benchmarks repository is available at\n\nhttps://github.com/CEED/benchmarks\n\nand contains implementations for the CEED bake-off problems with MFEM and\nNek5000 in the directories \ntests/mfem_bps\n and \ntests/nek5000_bps\n,\nrespectively. See the included \nREADME.md\n files for more details.\n\n\nVT CEED BP Software Release\n\n\nCheckout the new\n\nCEED BP Software Release\n\nby the Virginia Tech CEED team. Its main focus is on GPU performance with\nkernels written in the \nOCCA\n framework.\n\n\nBake-off Problems Description\n\n\nBake-off Problems\n. We define our first four bake-off problems, denoted as\nBP1, BP2, BP3, and BP4.\n\n\n\n\nBP1\n\n  Solve $B {\\underline u} = {\\underline f}$, where $B$ is the mass matrix.\n\n\nBP2\n\n  Solve the (block-diagonal) vector system,\n  $B {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $B$ is as prescribed\n  in BP1.\n\n\nBP3\n\n  Solve $A {\\underline u} = {\\underline f}$, where $A$ is the Poisson operator.\n\n\nBP4\n\n  Solve the (block-diagonal) vector system,\n  $A {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $A$ is as prescribed\n  in BP3.\n\n\n\n\nBake-off Problem Details\n. The following items describe the details common\nto all BPs:\n\n\n\n\nMesh: use a 3D box mesh with hexahedral elements.\n\n\nBoundary conditions (BCs): either no essential BCs, or essential BCs on\n  the whole boundary.\n\n\nSolution space orders: $p=1,2,3,\\ldots,8$, and optionally higher $p$.\n\n\nQuadrature: tensor product Gauss-Legendre (GL) with $q=p+2$ points in each\n  spatial dimension; the quadrature order is $2q-1=2p+3$.\n\n\nThe cases of $q=2$, for $p=1$, and $q=3$, for $p=2$, are of interest as\n  they provide a more favorable ratio of the total number of quadrature\n  points to the total number of unknowns. Note that this ratio is smaller (and\n  therefore, advantageous in terms of work per unknown) for larger $p$.\n\n\nUse nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each\n  spatial dimension.\n\n\nConsider mesh orders of $p_{\\rm mesh}=1$, and/or $p_{\\rm mesh}=p$.\n\n\nElements are assumed to be deformed, meaning that the local\n  element-by-element evaluation cannot exploit simplifications\n  arising from the absence of cross-terms in the Laplacian, etc.\n\n\nUse the QA/PA operator representation, see\n  \nTerminology and Notation\n.\n\n\nMeshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$.\n\n\nFor example:\n\n\nif $s=15$, then $s_1=s_2=s_3=5$\n\n\nif $s=16$, then $s_1=6$ and $s_2=s_3=5$\n\n\nif $s=17$, then $s_1=s_2=6$ and $s_3=5$\n\n\n\n\n\n\nConsider tests with $2^t$ processors, $0\\le t\\le s$, and partition the\n  mesh into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where\n  $\\{t_i\\}$ are derived from $t$ the same way $\\{s_i\\}$ are derived from\n  $s$. Using a partitioning of this type allows us to consider cases with\n  a small number of elements per MPI rank - down to one element/rank.\n\n\nAlternative mesh partitioning algorithms are also acceptable.\n\n\nConsider runs with \"large\" number of processors and vary the number of mesh\n  elements per MPI rank starting from 1 and gradually increasing to a number\n  where performance saturation is observed. This suite of benchmarks thus\n  captures both the strong-scale and weak-scale performance limits under the\n  assumption that the underlying code is scalable.\n\n\nUse the conjugate gradients (CG) iterative method to solve the linear system.\n  Since we are interested in evaluating the performance of the QA/PA operator\n  representation (see \nTerminology and Notation\n),\n  we assume no preconditioning, or simple diagonal preconditioning.\n\n\n\n\nRequired output:\n\n\n\n\nTotal number of MPI ranks and number of MPI ranks per compute node.\n\n\nNumber of mesh elements, $E$.\n\n\nPolynomial degree, $p$.\n\n\nTotal number of degrees of freedom, $n_T$ (size of a\n  \nT-vector\n), or approximately $n:=E p^3$.\n\n\nTime per iteration = total CG time $/$ number of CG iterations.\n\n\nTime is measured as maximum over all MPI ranks; using \nMPI_Wtime()\n or other\n  similar function.\n\n\n[optional]\n Number of iterations to reach relative residual reduction of\n  $10^{-6}$.\n\n\n[optional]\n Time for quadrature-point/partial assembly.\n\n\n\n\nTerminology and Notation\n\n\nVector representation/storage categories:\n\n\n\n\nTrue degrees of freedom/unknowns, \nT-vector\n:\n\n\neach unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$\n\n\nthis is a non-overlapping vector decomposition\n\n\nusually includes any essential (fixed) dofs.\n\n\n\n\n\n\n\nLocal (w.r.t. processors) degrees of freedom/unknowns, \nL-vector\n:\n\n\neach unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$\n\n\nthis is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor\n\n\nthe shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n  \n\n\n\n\n\n\nPer element decomposition, \nE-vector\n:\n\n\neach unknown $i$ has as many copies as the number of elements that contain\n  $i$\n\n\nusually, the copies of the unknowns are grouped by the element they belong\n  to.\n  \n\n\n\n\n\n\nIn the case of AMR with hanging nodes (giving rise to hanging dofs):\n\n\nthe L-vector is enhanced with the hanging/dependent dofs\n\n\nthe additional hanging/dependent dofs are duplicated when they are shared\n  by multiple processors\n\n\nthis way, an E-vector can be derived from an L-vector without any\n  communications and without additional computations to derive the dependend\n  dofs\n\n\nin other words, an entry in an E-vector is obtained by copying an entry\n  from the corresponding L-vector, optionally switching the sign of the\n  entry (for $H(\\mathrm{div})$- and $H(\\mathrm{curl})$-conforming spaces).\n\n\n\n\n\n\n\nIn the case of variable order spaces:\n\n\nthe dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case.\n\n\n\n\n\n\nQuadrature point vector, \nQ-vector\n:\n\n\nthis is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.\n\n\n\n\n\n\nIn many cases it is useful to distinguish two types of vectors:\n\n\nX-vector, or \nprimal\n X-vector, and X'-vector, or \ndual\n X-vector\n\n\nhere X can be any of the T, L, E, or Q categories\n\n\nfor example, the mass matrix operator maps a T-vector to a T'-vector\n\n\nthe solutions vector is a T-vector, and the RHS vector is a T'-vector\n\n\nusing the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.\n\n\n\n\n\n\n\n\nOperator representation/storage/action categories:\n\n\n\n\nFull true-dof parallel assembly, \nTA\n, or \nA\n:\n\n\nParCSR or similar format\n\n\nthe T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.\n\n\n\n\n\n\nFull local assembly, \nLA\n:\n\n\nCSR matrix on each rank\n\n\nthe parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action\n\n\nnote that $P$ is the operator mapping T-vectors to L-vectors.\n\n\n\n\n\n\nElement matrix assembly, \nEA\n:\n\n\neach element matrix is stored as a dense matrix\n\n\noptimized element and parallel prolongation operators\n\n\nnote that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.\n\n\n\n\n\n\nQuadrature-point/partial assembly, \nQA\n or \nPA\n:\n\n\nprecompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements\n\n\nthe stored data can be viewed as a Q-vector.\n\n\n\n\n\n\nUnassembled option,  \nUA\n or \nU\n:\n\n\nno assembly step\n\n\nthe action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).\n\n\n\n\n\n\n\n\nNotes and Remarks\n\n\n\n\nWhat are good partitioning algorithms for the strong scaling limit? The\n  problem is to generate well balanced partitions when the ratio \"number of\n  elements\" $/$ \"number of processors\" is small. METIS 4 does not do well on\n  this type of problems. What about METIS 5 and other graph partitioners? Maybe\n  we need to develop specialized algorithms?\n\n\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Benchmarks"
        }, 
        {
            "location": "/bps/#ceed-bake-off-problems-benchmarks", 
            "text": "This page contains the specifications of CEED's  bake-off problems : high-order\nkernels/benchmarks designed to test and compare the performance of high-order\ncodes.", 
            "title": "CEED Bake-off Problems (Benchmarks)"
        }, 
        {
            "location": "/bps/#short-summary", 
            "text": "This section is a just a quick reference using the notation from the Terminology and Notation  section below (cf.\nlibCEED  documentation ).  For more details and software, see the follow-on sections.", 
            "title": "Short Summary"
        }, 
        {
            "location": "/bps/#bake-off-problems-bps", 
            "text": "BP1 : scalar PCG with mass matrix, $q = p+2$  BP2 : vector PCG with mass matrix, $q = p+2$  BP3 : scalar PCG with stiffness matrix, $q = p+2$  BP3.5 : scalar PCG with stiffness matrix, $q = p+1$  BP4 : vector PCG with stiffness matrix, $q = p+2$  BP4.5 : vector PCG with stiffness matrix, $q = p+1$   These are all  T-vector -to- T-vector  and include parallel scatter +\nelement scatter + element evaluation kernel + element gather + parallel gather.", 
            "title": "Bake-off Problems (BPs)"
        }, 
        {
            "location": "/bps/#bake-off-kernels-bks", 
            "text": "BK1 : scalar  E-vector -to- E-vector  evaluation of mass matrix, $q = p+2$  BK2 : vector  E-vector -to- E-vector  evaluation of mass matrix, $q = p+2$  BK3 : scalar  E-vector -to- E-vector  evaluation of stiffness matrix, $q = p+2$  BK3.5 : scalar  E-vector -to- E-vector  evaluation of stiffness matrix, $q = p+1$  BK4 : vector   E-vector -to- E-vector  evaluation of stiffness matrix, $q = p+2$  BK4.5 : vector   E-vector -to- E-vector  evaluation of stiffness matrix, $q = p+1$   The BKs are parallel to the BPs, except they do not include parallel and element\nscatter/gather (the actions of  P  and  G  and their transposes).", 
            "title": "Bake-off Kernels (BKs)"
        }, 
        {
            "location": "/bps/#ceed-benchmarks-repository", 
            "text": "The CEED benchmarks repository is available at https://github.com/CEED/benchmarks \nand contains implementations for the CEED bake-off problems with MFEM and\nNek5000 in the directories  tests/mfem_bps  and  tests/nek5000_bps ,\nrespectively. See the included  README.md  files for more details.", 
            "title": "CEED Benchmarks Repository"
        }, 
        {
            "location": "/bps/#vt-ceed-bp-software-release", 
            "text": "Checkout the new CEED BP Software Release \nby the Virginia Tech CEED team. Its main focus is on GPU performance with\nkernels written in the  OCCA  framework.", 
            "title": "VT CEED BP Software Release"
        }, 
        {
            "location": "/bps/#bake-off-problems-description", 
            "text": "Bake-off Problems . We define our first four bake-off problems, denoted as\nBP1, BP2, BP3, and BP4.   BP1 \n  Solve $B {\\underline u} = {\\underline f}$, where $B$ is the mass matrix.  BP2 \n  Solve the (block-diagonal) vector system,\n  $B {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $B$ is as prescribed\n  in BP1.  BP3 \n  Solve $A {\\underline u} = {\\underline f}$, where $A$ is the Poisson operator.  BP4 \n  Solve the (block-diagonal) vector system,\n  $A {\\underline u}_i = {\\underline f}_i$ ($i=1,2,3$) where $A$ is as prescribed\n  in BP3.   Bake-off Problem Details . The following items describe the details common\nto all BPs:   Mesh: use a 3D box mesh with hexahedral elements.  Boundary conditions (BCs): either no essential BCs, or essential BCs on\n  the whole boundary.  Solution space orders: $p=1,2,3,\\ldots,8$, and optionally higher $p$.  Quadrature: tensor product Gauss-Legendre (GL) with $q=p+2$ points in each\n  spatial dimension; the quadrature order is $2q-1=2p+3$.  The cases of $q=2$, for $p=1$, and $q=3$, for $p=2$, are of interest as\n  they provide a more favorable ratio of the total number of quadrature\n  points to the total number of unknowns. Note that this ratio is smaller (and\n  therefore, advantageous in terms of work per unknown) for larger $p$.  Use nodal basis with $p+1$ Gauss-Legendre-Lobatto (GLL) points in each\n  spatial dimension.  Consider mesh orders of $p_{\\rm mesh}=1$, and/or $p_{\\rm mesh}=p$.  Elements are assumed to be deformed, meaning that the local\n  element-by-element evaluation cannot exploit simplifications\n  arising from the absence of cross-terms in the Laplacian, etc.  Use the QA/PA operator representation, see\n   Terminology and Notation .  Meshes: consider meshes with $E=2^s$ elements with $s\\in\\mathbb{N}$; for a\n  given $s$, use a 3D Cartesian mesh with\n  $2^{s_1}\\times 2^{s_2}\\times 2^{s_3}$ elements ($s_i\\in\\mathbb{N}$), where\n  $\\{s_i\\}$ are uniquely determined by the conditions: $s_1+s_2+s_3 = s$ and\n  $\\lfloor s/3\\rfloor+1\\ge s_1 \\ge s_2 \\ge s_3 \\ge \\lfloor s/3\\rfloor$.  For example:  if $s=15$, then $s_1=s_2=s_3=5$  if $s=16$, then $s_1=6$ and $s_2=s_3=5$  if $s=17$, then $s_1=s_2=6$ and $s_3=5$    Consider tests with $2^t$ processors, $0\\le t\\le s$, and partition the\n  mesh into $2^{t_1}\\times 2^{t_2}\\times 2^{t_3}$ uniform parts, where\n  $\\{t_i\\}$ are derived from $t$ the same way $\\{s_i\\}$ are derived from\n  $s$. Using a partitioning of this type allows us to consider cases with\n  a small number of elements per MPI rank - down to one element/rank.  Alternative mesh partitioning algorithms are also acceptable.  Consider runs with \"large\" number of processors and vary the number of mesh\n  elements per MPI rank starting from 1 and gradually increasing to a number\n  where performance saturation is observed. This suite of benchmarks thus\n  captures both the strong-scale and weak-scale performance limits under the\n  assumption that the underlying code is scalable.  Use the conjugate gradients (CG) iterative method to solve the linear system.\n  Since we are interested in evaluating the performance of the QA/PA operator\n  representation (see  Terminology and Notation ),\n  we assume no preconditioning, or simple diagonal preconditioning.   Required output:   Total number of MPI ranks and number of MPI ranks per compute node.  Number of mesh elements, $E$.  Polynomial degree, $p$.  Total number of degrees of freedom, $n_T$ (size of a\n   T-vector ), or approximately $n:=E p^3$.  Time per iteration = total CG time $/$ number of CG iterations.  Time is measured as maximum over all MPI ranks; using  MPI_Wtime()  or other\n  similar function.  [optional]  Number of iterations to reach relative residual reduction of\n  $10^{-6}$.  [optional]  Time for quadrature-point/partial assembly.", 
            "title": "Bake-off Problems Description"
        }, 
        {
            "location": "/bps/#terminology-and-notation", 
            "text": "Vector representation/storage categories:   True degrees of freedom/unknowns,  T-vector :  each unknown $i$ has exactly one copy, on exactly one processor, $rank(i)$  this is a non-overlapping vector decomposition  usually includes any essential (fixed) dofs.    Local (w.r.t. processors) degrees of freedom/unknowns,  L-vector :  each unknown $i$ has exactly one copy on each processor that owns an\n  element containing $i$  this is an overlapping vector decomposition with overlaps only across\n  different processors - there is no duplication of unknowns on a single\n  processor  the shared dofs/unknowns are the overlapping dofs, i.e. the ones that have\n  more than one copy, on different processors.\n      Per element decomposition,  E-vector :  each unknown $i$ has as many copies as the number of elements that contain\n  $i$  usually, the copies of the unknowns are grouped by the element they belong\n  to.\n      In the case of AMR with hanging nodes (giving rise to hanging dofs):  the L-vector is enhanced with the hanging/dependent dofs  the additional hanging/dependent dofs are duplicated when they are shared\n  by multiple processors  this way, an E-vector can be derived from an L-vector without any\n  communications and without additional computations to derive the dependend\n  dofs  in other words, an entry in an E-vector is obtained by copying an entry\n  from the corresponding L-vector, optionally switching the sign of the\n  entry (for $H(\\mathrm{div})$- and $H(\\mathrm{curl})$-conforming spaces).    In the case of variable order spaces:  the dependent dofs (usually on the higher-order side of a face/edge) can\n  be treated just like the hanging/dependent dofs case.    Quadrature point vector,  Q-vector :  this is similar to E-vector where instead of dofs, the vector represents\n  values at qudrature points, grouped by element.    In many cases it is useful to distinguish two types of vectors:  X-vector, or  primal  X-vector, and X'-vector, or  dual  X-vector  here X can be any of the T, L, E, or Q categories  for example, the mass matrix operator maps a T-vector to a T'-vector  the solutions vector is a T-vector, and the RHS vector is a T'-vector  using the parallel prolongation operator, one can map the solution\n  T-vector to a solution L-vector, etc.     Operator representation/storage/action categories:   Full true-dof parallel assembly,  TA , or  A :  ParCSR or similar format  the T in TA indicates that the data format represents an operator from a\n  T-vector to a T'-vector.    Full local assembly,  LA :  CSR matrix on each rank  the parallel prolongation operator, $P$, (and its transpose) should use\n  optimized matrix-free action  note that $P$ is the operator mapping T-vectors to L-vectors.    Element matrix assembly,  EA :  each element matrix is stored as a dense matrix  optimized element and parallel prolongation operators  note that the element prolongation operator is the mapping from an\n  L-vector to an E-vector.    Quadrature-point/partial assembly,  QA  or  PA :  precompute and store $w\\det(J)$, or $\\det(J)$ (in the case of mass matrix)\n  at all quadrature points in all mesh elements  the stored data can be viewed as a Q-vector.    Unassembled option,   UA  or  U :  no assembly step  the action uses directly the mesh node coordinates, and assumes specific\n  form of the coefficient, e.g. constant, piecewise-constant, or given as a\n  Q-vector (Q-coefficient).", 
            "title": "Terminology and Notation"
        }, 
        {
            "location": "/bps/#notes-and-remarks", 
            "text": "What are good partitioning algorithms for the strong scaling limit? The\n  problem is to generate well balanced partitions when the ratio \"number of\n  elements\" $/$ \"number of processors\" is small. METIS 4 does not do well on\n  this type of problems. What about METIS 5 and other graph partitioners? Maybe\n  we need to develop specialized algorithms?   MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "Notes and Remarks"
        }, 
        {
            "location": "/miniapps/", 
            "text": "CEED Miniapps\n\n\nCEED is developing a variety of miniapps encapsulating key physics and numerical\nkernels of high-order applications.  The miniapps are designed to be used in a\nvariety of \nco-design\n activities with ECP vendors, software\ntechnologies projects as well as external partners.\n\n\nCEED's current miniapps are documented below and can be found on\n\nGitHub\n. Please contact the CEED team if you have any\nquestions.\n\n\n\n\n\n\nNekbone\n\n\n\n\nNekbone solves a standard Poisson equation using a conjugate gradient iteration\nwith a simple or spectral element multigrid preconditioner on a block or linear\ngeometry.\n\n\nIt exposes the principal computational kernel to reveal the essential elements\nof the algorithmic-architectural coupling that is pertinent to \nNek5000\n,\nrelevant to large eddy simulation (LES) and direct numerical simulation (DNS) of\nturbulence in complex domains.\n\n\nNekbone-3.1 was released in Oct 2013 as part of the \nCESAR co-design\ncenter\n.\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\nDownload Nekbone\n\n\n\n\n\n\n\n\n\n\nLaghos \nNew\n\n\n\n\nLaghos solves the time-dependent Euler equation of compressible gas dynamics in\na moving Lagrangian frame using high-order finite element spatial discretization\nand explicit high-order time-stepping.\n\n\nIt captures the basic structure of many other compressible shock\nhydrocodes, including the \nBLAST code\n at\n\nLawrence Livermore National Laboratory\n. The miniapp\nis build on top of a general discretization library, \nMFEM\n,\nthus separating the pointwise physics from finite element and meshing concerns.\n\n\n\n\nComputational motives in Laghos include the following:\n\n\n\n\nSupport for moving (high-order) unstructured 2D and 3D meshes.\n\n\nContinuous and discontinuous high-order finite element discretization spaces\n  of runtime-specified order.\n\n\nExplicit time-stepping loop with a variety of time integrator options. Laghos\n  supports Runge-Kutta ODE solvers of orders 1, 2, 3, 4 and 6.\n\n\nFull and \npartial assembly\n options\n  for deriving and solving the resulting ODE systems.\n\n\nPoint-wise definition of mesh size, time-step estimate and artificial\n  viscosity coefficient.\n\n\nDomain-decomposed MPI parallelism.\n\n\nOptional in-situ visualization with \nGLVis\n and data output\n  for visualization / data analysis with \nVisIt\n.\n\n\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\nDownload Laghos\n\n\n\n\n\n\n\n\n\n\nNekCEM CEEDling \nNew\n\n\n\n\nNekCEM CEEDling is a \nNekCEM\n miniapp, solving the time-domain Maxwell\nequations for electromagnetic systems.  It allows software and hardware\ndevelopers to understand the basic structure and computational costs of NekCEM\nover a broad spectrum of architectures ranging from software-based simulators\nrunning at one ten-thousandth the speed of current processors to exascale\nplatforms running millions of times faster than single-core platforms.\n\n\nNekCEM provides flexibility to adapt new programming approaches for scalability\nand performance studies on a variety of platforms without having to understand\nall the features of NekCEM, currently supporting CPU/GPU runs.\n\n\n\n\n\n\nDownload NekCEM CEEDling\n\n\n\n\n\n\n\n\n\n\nHolmes \nNew\n\n\n\n\nHolmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.\n\n\nIts features include:\n\n\n\n\nElements\n\n\nSpectral element quadrilateral and hexahedral elements\n\n\nWarp \n blend high-order nodal triangular and tetrahedral elements\n\n\n\n\n\n\nDiscretizations\n\n\nElliptic: continuous and discontinuous Galerkin discretization\n\n\nHyperbolic: discontinuous Galerkin time-domain\n\n\n\n\n\n\nSolvers\n\n\nTwo level overlapping additive Schwartz elliptic problem preconditioning\n\n\nFast approximate block preconditioning\n\n\nAlgebraic multigrid coarse solver with heterogeneous acceleration\n\n\nParallel direct solver XXT for the coarsest level solve\n\n\n\n\n\n\nFull algebraic multigrid preconditioning\n\n\n\n\n\n\nImplementation\n\n\nPortable on-node parallelism via the Open Concurrent Compute Abstraction, \nOCCA\n\n\nMPI communications for distributed computations\n\n\n\n\n\n\nDependencies\n\n\nThe XXT parallel direct solver for the coarsest AMG solve\n\n\nGSLIB\n for optimized gather-scatter operations\n\n\n\n\n\n\nExample CEEDling apps\n\n\nIncompressible flow solvers\n\n\nElliptic solvers\n\n\nBoltzmann flow solver\n\n\nAcoustics time-domain solver\n\n\n\n\n\n\n\n\nHolmes is being developed in \nTim Warburton's group\n\nat \nVirginia Tech\n.\n\n\nThe project GitHub repository will be made public in 2017. Stay tuned for more details...\n\n\n\n\n\n\nHPGMG-FE\n\n\n\n\nThe High-Performance Geometric Multigrid Finite Element benchmark is a\nFull Multigrid solver for a third order accurate finite element\ndiscretization of an elliptic operator on mapped grids.  The benchmark\nis designed to give a picture of the \"performance spectrum\" across a\nrange of problem sizes and thus solution time, thereby giving a clear\npicture of both weak and strong scaling concerns.\n\n\nFor details, see the \nHPGMG website\n.\n\n\n\n\n\n\nMFEM Example Codes\n\n\n\n\nThe \nMFEM\n distribution includes a large number of simple example codes\nthat can be viewed as simple miniapps for model high-order physics:\n\n\n\n\nExample 1\n: nodal H1 FEM for the Laplace problem,\n\n\nExample 2\n: vector FEM for linear elasticity,\n\n\nExample 3\n: Nedelec H(curl) FEM for the definite Maxwell problem,\n\n\nExample 4\n: Raviart-Thomas H(div) FEM for the grad-div problem,\n\n\nExample 5\n: mixed pressure-velocity FEM for the Darcy problem,\n\n\nExample 6\n: non-conforming adaptive mesh refinement (AMR) for the Laplace problem,\n\n\nExample 7\n: Laplace problem on a surface (the unit sphere),\n\n\nExample 8\n: Discontinuous Petrov-Galerkin (DPG) for the Laplace problem,\n\n\nExample 9\n: Discontinuous Galerkin (DG) time-dependent advection,\n\n\nExample 10\n: time-dependent implicit nonlinear elasticity,\n\n\nExample 11\n: parallel Laplace eigensolver,\n\n\nExample 12\n: parallel linear elasticity eigensolver,\n\n\nExample 13\n: parallel Maxwell eigensolver,\n\n\nExample 14\n: Discontinuous Galerkin (DG) for the Laplace problem,\n\n\nExample 15\n: dynamic AMR for Laplace with prescribed time-dependent source,\n\n\nExample 16\n: time-dependent nonlinear heat equation,\n\n\nExample 17\n: Discontinuous Galerkin (DG) for linear elasticity.\n\n\n\n\nMost of the examples have a serial and a parallel version, illustrating the ease\nof transition and the minimal code changes between the two.\n\n\nOf particular relevance to CEED is the HPC versions of the example codes that\nuse a \nset of templated classes\n to efficiently\nimplement high-order operator evaluation.\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\n\n\nDownload MFEM", 
            "title": "Miniapps"
        }, 
        {
            "location": "/miniapps/#ceed-miniapps", 
            "text": "CEED is developing a variety of miniapps encapsulating key physics and numerical\nkernels of high-order applications.  The miniapps are designed to be used in a\nvariety of  co-design  activities with ECP vendors, software\ntechnologies projects as well as external partners.  CEED's current miniapps are documented below and can be found on GitHub . Please contact the CEED team if you have any\nquestions.", 
            "title": "CEED Miniapps"
        }, 
        {
            "location": "/miniapps/#nekbone", 
            "text": "Nekbone solves a standard Poisson equation using a conjugate gradient iteration\nwith a simple or spectral element multigrid preconditioner on a block or linear\ngeometry.  It exposes the principal computational kernel to reveal the essential elements\nof the algorithmic-architectural coupling that is pertinent to  Nek5000 ,\nrelevant to large eddy simulation (LES) and direct numerical simulation (DNS) of\nturbulence in complex domains.  Nekbone-3.1 was released in Oct 2013 as part of the  CESAR co-design\ncenter .    Documentation     Download Nekbone", 
            "title": "Nekbone"
        }, 
        {
            "location": "/miniapps/#laghos-new", 
            "text": "Laghos solves the time-dependent Euler equation of compressible gas dynamics in\na moving Lagrangian frame using high-order finite element spatial discretization\nand explicit high-order time-stepping.  It captures the basic structure of many other compressible shock\nhydrocodes, including the  BLAST code  at Lawrence Livermore National Laboratory . The miniapp\nis build on top of a general discretization library,  MFEM ,\nthus separating the pointwise physics from finite element and meshing concerns.   Computational motives in Laghos include the following:   Support for moving (high-order) unstructured 2D and 3D meshes.  Continuous and discontinuous high-order finite element discretization spaces\n  of runtime-specified order.  Explicit time-stepping loop with a variety of time integrator options. Laghos\n  supports Runge-Kutta ODE solvers of orders 1, 2, 3, 4 and 6.  Full and  partial assembly  options\n  for deriving and solving the resulting ODE systems.  Point-wise definition of mesh size, time-step estimate and artificial\n  viscosity coefficient.  Domain-decomposed MPI parallelism.  Optional in-situ visualization with  GLVis  and data output\n  for visualization / data analysis with  VisIt .     Documentation     Download Laghos", 
            "title": "Laghos New"
        }, 
        {
            "location": "/miniapps/#nekcem-ceedling-new", 
            "text": "NekCEM CEEDling is a  NekCEM  miniapp, solving the time-domain Maxwell\nequations for electromagnetic systems.  It allows software and hardware\ndevelopers to understand the basic structure and computational costs of NekCEM\nover a broad spectrum of architectures ranging from software-based simulators\nrunning at one ten-thousandth the speed of current processors to exascale\nplatforms running millions of times faster than single-core platforms.  NekCEM provides flexibility to adapt new programming approaches for scalability\nand performance studies on a variety of platforms without having to understand\nall the features of NekCEM, currently supporting CPU/GPU runs.    Download NekCEM CEEDling", 
            "title": "NekCEM CEEDling New"
        }, 
        {
            "location": "/miniapps/#holmes-new", 
            "text": "Holmes is an experimental testbed for multi-level parallel implementations of\nhigh-order finite element computations.  Its features include:   Elements  Spectral element quadrilateral and hexahedral elements  Warp   blend high-order nodal triangular and tetrahedral elements    Discretizations  Elliptic: continuous and discontinuous Galerkin discretization  Hyperbolic: discontinuous Galerkin time-domain    Solvers  Two level overlapping additive Schwartz elliptic problem preconditioning  Fast approximate block preconditioning  Algebraic multigrid coarse solver with heterogeneous acceleration  Parallel direct solver XXT for the coarsest level solve    Full algebraic multigrid preconditioning    Implementation  Portable on-node parallelism via the Open Concurrent Compute Abstraction,  OCCA  MPI communications for distributed computations    Dependencies  The XXT parallel direct solver for the coarsest AMG solve  GSLIB  for optimized gather-scatter operations    Example CEEDling apps  Incompressible flow solvers  Elliptic solvers  Boltzmann flow solver  Acoustics time-domain solver     Holmes is being developed in  Tim Warburton's group \nat  Virginia Tech .  The project GitHub repository will be made public in 2017. Stay tuned for more details...", 
            "title": "Holmes New"
        }, 
        {
            "location": "/miniapps/#hpgmg-fe", 
            "text": "The High-Performance Geometric Multigrid Finite Element benchmark is a\nFull Multigrid solver for a third order accurate finite element\ndiscretization of an elliptic operator on mapped grids.  The benchmark\nis designed to give a picture of the \"performance spectrum\" across a\nrange of problem sizes and thus solution time, thereby giving a clear\npicture of both weak and strong scaling concerns.  For details, see the  HPGMG website .", 
            "title": "HPGMG-FE"
        }, 
        {
            "location": "/miniapps/#mfem-example-codes", 
            "text": "The  MFEM  distribution includes a large number of simple example codes\nthat can be viewed as simple miniapps for model high-order physics:   Example 1 : nodal H1 FEM for the Laplace problem,  Example 2 : vector FEM for linear elasticity,  Example 3 : Nedelec H(curl) FEM for the definite Maxwell problem,  Example 4 : Raviart-Thomas H(div) FEM for the grad-div problem,  Example 5 : mixed pressure-velocity FEM for the Darcy problem,  Example 6 : non-conforming adaptive mesh refinement (AMR) for the Laplace problem,  Example 7 : Laplace problem on a surface (the unit sphere),  Example 8 : Discontinuous Petrov-Galerkin (DPG) for the Laplace problem,  Example 9 : Discontinuous Galerkin (DG) time-dependent advection,  Example 10 : time-dependent implicit nonlinear elasticity,  Example 11 : parallel Laplace eigensolver,  Example 12 : parallel linear elasticity eigensolver,  Example 13 : parallel Maxwell eigensolver,  Example 14 : Discontinuous Galerkin (DG) for the Laplace problem,  Example 15 : dynamic AMR for Laplace with prescribed time-dependent source,  Example 16 : time-dependent nonlinear heat equation,  Example 17 : Discontinuous Galerkin (DG) for linear elasticity.   Most of the examples have a serial and a parallel version, illustrating the ease\nof transition and the minimal code changes between the two.  Of particular relevance to CEED is the HPC versions of the example codes that\nuse a  set of templated classes  to efficiently\nimplement high-order operator evaluation.    Documentation     Download MFEM", 
            "title": "MFEM Example Codes"
        }, 
        {
            "location": "/software/", 
            "text": "Latest Releases\n\n\nThe CEED software is open-source and publicly available on\n\nGitHub\n. The latest releases of our main software\ncomponents are available from:\n\n\n\n\n\n\nCEED-1.0\n\n\n\n\n\n\n\n\nlibCEED-0.2\n\n\n\n\n\n\n\n\nNek5000-17.0\n\n\n\n\n\n\n\n\nmfem-3.3.2\n\n\n\n\n\n\n\n\nLaghos-1.0\n\n\n\n\n\n\n\n\nNekbone-17.0\n\n\n\n\n\n\nPlease use the \nCEED user forum\n to report\n\nbugs\n or post\n\nquestions\n or\n\ncomments\n.\n\n\nSoftware Catalog\n\n\nThe CEED team includes members of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:\n\n\n\n\n\n\nNext-generation \nfinite element\n \ndiscretization\n\n  \nlibraries\n that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.\n\n\n\n\n\n\nMiniapps\n combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and \nbenchmark problems\n.\n\n\n\n\n\n\nBroadly applicable technologies, including extensions of dense linear algebra\n  libraries to support \nfast tensor contractions\n, scalable\n  matrix-free \nlinear solvers\n and programming models for \nperformance\n  portability\n.", 
            "title": "Catalog"
        }, 
        {
            "location": "/software/#latest-releases", 
            "text": "The CEED software is open-source and publicly available on GitHub . The latest releases of our main software\ncomponents are available from:    CEED-1.0     libCEED-0.2     Nek5000-17.0     mfem-3.3.2     Laghos-1.0     Nekbone-17.0    Please use the  CEED user forum  to report bugs  or post questions  or comments .", 
            "title": "Latest Releases"
        }, 
        {
            "location": "/software/#software-catalog", 
            "text": "The CEED team includes members of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects.  Building on\nthese efforts, the co-design center is producing a range of software products,\nincluding:    Next-generation  finite element   discretization \n   libraries  that enable unstructured PDE-based applications to take\n  full advantage of exascale resources. These libraries cover the full spectrum\n  of discretizations, from assembled low-order to matrix-free high-order\n  methods.    Miniapps  combining applications-relevant physics with key\n  high-order kernels that use matrix-free forms for efficient performance. CEED\n  also develops element-level kernels and  benchmark problems .    Broadly applicable technologies, including extensions of dense linear algebra\n  libraries to support  fast tensor contractions , scalable\n  matrix-free  linear solvers  and programming models for  performance\n  portability .", 
            "title": "Software Catalog"
        }, 
        {
            "location": "/ceed-1.0/", 
            "text": "CEED 1.0 Software Distribution\n\n\nThe CEED distribution is a collection of software packages that can be\nintegrated together to enable efficient discretizations in a variety of\nhigh-order applications on unstructured grids.\n\n\nCEED is using the \nSpack\n package manager for compatible\nbuilding and installation of its software components.\n\n\nIn this initial version, CEED 1.0, the CEED software suite consists of the\nfollowing 12 packages, plus the CEED \nmeta-package\n:\n\n\n\n\nGSLIB\n\n\nHPGMG\n\n\nLaghos\n\n\nlibCEED\n\n\nMAGMA\n\n\nMFEM\n\n\nNek5000\n\n\nNekbone\n\n\nNekCEM\n\n\nPETSc\n\n\nPUMI\n\n\nOCCA\n\n\n\n\nFirst-time users should read \nSimple Installation\n and\n\nUsing the Installation\n below. (Quick summary: you can\nbuild and install all of the above packages with: \nspack install ceed\n)\n\n\nIf you are familiar with Spack, consider using the following machine-specific\nconfigurations for CEED (see also \nxSDK's config files\n).\n\n\n\n\n\n\n\n\nPlatform\n\n\nArchitecture\n\n\nSpack Configuration\n\n\n\n\n\n\n\n\n\n\n\n\nMac\n\n\ndarwin-x86_64\n\n\npackages\n \n\n\n\n\n\n\n\n\nLinux\n (RHEL7)\n\n\nlinux-rhel7-x86_64\n\n\npackages\n \n\n\n\n\n\n\n\n\nCori\n (NERSC)\n\n\ncray-CNL-haswell\n\n\npackages\n \n\n\n\n\n\n\n\n\nEdison\n (NERSC)\n\n\ncray-CNL-ivybridge\n\n\npackages\n \n\n\n\n\n\n\n\n\nTheta\n (ALCF)\n\n\ncray-CNL-mic_knl\n\n\npackages\n \n\n\n\n\n\n\n\n\nTitan\n (OLCF)\n\n\ncray-CNL-interlagos\n\n\npackages\n \n\n\n\n\n\n\n\n\nCORAL-EA\n (LLNL)\n\n\nblueos_3_ppc64le_ib\n\n\npackages\n \n \n \ncompilers\n \n\n\n\n\n\n\n\n\nTOSS3\n (LLNL)\n\n\ntoss_3_x86_64_ib\n\n\npackages\n \n \n \ncompilers\n \n\n\n\n\n\n\n\n\n\n\nFor additional details, please consult the following sections:\n\n\n\n\nSimple Installation\n\n\nUsing the Installation\n\n\nBuilding MFEM-based Applications\n\n\nBuilding libCEED-based Applications\n\n\nGPU demo\n\n\n\n\n\n\nSpack for Beginners\n\n\nTips and Troubleshooting\n\n\nBuilding on Mac\n\n\nBuilding on Linux\n\n\nBuilding at LLNL\n\n\nTOSS3 Platforms\n\n\nCORAL Early Access Platforms\n\n\n\n\n\n\nBuilding at NERSC\n\n\nCori\n\n\nEdison\n\n\n\n\n\n\nBuilding at ALCF\n\n\nTheta\n\n\n\n\n\n\nBuilding at OLCF\n\n\nTitan\n\n\n\n\n\n\nInstalling CUDA\n\n\n\n\n\n\n\n\nThe CEED team can be contacted by posting to our \nUser Forum\n or\nvia email at \nceed-users@llnl.gov\n.\nFor issues related to the CEED Spack packages,\nplease start a discussion on the GitHub \n@spack/ceed\n page.\n\n\nSimple Installation\n\n\nIf Spack is already available on your system and is visible in your \nPATH\n, you\ncan install the CEED software simply with:\n\n\nspack install -v ceed\n\n\n\n\nTo enable package testing during the build process, use instead:\n\n\nspack install -v --test=all ceed\n\n\n\n\nIf you don't have Spack, you can download it and install CEED with the following\ncommands:\n\n\ngit clone https://github.com/spack/spack.git\ncd spack\n./bin/spack install -v ceed\n\n\n\n\nTo avoid long compile times, we strongly recommend that you add a \npackages.yaml\n file\nfor your platform, see above and the \nTips and Troubleshooting\n\nsection.\n\n\nUsing the Installation\n\n\nSpack will install the CEED packages (and the libraries they depend on) in a\nsubtree of \n./opt/spack/\narchitecture\n/\ncompiler\n/\n that is specific to the\narchitecture and compiler used (multiple compiler and/or architecture builds can\ncoexist in a single Spack directory).\n\n\nBelow are several examples of how the Spack installation can be linked with and\nused in user applications.\n\n\nBuilding MFEM-based Applications\n\n\nThe simplest way to use the Spack installation is through the\n\nspack location\n command. For example, MFEM-based codes, such as\nthe MFEM examples, can be simply built as follows:\n\n\ngit clone git@github.com:mfem/mfem.git\ncd mfem; git checkout v3.3.2\ncd examples\nmake CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk\ncd ../miniapps/electromagnetics\nmake CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk\n\n\n\n\nAlternatively, the Spack installation can be exported to a local directory:\n\n\nmkdir ceed\nspack view --verbose symlink ceed/mfem mfem\n\n\n\n\nThe \nceed/mfem\n directory now contains the Spack-built MFEM with all of its\ndependencies (technically, it contains links to all the build files inside the\n\n./opt/spack/\n subdirectory for MFEM). In particular, the MFEM library in\n\nceed/mfem/lib\n and the MFEM build configuration file in\n\nceed/mfem/share/mfem/config.mk\n.\n\n\nThis directory can be used to build the MFEM examples as follows:\n\n\ngit clone git@github.com:mfem/mfem.git\ncd mfem; git checkout v3.3.2\ncd examples/petsc\nmake CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk\ncd ..\nmake CONFIG_MK=../../ceed/mfem/share/mfem/config.mk\n\n\n\n\nThe MFEM miniapps can further be built with:\n\n\ncd ../miniapps/electromagnetics\nmake CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk\n\n\n\n\nBuilding libCEED-based Applications\n\n\nBelow we illustrate how to use the Spack installation to build libCEED-based\napplications, by building the examples in the current libCEED distribution.\n\n\nUsing \nspack location\n, the libCEED examples can be built as\nfollows:\n\n\ngit clone git@github.com:CEED/libCEED.git\ncd libCEED/examples/ceed\nmake CEED_DIR=`spack location -i libceed`\n./ex1 -ceed /cpu/self\n\n\n\n\nIf you have multiple builds of \nlibceed\n or \nocca\n you need to be more specific\nin the above \nspack location\n command. To list all \nlibceed\n and\n\nocca\n versions use \nspack find\n:\n\n\nspack find -lv libceed occa\n\n\n\n\nThen either use \nvariants\n to choose a unique version, e.g.\n\nlibceed~cuda\n, or specify the hashes printed in front of the \nlibceed\n\n\nspec\n, e.g. \nlibceed/yb3fvek\n or just \n/yb3fvek\n (and similarly\nfor \nocca\n).\n\n\nThe serial, OpenMP, OpenCL and GPU OCCA backends can be used with:\n\n\n./ex1 -ceed /cpu/occa\n./ex1 -ceed /omp/occa\n./ex1 -ceed /ocl/occa\n./ex1 -ceed /gpu/occa\n\n\n\n\nIn order to use the OCCA GPU backend, one needs to install CEED with the \ncuda\n\n\nvariant\n enabled, i.e. using the \nspec\n \nceed+cuda\n:\n\n\nspack install -v ceed+cuda\n\n\n\n\nFor more details, see the section \nGPU demo\n below.\n\n\nWith the MAGMA backend, the \n/cpu/magma\n and \n/gpu/magma\n resource descriptors\ncan also be used.\n\n\nThe MFEM/libCEED and PETSc/libCEED examples can be further built with:\n\n\ncd examples/mfem\nmake CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem`\n./bp1 -no-vis -o 2 -ceed /cpu/self\n./bp3 -no-vis -o 2 -ceed /cpu/self\ncd ../petsc\nmake CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc`\n./bp1 -degree 2 -ceed /cpu/self\n\n\n\n\nNote that if \nPETSC_ARCH\n is set in your environment, you must either unset it\nor also pass \nPETSC_ARCH=\n in the above command.\n\n\nDepending on the available backends, additional CEED resource descriptors,\ne.g. \npetsc/bp1 -degree 2 -ceed /ocl/occa\n or \nmfem/bp1 -no-vis --order 2 -ceed\n/gpu/occa\n can be provided.\n\n\nFinally, the Nek5000/libCEED examples can be built as follows:\n\n\ncd ../nek5000\nexport CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000`\n./make-nek-examples.sh\n\n\n\n\nThen you can run the Nek5000 examples as follows:\n\n\nexport MPIEXEC=`spack location -i openmpi`/bin/mpiexec\n./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3\n\n\n\n\nIn the above example, replace \nopenmpi\n with wahtever the MPI implementation\nyou have installed with spack. Also, you can do \n./run-nek-example.sh -h\n\nto find out the options supported by the run script.\n\n\noptions:\n   -h|-help     Print this usage information and exit\n   -c|-ceed     Ceed backend to be used for the run (optional, default: /cpu/self)\n   -e|-example  Example name (optional, default: bp1)\n   -n|-np       Specify number of MPI ranks for the run (optional, default: 4)\n   -b|-box      Specify the box geometry to be found in ./boxes/ directory (Mandatory)\n\n\n\n\nMore information on running the Nek5000 examples can be found in the libCEED\n\ndocumentation\n.\n\n\nAlternatively, one can export the Spack install to a local directory:\n\n\nspack view --verbose symlink ceed/libceed libceed\nspack view --verbose symlink ceed/petsc petsc\nspack view --verbose symlink ceed/mfem mfem\nspack view --verbose symlink ceed/nek5000 nek5000\n\n\n\n\nand use that to specify the \nCEED_DIR\n, \nMFEM_DIR\n and \nPETSC_DIR\n variables:\n\n\ncd libCEED/examples/ceed\nmake CEED_DIR=../../ceed/libceed\n./ex1 -ceed /cpu/self\ncd mfem\nmake CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem\n./bp1 -no-vis -o 2 -ceed /cpu/self\n./bp3 -no-vis -o 2 -ceed /cpu/self\ncd ../petsc\nmake CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc\n./bp1 -degree 2 -ceed /cpu/self\n\n\n\n\nGPU demo\n\n\nBelow is the full set of commands to install the CEED distribution on a\nGPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM,\nPETSc and Nek examples. Note that these are very different codes (C++, C, F77)\nwhich can nevertheless take advanatage through libCEED of a common set of GPU\nkernels.\n\n\nThe \nsetenv\n commands below assume \ncsh\n/\ntcsh\n. We strongly recommend to add a\n\npackages.yaml\n file in order to avoid long compile times, see \nTips and\nTroubleshooting\n.\n\n\n# Install CEED 1.0 distribution via Spack\ngit clone git@github.com:spack/spack.git\ncd spack\nspack install ceed+cuda\n\n# Setup CEED component directories\nsetenv CEED_DIR  `spack location -i libceed`\nsetenv MFEM_DIR  `spack location -i mfem`\nsetenv PETSC_DIR `spack location -i petsc`\nsetenv NEK5K_DIR `spack location -i nek5000`\n\n# Clean OCCA cache\n# rm -rf ~/.occa\n\n# Clone libCEED examples directory as proxy for libCEED-based codes\ngit clone git@github.com:CEED/libCEED.git\nmv libCEED/examples ceed-examples\nrm -rf libCEED\n\n# libCEED examples on CPU and GPU\ncd ceed-examples/ceed\nmake\n./ex1 -ceed /cpu/self\n./ex1 -ceed /gpu/occa\ncd ../..\n\n# MFEM+libCEED examples on CPU and GPU\ncd ceed-examples/mfem\nmake\n./bp1 -ceed /cpu/self -no-vis\n./bp1 -ceed /gpu/occa -no-vis\ncd ../..\n\n# PETSc+libCEED examples on CPU and GPU\ncd ceed-examples/petsc\nmake\n./bp1 -ceed /cpu/self\n./bp1 -ceed /gpu/occa\ncd ../..\n\n# Nek+libCEED examples on CPU and GPU\ncd ceed-examples/nek5000\n./make-nek-examples.sh\n./run-nek-example.sh -ceed /cpu/self -b 3\n./run-nek-example.sh -ceed /gpu/occa -b 3\ncd ../..\n\n\n\n\nSpack for Beginners\n\n\nSpack is a package manager for scientific software that supports multiple\nversions, configurations, platforms, and compilers.\n\n\nWhile Spack does not change the build system that already exists in each CEED\ncomponent, it coordinates the dependencies between these components and enables\nthem to be built with the same compilers and options.\n\n\nIf you are new to Spack, here are some Spack commands and options that you may\nfind useful:\n\n\n\n\n\n\nSpack is a set of Python scripts so there is nothing to install!\n  Just download with \ngit clone https://github.com/spack/spack.git\n and add\n  \nspack/bin\n to your path with the following commands:\n\n  \n. share/spack/setup-env.sh\n for \nbash\n/\nzsh\n or \n\n  \nsh\n  setenv SPACK_ROOT `pwd`; source $SPACK_ROOT/share/spack/setup-env.csh\n\n  for \ncsh\n/\ntcsh\n.\n\n\n\n\n\n\nSpack should automatically locate the standard compilers on your system.  Use\n  \nspack compilers\n to list the ones that have been found. If you need to\n  configure additional compilers, you can do that through the config file,\n  \n~/.spack/compilers.yaml\n, or the platform-specific config file,\n  \n~/.spack/\nplatform\n/compilers.yaml\n. Some examples of such files are\n  provided below. Check the \nSpack documentation\n for additional\n  details.\n\n\n\n\n\n\nSpack likes to build all of its packages. The file \n~/.spack/packages.yaml\n,\n  and similarly the platform-specific, \n~/.spack/\nplatform\n/packages.yaml\n,\n  allow you to list the packages already installed on your system for Spack to\n  use instead of compiling them itself. Some examples are provided below.\n\n\n\n\n\n\nSkip the \n-v\n option of \nspack install\n to see only a summary for the building\n  of each package (as opposed to the compilation of individual files): \nspack\n  install ceed\n.  You can still turn the detailed build output on and off by\n  pressing the \nv\n key in the Spack terminal.\n\n\n\n\n\n\nTo troubleshoot the spack install process: \nspack --debug --verbose install ceed\n.\n\n\n\n\n\n\nTo do a dry run of the spack install process: \nspack install --fake ceed\n.\n  Note that you will have to run \nspack uninstall --all\n to clean up after this.\n\n\n\n\n\n\nTo see the specific packages that will be installed for a particular package,\n  e.g. \nceed\n, use: \nspack spec -I ceed\n.\n\n\n\n\n\n\nTo see the list of all installed packages: \nspack find\n.\n\n\n\n\n\n\nTo list the location where all different versions of the \nceed\n package were\n  installed: \nspack find --long --paths ceed\n. Alternatively, for a specific\n  version you can use \nspack location --install-dir ceed\n.\n\n\n\n\n\n\nTo uninstall a package, e.g. mfem, including all packages that depend on it:\n  \nspack uninstall --all --dependents mfem\n, or \nspack uninstall /qzn2u7t\n for a\n  particular hash.\n\n\n\n\n\n\nTo uninstall \nall\n packages that were ever installed by Spack: \nspack\n  uninstall --all\n.  In this case you may also want to clear the caches that\n  Spack maintains with: \nspack clean -a\n.\n\n\n\n\n\n\nTips and Troubleshooting\n\n\nBuilding on a Mac\n\n\nThe file \ndarwin-x86_64-packages.yaml\n provides a\nsample \npackages.yaml\n file based on \nHomebrew\n, that should work on\nmost Macs. (You can use MacPorts instead of Homebrew if you prefer.)\n\n\npackages:\n    all:\n        compiler: [clang]\n        providers:\n            blas: [veclibfort]\n            lapack: [veclibfort]\n            mpi: [openmpi]\n    openmpi:\n        paths:\n            openmpi@3.0.0: ~/brew\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.10.2: ~/brew\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.1.85: /usr/local/cuda\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /opt/X11\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /opt/X11\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.10: /usr\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.11: /usr\n        buildable: False\n\n\n\n\nThe packages in \n~/brew\n were installed with \nbrew install package\n.  If you\ndon't have Homebrew, you can install it and the needed tools with:\n\n\ngit clone https://github.com/Homebrew/brew.git\ncd brew\nbin/brew install openmpi cmake python zlib\n\n\n\n\nThe packages in \n/usr\n are provided by Apple and come pre-built with Mac OS\nX. The \ncuda\n package is provided by NVIDIA and should be installed separately\nby downloading it from \nNVIDIA\n. We are using the Clang compiler,\nOpenMPI, and Apple's BLAS/LAPACK accelerator library.\n\n\nBuilding on a Linux Desktop\n\n\nThe file \nlinux-rhel7-x86_64-packages.yaml\n\nprovides a sample \npackages.yaml\n file that can be adapted to work on most Linux\ndesktops (this particular file was tested on RHEL7).\n\n\npackages:\n    all:\n        compiler: [gcc]\n        providers:\n            mpi: [openmpi]\n            blas: [netlib-lapack]\n            lapack: [netlib-lapack]\n    netlib-lapack:\n        paths:\n            netlib-lapack@system: /usr/lib64\n        buildable: False\n    openmpi:\n        paths:\n            openmpi@3.0.0: ~/local\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.10.2: ~/local\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.1.85: ~/local/cuda\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n             python@2.7.14: /usr\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.11: /usr/lib64\n        buildable: False\n\n\n\n\nThe above file uses user-installed OpenMPI, CMake and CUDA packages, with the\nrest of the CEED prerequisites installed via the \nyum package manager\n.\n\n\nBuilding at LLNL's Computing Center\n\n\nTOSS3 Platforms\n\n\nThe file \ntoss_3_x86_64_ib-packages.yaml\n is an\nexample of a \npackages.yaml\n file for the TOSS3 system type at LLNL's \nLivermore\nComputing\n center.\n\n\npackages:\n    all:\n        compiler: [intel, gcc, clang, pgi]\n        providers:\n            mpi: [mvapich2, mpich, openmpi]\n            blas: [intel-mkl, openblas]\n            lapack: [intel-mkl, openblas]\n    intel-mkl:\n        paths:\n            intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0\n        buildable: False\n    mvapich2:\n        paths:\n            mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1\n            mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3\n            mvapich2@2.2%gcc@7.1.0: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-7.1.0\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.8.2: /usr/tce/packages/cmake/cmake-3.8.2\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14: /usr/tce/packages/python/python-2.7.14\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.7: /usr\n        buildable: False\n\n\n\n\nThe above file can be used to build CEED with different compilers (Intel being\nthe default), for example:\n\n\nspack install ceed%gcc~petsc\n\n\n\n\nA corresponding \ncompilers.yaml\n file for the TOSS3 platform can be found here:\n\ntoss_3_x86_64_ib-compilers.yaml\n.\n\n\nCORAL Early Access Platforms\n\n\nThe file \nblueos_3_ppc64le_ib-packages.yaml\n\nis an example of a \npackages.yaml\n file for the CORAL early access systems at\nLLNL's \nLivermore Computing\n center (this particular file is for the\nRay machine).\n\n\npackages:\n    all:\n        compiler: [xl_r, xl, gcc, clang, pgi]\n        providers:\n            mpi: [spectrum-mpi]\n            blas: [essl]\n            lapack: [netlib-lapack]\n    essl:\n        paths:\n            essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0\n        variants: threads=none\n        version: [6.1.0]\n        buildable: False\n    veclibfort:\n        buildable: False\n    intel-parallel-studio:\n        buildable: False\n    intel-mkl:\n        buildable: False\n    atlas:\n        buildable: False\n    openblas:  # OpenBLAS can be built only with gcc\n        buildable: False\n    netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant\n        variants: +external-blas~lapacke\n    spectrum-mpi:\n        paths:\n            spectrum-mpi@2017-04-03%xl_r@13.1.7-beta3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.03.21\n            spectrum-mpi@2017-04-03%xl_r@13.1.7-beta2: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.02.22\n            spectrum-mpi@2017-04-03%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-gcc-4.9.3\n            spectrum-mpi@2017-04-03%clang@3.8.0: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-clang-coral-2018.02.09\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2\n        version: [3.9.2]\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.0.176: /usr/tce/packages/cuda/cuda-9.0.176\n            cuda@9.1.85: /usr/tce/packages/cuda/cuda-9.1.85\n        version: [9.0.176, 9.1.85]\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14: /usr/tcetmp/packages/python/python-2.7.14\n        version: [2.7.14]\n        buildable: False\n\n\n\n\nA corresponding \ncompilers.yaml\n file can be found here:\n\nblueos_3_ppc64le_ib-compilers.yaml\n.\n\n\nBuilding at NERSC\n\n\nCori\n\n\nThe file \ncori-packages.yaml\n is an example of a\n\npackages.yaml\n file for the Cori system at \nNERSC\n.\n\n\npackages:\n    all:\n        compiler: [gcc@5.2.0, intel/16.0.3.210]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-haswell: cray-mpich\n            mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-haswell: cray-mpich\n        buildable: False\n\n    cmake:\n        modules:\n            cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-haswell: cmake\n            cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-haswell: cmake\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14%gcc@5.2.0 arch=cray-CNL-haswell: /usr\n            python@2.7.14%intel@16.0.3.210 arch=cray-CNL-haswell: /usr\n        buildable: False\n\n\n\n\nEdison\n\n\nThe file \nedison-packages.yaml\n is an example of a\n\npackages.yaml\n file for the Edison system at \nNERSC\n.\n\n\npackages:\n    all:\n        compiler: [gcc@5.2.0, intel/16.0.3.210]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-ivybridge: cray-mpich\n            mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-ivybridge: cray-mpich\n        buildable: False\n\n    cmake:\n        modules:\n            cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-ivybridge: cmake\n            cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-ivybridge: cmake\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14%gcc@5.2.0 arch=cray-CNL-ivybridge: /usr\n            python@2.7.14%intel@16.0.3.210 arch=cray-CNL-ivybridge: /usr\n        buildable: False\n\n\n\n\nBuilding at ALCF\n\n\nTheta\n\n\nThe file \ntheta-packages.yaml\n is an example of a\n\npackages.yaml\n file for the Theta system at \nALCF\n.\n\n\npackages:\n    all:\n        compiler: [intel@16.0.3.210, gcc@5.3.0]\n        providers:\n            mpi: [mpich]\n    intel-mkl:\n        paths:\n            intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel\n        buildable: False\n    mpich:\n        modules:\n            # requires 'module load cce' otherwise gives parsing error\n            mpich@7.6.3%gcc@5.3.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3\n            mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.5.2%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr\n            cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.13%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr\n            python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr\n        buildable: False\n\n\n\n\n\nBuilding at OLCF\n\n\nTitan\n\n\nThe file \ntitan-packages.yaml\n is an example of a\n\npackages.yaml\n file for the Titan system at \nOLCF\n.\n\n\npackages:\n    all:\n        compiler: [cce/8.6.4]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.3%cce@8.6.4 arch=cray-CNL-interlagos: cray-mpich\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.9.0%cce@8.6.4: /autofs/nccs-svm1_sw/titan/.swci/0-login/opt/spack/20170612/linux-suse_linux11-x86_64/gcc-4.3.4/cmake-3.9.0-owxiriblogovogl5zbrg45ulm3ln34cx/bin\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        modules:\n            python@2.7.9%cce@8.6.4 arch=cray-CNL-interlagos: python\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.17: /usr/lib64\n        buildable: False\n\n\n\n\nThe default install of curl on Titan does not support ssl, so you need to add\nthe path of a newer install to your \nPATH\n:\n\n\nmodule show curl  # get the /path/to/curl/bin/dir\nexport PATH=/path/to/curl/bin/dir:$PATH\n\n\n\n\nAdditional issues on Titan: Spack does not support \ncray-libsci\n for BLAS/LAPACK\n(there is no 'dummy package' for \ncray-libsci yet\n); the Cray\ncompiler, \ncce\n, fails to build \nopenblas\n (it does not e.g.  recognize the\n\n-m64\n flag, there may be other issues).\n\n\nWith these caveats, the CEED metapackage can be installed with:\n\n\n./bin/spack --debug --verbose install -v ceed%pgi@17.9.0 target=interlagos\n\n\n\n\nNote that \nspack\n will hang if you redirect \nstd[err|out]\n to a file (\n log\n)\nand background the command (by appending an \n).\n\n\nInstalling CUDA\n\n\n\n\n\n\nSeveral CEED packages depend on CUDA: OCCA, MAGMA and libCEED.\n\n\n\n\n\n\nTo build these, add the \ncuda\n variant to the Spack build:\n  \nsh\n  spack install ceed+cuda\n\n\n\n\n\n\nYou will need to have the NVIDIA CUDA SDK and driver installed on your system,\n  see \ndeveloper.nvidia.com\n, and specify it in the \npackages.yaml\n\n  file. See, for example, the \ncuda\n section in \nBuilding on a Mac\n,\n  or the \nlinux-rhel7-x86_64-packages.yaml\n file.", 
            "title": "CEED 1.0"
        }, 
        {
            "location": "/ceed-1.0/#ceed-10-software-distribution", 
            "text": "The CEED distribution is a collection of software packages that can be\nintegrated together to enable efficient discretizations in a variety of\nhigh-order applications on unstructured grids.  CEED is using the  Spack  package manager for compatible\nbuilding and installation of its software components.  In this initial version, CEED 1.0, the CEED software suite consists of the\nfollowing 12 packages, plus the CEED  meta-package :   GSLIB  HPGMG  Laghos  libCEED  MAGMA  MFEM  Nek5000  Nekbone  NekCEM  PETSc  PUMI  OCCA   First-time users should read  Simple Installation  and Using the Installation  below. (Quick summary: you can\nbuild and install all of the above packages with:  spack install ceed )  If you are familiar with Spack, consider using the following machine-specific\nconfigurations for CEED (see also  xSDK's config files ).     Platform  Architecture  Spack Configuration       Mac  darwin-x86_64  packages       Linux  (RHEL7)  linux-rhel7-x86_64  packages       Cori  (NERSC)  cray-CNL-haswell  packages       Edison  (NERSC)  cray-CNL-ivybridge  packages       Theta  (ALCF)  cray-CNL-mic_knl  packages       Titan  (OLCF)  cray-CNL-interlagos  packages       CORAL-EA  (LLNL)  blueos_3_ppc64le_ib  packages       compilers       TOSS3  (LLNL)  toss_3_x86_64_ib  packages       compilers        For additional details, please consult the following sections:   Simple Installation  Using the Installation  Building MFEM-based Applications  Building libCEED-based Applications  GPU demo    Spack for Beginners  Tips and Troubleshooting  Building on Mac  Building on Linux  Building at LLNL  TOSS3 Platforms  CORAL Early Access Platforms    Building at NERSC  Cori  Edison    Building at ALCF  Theta    Building at OLCF  Titan    Installing CUDA     The CEED team can be contacted by posting to our  User Forum  or\nvia email at  ceed-users@llnl.gov .\nFor issues related to the CEED Spack packages,\nplease start a discussion on the GitHub  @spack/ceed  page.", 
            "title": "CEED 1.0 Software Distribution"
        }, 
        {
            "location": "/ceed-1.0/#simple-installation", 
            "text": "If Spack is already available on your system and is visible in your  PATH , you\ncan install the CEED software simply with:  spack install -v ceed  To enable package testing during the build process, use instead:  spack install -v --test=all ceed  If you don't have Spack, you can download it and install CEED with the following\ncommands:  git clone https://github.com/spack/spack.git\ncd spack\n./bin/spack install -v ceed  To avoid long compile times, we strongly recommend that you add a  packages.yaml  file\nfor your platform, see above and the  Tips and Troubleshooting \nsection.", 
            "title": "Simple Installation"
        }, 
        {
            "location": "/ceed-1.0/#using-the-installation", 
            "text": "Spack will install the CEED packages (and the libraries they depend on) in a\nsubtree of  ./opt/spack/ architecture / compiler /  that is specific to the\narchitecture and compiler used (multiple compiler and/or architecture builds can\ncoexist in a single Spack directory).  Below are several examples of how the Spack installation can be linked with and\nused in user applications.", 
            "title": "Using the Installation"
        }, 
        {
            "location": "/ceed-1.0/#building-mfem-based-applications", 
            "text": "The simplest way to use the Spack installation is through the spack location  command. For example, MFEM-based codes, such as\nthe MFEM examples, can be simply built as follows:  git clone git@github.com:mfem/mfem.git\ncd mfem; git checkout v3.3.2\ncd examples\nmake CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk\ncd ../miniapps/electromagnetics\nmake CONFIG_MK=`spack location -i mfem`/share/mfem/config.mk  Alternatively, the Spack installation can be exported to a local directory:  mkdir ceed\nspack view --verbose symlink ceed/mfem mfem  The  ceed/mfem  directory now contains the Spack-built MFEM with all of its\ndependencies (technically, it contains links to all the build files inside the ./opt/spack/  subdirectory for MFEM). In particular, the MFEM library in ceed/mfem/lib  and the MFEM build configuration file in ceed/mfem/share/mfem/config.mk .  This directory can be used to build the MFEM examples as follows:  git clone git@github.com:mfem/mfem.git\ncd mfem; git checkout v3.3.2\ncd examples/petsc\nmake CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk\ncd ..\nmake CONFIG_MK=../../ceed/mfem/share/mfem/config.mk  The MFEM miniapps can further be built with:  cd ../miniapps/electromagnetics\nmake CONFIG_MK=../../../ceed/mfem/share/mfem/config.mk", 
            "title": "Building MFEM-based Applications"
        }, 
        {
            "location": "/ceed-1.0/#building-libceed-based-applications", 
            "text": "Below we illustrate how to use the Spack installation to build libCEED-based\napplications, by building the examples in the current libCEED distribution.  Using  spack location , the libCEED examples can be built as\nfollows:  git clone git@github.com:CEED/libCEED.git\ncd libCEED/examples/ceed\nmake CEED_DIR=`spack location -i libceed`\n./ex1 -ceed /cpu/self  If you have multiple builds of  libceed  or  occa  you need to be more specific\nin the above  spack location  command. To list all  libceed  and occa  versions use  spack find :  spack find -lv libceed occa  Then either use  variants  to choose a unique version, e.g. libceed~cuda , or specify the hashes printed in front of the  libceed  spec , e.g.  libceed/yb3fvek  or just  /yb3fvek  (and similarly\nfor  occa ).  The serial, OpenMP, OpenCL and GPU OCCA backends can be used with:  ./ex1 -ceed /cpu/occa\n./ex1 -ceed /omp/occa\n./ex1 -ceed /ocl/occa\n./ex1 -ceed /gpu/occa  In order to use the OCCA GPU backend, one needs to install CEED with the  cuda  variant  enabled, i.e. using the  spec   ceed+cuda :  spack install -v ceed+cuda  For more details, see the section  GPU demo  below.  With the MAGMA backend, the  /cpu/magma  and  /gpu/magma  resource descriptors\ncan also be used.  The MFEM/libCEED and PETSc/libCEED examples can be further built with:  cd examples/mfem\nmake CEED_DIR=`spack location -i libceed` MFEM_DIR=`spack location -i mfem`\n./bp1 -no-vis -o 2 -ceed /cpu/self\n./bp3 -no-vis -o 2 -ceed /cpu/self\ncd ../petsc\nmake CEED_DIR=`spack location -i libceed` PETSC_DIR=`spack location -i petsc`\n./bp1 -degree 2 -ceed /cpu/self  Note that if  PETSC_ARCH  is set in your environment, you must either unset it\nor also pass  PETSC_ARCH=  in the above command.  Depending on the available backends, additional CEED resource descriptors,\ne.g.  petsc/bp1 -degree 2 -ceed /ocl/occa  or  mfem/bp1 -no-vis --order 2 -ceed\n/gpu/occa  can be provided.  Finally, the Nek5000/libCEED examples can be built as follows:  cd ../nek5000\nexport CEED_DIR=`spack location -i libceed` NEK5K_DIR=`spack location -i nek5000`\n./make-nek-examples.sh  Then you can run the Nek5000 examples as follows:  export MPIEXEC=`spack location -i openmpi`/bin/mpiexec\n./run-nek-example.sh -e bp1 -c /cpu/self -n 2 -b 3  In the above example, replace  openmpi  with wahtever the MPI implementation\nyou have installed with spack. Also, you can do  ./run-nek-example.sh -h \nto find out the options supported by the run script.  options:\n   -h|-help     Print this usage information and exit\n   -c|-ceed     Ceed backend to be used for the run (optional, default: /cpu/self)\n   -e|-example  Example name (optional, default: bp1)\n   -n|-np       Specify number of MPI ranks for the run (optional, default: 4)\n   -b|-box      Specify the box geometry to be found in ./boxes/ directory (Mandatory)  More information on running the Nek5000 examples can be found in the libCEED documentation .  Alternatively, one can export the Spack install to a local directory:  spack view --verbose symlink ceed/libceed libceed\nspack view --verbose symlink ceed/petsc petsc\nspack view --verbose symlink ceed/mfem mfem\nspack view --verbose symlink ceed/nek5000 nek5000  and use that to specify the  CEED_DIR ,  MFEM_DIR  and  PETSC_DIR  variables:  cd libCEED/examples/ceed\nmake CEED_DIR=../../ceed/libceed\n./ex1 -ceed /cpu/self\ncd mfem\nmake CEED_DIR=../../../ceed/libceed MFEM_DIR=../../../ceed/mfem\n./bp1 -no-vis -o 2 -ceed /cpu/self\n./bp3 -no-vis -o 2 -ceed /cpu/self\ncd ../petsc\nmake CEED_DIR=../../../ceed/mfem PETSC_DIR=../../../ceed/petsc\n./bp1 -degree 2 -ceed /cpu/self", 
            "title": "Building libCEED-based Applications"
        }, 
        {
            "location": "/ceed-1.0/#gpu-demo", 
            "text": "Below is the full set of commands to install the CEED distribution on a\nGPU-capable machine and then use its libCEED GPU kernels to accelerate MFEM,\nPETSc and Nek examples. Note that these are very different codes (C++, C, F77)\nwhich can nevertheless take advanatage through libCEED of a common set of GPU\nkernels.  The  setenv  commands below assume  csh / tcsh . We strongly recommend to add a packages.yaml  file in order to avoid long compile times, see  Tips and\nTroubleshooting .  # Install CEED 1.0 distribution via Spack\ngit clone git@github.com:spack/spack.git\ncd spack\nspack install ceed+cuda\n\n# Setup CEED component directories\nsetenv CEED_DIR  `spack location -i libceed`\nsetenv MFEM_DIR  `spack location -i mfem`\nsetenv PETSC_DIR `spack location -i petsc`\nsetenv NEK5K_DIR `spack location -i nek5000`\n\n# Clean OCCA cache\n# rm -rf ~/.occa\n\n# Clone libCEED examples directory as proxy for libCEED-based codes\ngit clone git@github.com:CEED/libCEED.git\nmv libCEED/examples ceed-examples\nrm -rf libCEED\n\n# libCEED examples on CPU and GPU\ncd ceed-examples/ceed\nmake\n./ex1 -ceed /cpu/self\n./ex1 -ceed /gpu/occa\ncd ../..\n\n# MFEM+libCEED examples on CPU and GPU\ncd ceed-examples/mfem\nmake\n./bp1 -ceed /cpu/self -no-vis\n./bp1 -ceed /gpu/occa -no-vis\ncd ../..\n\n# PETSc+libCEED examples on CPU and GPU\ncd ceed-examples/petsc\nmake\n./bp1 -ceed /cpu/self\n./bp1 -ceed /gpu/occa\ncd ../..\n\n# Nek+libCEED examples on CPU and GPU\ncd ceed-examples/nek5000\n./make-nek-examples.sh\n./run-nek-example.sh -ceed /cpu/self -b 3\n./run-nek-example.sh -ceed /gpu/occa -b 3\ncd ../..", 
            "title": "GPU demo"
        }, 
        {
            "location": "/ceed-1.0/#spack-for-beginners", 
            "text": "Spack is a package manager for scientific software that supports multiple\nversions, configurations, platforms, and compilers.  While Spack does not change the build system that already exists in each CEED\ncomponent, it coordinates the dependencies between these components and enables\nthem to be built with the same compilers and options.  If you are new to Spack, here are some Spack commands and options that you may\nfind useful:    Spack is a set of Python scripts so there is nothing to install!\n  Just download with  git clone https://github.com/spack/spack.git  and add\n   spack/bin  to your path with the following commands: \n   . share/spack/setup-env.sh  for  bash / zsh  or  \n   sh\n  setenv SPACK_ROOT `pwd`; source $SPACK_ROOT/share/spack/setup-env.csh \n  for  csh / tcsh .    Spack should automatically locate the standard compilers on your system.  Use\n   spack compilers  to list the ones that have been found. If you need to\n  configure additional compilers, you can do that through the config file,\n   ~/.spack/compilers.yaml , or the platform-specific config file,\n   ~/.spack/ platform /compilers.yaml . Some examples of such files are\n  provided below. Check the  Spack documentation  for additional\n  details.    Spack likes to build all of its packages. The file  ~/.spack/packages.yaml ,\n  and similarly the platform-specific,  ~/.spack/ platform /packages.yaml ,\n  allow you to list the packages already installed on your system for Spack to\n  use instead of compiling them itself. Some examples are provided below.    Skip the  -v  option of  spack install  to see only a summary for the building\n  of each package (as opposed to the compilation of individual files):  spack\n  install ceed .  You can still turn the detailed build output on and off by\n  pressing the  v  key in the Spack terminal.    To troubleshoot the spack install process:  spack --debug --verbose install ceed .    To do a dry run of the spack install process:  spack install --fake ceed .\n  Note that you will have to run  spack uninstall --all  to clean up after this.    To see the specific packages that will be installed for a particular package,\n  e.g.  ceed , use:  spack spec -I ceed .    To see the list of all installed packages:  spack find .    To list the location where all different versions of the  ceed  package were\n  installed:  spack find --long --paths ceed . Alternatively, for a specific\n  version you can use  spack location --install-dir ceed .    To uninstall a package, e.g. mfem, including all packages that depend on it:\n   spack uninstall --all --dependents mfem , or  spack uninstall /qzn2u7t  for a\n  particular hash.    To uninstall  all  packages that were ever installed by Spack:  spack\n  uninstall --all .  In this case you may also want to clear the caches that\n  Spack maintains with:  spack clean -a .", 
            "title": "Spack for Beginners"
        }, 
        {
            "location": "/ceed-1.0/#tips-and-troubleshooting", 
            "text": "", 
            "title": "Tips and Troubleshooting"
        }, 
        {
            "location": "/ceed-1.0/#building-on-a-mac", 
            "text": "The file  darwin-x86_64-packages.yaml  provides a\nsample  packages.yaml  file based on  Homebrew , that should work on\nmost Macs. (You can use MacPorts instead of Homebrew if you prefer.)  packages:\n    all:\n        compiler: [clang]\n        providers:\n            blas: [veclibfort]\n            lapack: [veclibfort]\n            mpi: [openmpi]\n    openmpi:\n        paths:\n            openmpi@3.0.0: ~/brew\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.10.2: ~/brew\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.1.85: /usr/local/cuda\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /opt/X11\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /opt/X11\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.10: /usr\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.11: /usr\n        buildable: False  The packages in  ~/brew  were installed with  brew install package .  If you\ndon't have Homebrew, you can install it and the needed tools with:  git clone https://github.com/Homebrew/brew.git\ncd brew\nbin/brew install openmpi cmake python zlib  The packages in  /usr  are provided by Apple and come pre-built with Mac OS\nX. The  cuda  package is provided by NVIDIA and should be installed separately\nby downloading it from  NVIDIA . We are using the Clang compiler,\nOpenMPI, and Apple's BLAS/LAPACK accelerator library.", 
            "title": "Building on a Mac"
        }, 
        {
            "location": "/ceed-1.0/#building-on-a-linux-desktop", 
            "text": "The file  linux-rhel7-x86_64-packages.yaml \nprovides a sample  packages.yaml  file that can be adapted to work on most Linux\ndesktops (this particular file was tested on RHEL7).  packages:\n    all:\n        compiler: [gcc]\n        providers:\n            mpi: [openmpi]\n            blas: [netlib-lapack]\n            lapack: [netlib-lapack]\n    netlib-lapack:\n        paths:\n            netlib-lapack@system: /usr/lib64\n        buildable: False\n    openmpi:\n        paths:\n            openmpi@3.0.0: ~/local\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.10.2: ~/local\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.1.85: ~/local/cuda\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n             python@2.7.14: /usr\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.11: /usr/lib64\n        buildable: False  The above file uses user-installed OpenMPI, CMake and CUDA packages, with the\nrest of the CEED prerequisites installed via the  yum package manager .", 
            "title": "Building on a Linux Desktop"
        }, 
        {
            "location": "/ceed-1.0/#building-at-llnls-computing-center", 
            "text": "", 
            "title": "Building at LLNL's Computing Center"
        }, 
        {
            "location": "/ceed-1.0/#toss3-platforms", 
            "text": "The file  toss_3_x86_64_ib-packages.yaml  is an\nexample of a  packages.yaml  file for the TOSS3 system type at LLNL's  Livermore\nComputing  center.  packages:\n    all:\n        compiler: [intel, gcc, clang, pgi]\n        providers:\n            mpi: [mvapich2, mpich, openmpi]\n            blas: [intel-mkl, openblas]\n            lapack: [intel-mkl, openblas]\n    intel-mkl:\n        paths:\n            intel-mkl@2018.0.128: /usr/tce/packages/mkl/mkl-2018.0\n        buildable: False\n    mvapich2:\n        paths:\n            mvapich2@2.2%intel@18.0.1: /usr/tce/packages/mvapich2/mvapich2-2.2-intel-18.0.1\n            mvapich2@2.2%gcc@4.9.3: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-4.9.3\n            mvapich2@2.2%gcc@7.1.0: /usr/tce/packages/mvapich2/mvapich2-2.2-gcc-7.1.0\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.8.2: /usr/tce/packages/cmake/cmake-3.8.2\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14: /usr/tce/packages/python/python-2.7.14\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.7: /usr\n        buildable: False  The above file can be used to build CEED with different compilers (Intel being\nthe default), for example:  spack install ceed%gcc~petsc  A corresponding  compilers.yaml  file for the TOSS3 platform can be found here: toss_3_x86_64_ib-compilers.yaml .", 
            "title": "TOSS3 Platforms"
        }, 
        {
            "location": "/ceed-1.0/#coral-early-access-platforms", 
            "text": "The file  blueos_3_ppc64le_ib-packages.yaml \nis an example of a  packages.yaml  file for the CORAL early access systems at\nLLNL's  Livermore Computing  center (this particular file is for the\nRay machine).  packages:\n    all:\n        compiler: [xl_r, xl, gcc, clang, pgi]\n        providers:\n            mpi: [spectrum-mpi]\n            blas: [essl]\n            lapack: [netlib-lapack]\n    essl:\n        paths:\n            essl@6.1.0: /usr/tcetmp/packages/essl/essl-6.1.0\n        variants: threads=none\n        version: [6.1.0]\n        buildable: False\n    veclibfort:\n        buildable: False\n    intel-parallel-studio:\n        buildable: False\n    intel-mkl:\n        buildable: False\n    atlas:\n        buildable: False\n    openblas:  # OpenBLAS can be built only with gcc\n        buildable: False\n    netlib-lapack: # prefer netlib-lapack with '+external-blas' and '~lapacke' variant\n        variants: +external-blas~lapacke\n    spectrum-mpi:\n        paths:\n            spectrum-mpi@2017-04-03%xl_r@13.1.7-beta3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.03.21\n            spectrum-mpi@2017-04-03%xl_r@13.1.7-beta2: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-xl-beta-2018.02.22\n            spectrum-mpi@2017-04-03%gcc@4.9.3: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-gcc-4.9.3\n            spectrum-mpi@2017-04-03%clang@3.8.0: /usr/tce/packages/spectrum-mpi/spectrum-mpi-2017.04.03-clang-coral-2018.02.09\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.9.2: /usr/tce/packages/cmake/cmake-3.9.2\n        version: [3.9.2]\n        buildable: False\n    cuda:\n        paths:\n            cuda@9.0.176: /usr/tce/packages/cuda/cuda-9.0.176\n            cuda@9.1.85: /usr/tce/packages/cuda/cuda-9.1.85\n        version: [9.0.176, 9.1.85]\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14: /usr/tcetmp/packages/python/python-2.7.14\n        version: [2.7.14]\n        buildable: False  A corresponding  compilers.yaml  file can be found here: blueos_3_ppc64le_ib-compilers.yaml .", 
            "title": "CORAL Early Access Platforms"
        }, 
        {
            "location": "/ceed-1.0/#building-at-nersc", 
            "text": "", 
            "title": "Building at NERSC"
        }, 
        {
            "location": "/ceed-1.0/#cori", 
            "text": "The file  cori-packages.yaml  is an example of a packages.yaml  file for the Cori system at  NERSC .  packages:\n    all:\n        compiler: [gcc@5.2.0, intel/16.0.3.210]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-haswell: cray-mpich\n            mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-haswell: cray-mpich\n        buildable: False\n\n    cmake:\n        modules:\n            cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-haswell: cmake\n            cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-haswell: cmake\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14%gcc@5.2.0 arch=cray-CNL-haswell: /usr\n            python@2.7.14%intel@16.0.3.210 arch=cray-CNL-haswell: /usr\n        buildable: False", 
            "title": "Cori"
        }, 
        {
            "location": "/ceed-1.0/#edison", 
            "text": "The file  edison-packages.yaml  is an example of a packages.yaml  file for the Edison system at  NERSC .  packages:\n    all:\n        compiler: [gcc@5.2.0, intel/16.0.3.210]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.0%gcc@5.2.0 arch=cray-CNL-ivybridge: cray-mpich\n            mpich@7.6.0%intel@16.0.3.210 arch=cray-CNL-ivybridge: cray-mpich\n        buildable: False\n\n    cmake:\n        modules:\n            cmake@3.8.2%gcc@5.2.0 arch=cray-CNL-ivybridge: cmake\n            cmake@3.8.2%intel@16.0.3.210 arch=cray-CNL-ivybridge: cmake\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.14%gcc@5.2.0 arch=cray-CNL-ivybridge: /usr\n            python@2.7.14%intel@16.0.3.210 arch=cray-CNL-ivybridge: /usr\n        buildable: False", 
            "title": "Edison"
        }, 
        {
            "location": "/ceed-1.0/#building-at-alcf", 
            "text": "", 
            "title": "Building at ALCF"
        }, 
        {
            "location": "/ceed-1.0/#theta", 
            "text": "The file  theta-packages.yaml  is an example of a packages.yaml  file for the Theta system at  ALCF .  packages:\n    all:\n        compiler: [intel@16.0.3.210, gcc@5.3.0]\n        providers:\n            mpi: [mpich]\n    intel-mkl:\n        paths:\n            intel-mkl@16.0.3.210%intel@16.0.3.210 arch=cray-CNL-mic_knl: /opt/intel\n        buildable: False\n    mpich:\n        modules:\n            # requires 'module load cce' otherwise gives parsing error\n            mpich@7.6.3%gcc@5.3.0 arch=cray-CNL-mic_knl: cray-mpich/7.6.3\n            mpich@7.6.3%intel@16.0.3.210 arch=cray-CNL-mic_knl: cray-mpich/7.6.3\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.5.2%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr\n            cmake@3.5.2%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        paths:\n            python@2.7.13%gcc@5.3.0 arch=cray-CNL-mic_knl: /usr\n            python@2.7.13%intel@16.0.3.210 arch=cray-CNL-mic_knl: /usr\n        buildable: False", 
            "title": "Theta"
        }, 
        {
            "location": "/ceed-1.0/#building-at-olcf", 
            "text": "", 
            "title": "Building at OLCF"
        }, 
        {
            "location": "/ceed-1.0/#titan", 
            "text": "The file  titan-packages.yaml  is an example of a packages.yaml  file for the Titan system at  OLCF .  packages:\n    all:\n        compiler: [cce/8.6.4]\n        providers:\n            mpi: [mpich]\n    mpich:\n        modules:\n            mpich@7.6.3%cce@8.6.4 arch=cray-CNL-interlagos: cray-mpich\n        buildable: False\n\n    cmake:\n        paths:\n            cmake@3.9.0%cce@8.6.4: /autofs/nccs-svm1_sw/titan/.swci/0-login/opt/spack/20170612/linux-suse_linux11-x86_64/gcc-4.3.4/cmake-3.9.0-owxiriblogovogl5zbrg45ulm3ln34cx/bin\n        buildable: False\n    libx11:\n        paths:\n            libx11@system: /usr\n        version: [system]\n        buildable: False\n    libxt:\n        paths:\n            libxt@system: /usr\n        version: [system]\n        buildable: False\n    python:\n        modules:\n            python@2.7.9%cce@8.6.4 arch=cray-CNL-interlagos: python\n        buildable: False\n    zlib:\n        paths:\n            zlib@1.2.17: /usr/lib64\n        buildable: False  The default install of curl on Titan does not support ssl, so you need to add\nthe path of a newer install to your  PATH :  module show curl  # get the /path/to/curl/bin/dir\nexport PATH=/path/to/curl/bin/dir:$PATH  Additional issues on Titan: Spack does not support  cray-libsci  for BLAS/LAPACK\n(there is no 'dummy package' for  cray-libsci yet ); the Cray\ncompiler,  cce , fails to build  openblas  (it does not e.g.  recognize the -m64  flag, there may be other issues).  With these caveats, the CEED metapackage can be installed with:  ./bin/spack --debug --verbose install -v ceed%pgi@17.9.0 target=interlagos  Note that  spack  will hang if you redirect  std[err|out]  to a file (  log )\nand background the command (by appending an  ).", 
            "title": "Titan"
        }, 
        {
            "location": "/ceed-1.0/#installing-cuda", 
            "text": "Several CEED packages depend on CUDA: OCCA, MAGMA and libCEED.    To build these, add the  cuda  variant to the Spack build:\n   sh\n  spack install ceed+cuda    You will need to have the NVIDIA CUDA SDK and driver installed on your system,\n  see  developer.nvidia.com , and specify it in the  packages.yaml \n  file. See, for example, the  cuda  section in  Building on a Mac ,\n  or the  linux-rhel7-x86_64-packages.yaml  file.", 
            "title": "Installing CUDA"
        }, 
        {
            "location": "/ceed-code/", 
            "text": "CEED APIs\n\n\nCEED is building on the efforts of the \nNek5000\n, \nMFEM\n,\n\nMAGMA\n, \nOCCA\n and \nPETSc\n projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.\n\n\nLow level API\n\n\nThe CEED low-level API, \nlibCEED\n operates with the foundational\ncomponents of finite element operators, described by the following\ndecomposition:\n\n\n\n\nTo achieve high-performance, it is critical to take advantage of the\ntensor-product structure of both the finite element basis and the quadrature\nrule to efficiently apply the action of $B$ without necessarily computing its\nentries. This is generally know as \nsum factorization\n.  In the\ncase where we precompute and store the $D$ matrix, we call the algorithm\n\npartial assembly\n.\n\n\nSince libCEED is based on a common operator description at algebraic level, it\ncan also be used as the foundation for an efficient high-order \noperator\nformat\n.\n\n\nFor more information of the CEED low level API see the \nlibCEED\n page.\n\n\nHigh level API\n\n\nThe CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.\n\n\nGiven such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.\n\n\nThe global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.\n\n\nThis API is currently under development. Stay tuned for more details...\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/ceed-code/#ceed-apis", 
            "text": "CEED is building on the efforts of the  Nek5000 ,  MFEM , MAGMA ,  OCCA  and  PETSc  projects to develop\napplication program interfaces (APIs), both at high-level and at low-level.\nMultiple APIs are necessary to enable application to take advantage of\nCEED-developed high-order technologies at the level they are comfortable with.\nIn addition, our high-level API will call internally low-level API\nfunctionality.", 
            "title": "CEED APIs"
        }, 
        {
            "location": "/ceed-code/#low-level-api", 
            "text": "The CEED low-level API,  libCEED  operates with the foundational\ncomponents of finite element operators, described by the following\ndecomposition:   To achieve high-performance, it is critical to take advantage of the\ntensor-product structure of both the finite element basis and the quadrature\nrule to efficiently apply the action of $B$ without necessarily computing its\nentries. This is generally know as  sum factorization .  In the\ncase where we precompute and store the $D$ matrix, we call the algorithm partial assembly .  Since libCEED is based on a common operator description at algebraic level, it\ncan also be used as the foundation for an efficient high-order  operator\nformat .  For more information of the CEED low level API see the  libCEED  page.", 
            "title": "Low level API"
        }, 
        {
            "location": "/ceed-code/#high-level-api", 
            "text": "The CEED high-level API operates with global discretization concepts,\nspecifying a global mesh, finite element spaces and PDE operators to be\ndiscretized with the point-wise physics representing the coefficients in these\noperators.  Given such inputs, CEED provides efficient discretization and evaluation of the\nrequested operators, without the need for the application to be concerned with\nelement-level operations.\nInternally, the high-level API relies on CEED's low-level API described below.  The global perspective also allows CEED to provide general unstructured adaptive\nmesh refinement support, with minimal impact in the application code.  This API is currently under development. Stay tuned for more details...  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "High level API"
        }, 
        {
            "location": "/libceed/", 
            "text": "libCEED\n \n\n\nlibCEED is a high-order API library, that provides a common algebraic low-level\noperator description, allowing a wide variety of applications to take advantage\nof the efficient operator evaluation algorithms in the different CEED packages\n(from a single source).\n\n\nOne of the challenges with high-order methods is that a global sparse matrix is\nno longer a good representation of a high-order linear operator, both with\nrespect to the FLOPs needed for its evaluation, as well as the memory transfer\nneeded for a matvec. Thus, high-order methods require a new \"format\" that still\nrepresents a linear (or more generally non-linear) operator, but not through a\nsparse matrix.\n\n\nThe goal of libCEED is to propose such a format, as well as supporting\nimplementations and data structures, that enable efficient operator evaluation\non a variety of computational device types (CPUs, GPUs, etc.). This new operator\ndescription is based on algebraically factored form, which is easy to\nincorporate in a wide variety of applications, without significant refactoring\nof their own discretization infrastructure.\n\n\nFor more information visit \nhttps://github.com/CEED/libCEED\n.", 
            "title": "libCEED"
        }, 
        {
            "location": "/libceed/#libceed", 
            "text": "libCEED is a high-order API library, that provides a common algebraic low-level\noperator description, allowing a wide variety of applications to take advantage\nof the efficient operator evaluation algorithms in the different CEED packages\n(from a single source).  One of the challenges with high-order methods is that a global sparse matrix is\nno longer a good representation of a high-order linear operator, both with\nrespect to the FLOPs needed for its evaluation, as well as the memory transfer\nneeded for a matvec. Thus, high-order methods require a new \"format\" that still\nrepresents a linear (or more generally non-linear) operator, but not through a\nsparse matrix.  The goal of libCEED is to propose such a format, as well as supporting\nimplementations and data structures, that enable efficient operator evaluation\non a variety of computational device types (CPUs, GPUs, etc.). This new operator\ndescription is based on algebraically factored form, which is easy to\nincorporate in a wide variety of applications, without significant refactoring\nof their own discretization infrastructure.  For more information visit  https://github.com/CEED/libCEED .", 
            "title": "libCEED "
        }, 
        {
            "location": "/mfem/", 
            "text": "MFEM\n \n\n\nMFEM is a free, lightweight, scalable C++ library for finite element methods.\n\n\nIts goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.\n\n\nIts features include:\n\n\n\n\nArbitrary high-order finite element \nmeshes\n\nand \nspaces\n.\n\n\nWide variety\n of finite element discretization approaches.\n\n\nConforming and nonconforming \nadaptive mesh refinement\n.\n\n\nScalable to \nhundreds of thousands\n of cores.\n\n\n... and \nmany more\n.\n\n\n\n\nMFEM is being developed in \nCASC\n,\n\nLLNL\n and is freely available under LGPL 2.1.\n\n\nIn CEED, MFEM is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MFEM website: \nhttp://mfem.org\n.", 
            "title": "MFEM"
        }, 
        {
            "location": "/mfem/#mfem", 
            "text": "MFEM is a free, lightweight, scalable C++ library for finite element methods.  Its goal is to enable high-performance scalable finite element discretization\nresearch and application development on a wide variety of platforms, ranging\nfrom laptops to exascale supercomputers.  Its features include:   Arbitrary high-order finite element  meshes \nand  spaces .  Wide variety  of finite element discretization approaches.  Conforming and nonconforming  adaptive mesh refinement .  Scalable to  hundreds of thousands  of cores.  ... and  many more .   MFEM is being developed in  CASC , LLNL  and is freely available under LGPL 2.1.  In CEED, MFEM is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the MFEM website:  http://mfem.org .", 
            "title": "MFEM "
        }, 
        {
            "location": "/nek/", 
            "text": "Nek\n \n\n\nNek5000/NekCEM is an open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications, including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.\n\n\nIt features state-of-the-art, scalable, high-order spectral element-based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.\n\n\nNek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.\n\n\n\n\nReactor analysis\n\n\nAerospace application\n\n\nEngine application\n\n\nOcean modeling\n\n\nTurbulence modeling\n\n\nElectromagnetics modeling\n\n\nDrift-diffusion  modeling\n\n\n\n\nNek5000 and NekCEM are freely available under a BSD license.\n\n\nIn CEED, Nek is a main component of the efforts in the \nApplications\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the Nek5000 website: \nhttps://nek5000.mcs.anl.gov\n.", 
            "title": "Nek"
        }, 
        {
            "location": "/nek/#nek", 
            "text": "Nek5000/NekCEM is an open-source simulation-software package that delivers highly accurate\nsolutions for a wide range of scientific applications, including fluid flow, thermal convection,\ncombustion, magnetohydrodynamics, and electromagnetics.  It features state-of-the-art, scalable, high-order spectral element-based algorithms that are\nfast and efficient on platforms ranging from laptops to the world\u2019s fastest computers.  Nek5000/NekCEM's users include over 320 scientists and engineers in academia, laboratories and\nindustry. We highlight a few of the many applications using Nek5000/NekCEM today.   Reactor analysis  Aerospace application  Engine application  Ocean modeling  Turbulence modeling  Electromagnetics modeling  Drift-diffusion  modeling   Nek5000 and NekCEM are freely available under a BSD license.  In CEED, Nek is a main component of the efforts in the  Applications  and  Finite Element  thrusts.  For more information, see the Nek5000 website:  https://nek5000.mcs.anl.gov .", 
            "title": "Nek "
        }, 
        {
            "location": "/gslib/", 
            "text": "GSLIB\n\n\nGSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications.  It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers.\n\n\nAny global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix.  GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$.  Users simply provide the local-to-global map $j(i)$ on each processor.  GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms.  The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks.\n\n\nGSLIB supports\n\n\n\n\nStencils of arbitrary width\n\n\n64-bit indexing for global addressing\n\n\nshort / long ints, floats, doubles\n\n\narbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$\n\n\nGPUDirect communication (beta-version)\n\n\ngather-scatter on arbitrary m-tuples (vector fields)\n\n\n\n\nAdditional features include:\n\n\n\n\nXXT solver (parallel direct solver for coarse-grid problems)\n\n\nAMG solver (design for solving coarse-grid problems in parallel)\n\n\nRobust and efficient spectral/finite element interpolation in parallel.\n\n\n\n\nApplications and libraries using GSLIB include \nNek5000\n, \nNekCEM\n,\n\nNektar++\n, and \nMOAB\n.\n\n\nIn CEED, GSLIB is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the GSLIB GitHub repository: \nhttps://github.com/gslib/gslib\n.\n\n\nMathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "GSLIB"
        }, 
        {
            "location": "/gslib/#gslib", 
            "text": "GSLIB is a library for Gather/Scatter-type nearest neighbor data exchanges for SEM, FEM, and finite-difference applications.  It can also be used for efficient transpose and reduction operations and for smoothing, prolongation, and restriction in AMG and other solvers.  Any global-to-local map, $l(i) = g(j(i))$, can be expressed as the matrix-vector product $l=Qg$, where $Q$ is a Boolean matrix.  GSLIB supports the parallel matrix-vector products $Q$, $Q^T$, and $QQ^T$.  Users simply provide the local-to-global map $j(i)$ on each processor.  GSLIB identifies shared global vertices across multiple processors and sets up the required communication exchange to use the fastest of three available algorithms.  The user does not need to know or express data locality to efficiently establish communication on anywhere from one to millions of ranks.  GSLIB supports   Stencils of arbitrary width  64-bit indexing for global addressing  short / long ints, floats, doubles  arbitrary associative/commutative operators (+,*,min/max) for reduction, $Q^T$  GPUDirect communication (beta-version)  gather-scatter on arbitrary m-tuples (vector fields)   Additional features include:   XXT solver (parallel direct solver for coarse-grid problems)  AMG solver (design for solving coarse-grid problems in parallel)  Robust and efficient spectral/finite element interpolation in parallel.   Applications and libraries using GSLIB include  Nek5000 ,  NekCEM , Nektar++ , and  MOAB .  In CEED, GSLIB is primarily involved in the efforts of the  Software  thrust.  For more information, see the GSLIB GitHub repository:  https://github.com/gslib/gslib .  MathJax.Hub.Config({TeX: {equationNumbers: {autoNumber: \"all\"}}, tex2jax: {inlineMath: [['$','$']]}});", 
            "title": "GSLIB"
        }, 
        {
            "location": "/magma/", 
            "text": "MAGMA\n\n\n\n\n\nMAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.\n\n\n\n\nMAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.\n\n\nHybrid Algorithms\n\n\nMAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.\n\n\n\n\nIn either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.\n\n\nPerformance and Energy Efficiency\n\n\nMAGMA solvers run close to the machine's peak performance. The LU factorization for\nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.\n\n\n\n\nMAGMA Batched\n\n\nMAGMA Batched targets small linear algebra operations. Small computational tasks are difficult\nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.\n\n\n\n\nMAGMA Sparse\n\n\nMAGMA Sparse targets the development of high-performance sparse linear algebra operations on\naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace\niterative solvers, eigensolvers, and preconditioners.\n\n\n\n\nMAGMA Development\n\n\nMAGMA is being developed at the \nInnovative Computing Laboratory\n\nof the \nUniversity of Tennessee\n.\n\n\nIn CEED, MAGMA is primarily involved in the efforts of the \nSoftware\n, \nHardware\n and \nFinite Element\n thrusts.\n\n\nFor more information, see the MAGMA website: \nhttp://icl.cs.utk.edu/magma\n.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#magma", 
            "text": "MAGMA (Matrix Algebra on GPU and Multicore Architectures) is a collection of\nnext generation linear algebra libraries for heterogeneous architectures. MAGMA\nis designed and implemented by the team that developed LAPACK and ScaLAPACK,\nincorporating the latest developments in hybrid synchronization- and\ncommunication-avoiding algorithms, as well as dynamic runtime\nsystems. Interfaces for the current LAPACK and BLAS standards are supported to\nallow computational scientists to seamlessly port any linear algebra reliant\nsoftware components to heterogeneous architectures.   MAGMA allows applications to fully exploit the power of current heterogeneous\nsystems of multi/many-core CPUs and multi-GPUs to deliver the fastest possible\ntime to accurate solution within given energy constraints.", 
            "title": "MAGMA"
        }, 
        {
            "location": "/magma/#hybrid-algorithms", 
            "text": "MAGMA uses a hybridization methodology where algorithms of interest are split\ninto tasks of varying granularity and their execution scheduled over the\navailable hardware components.  Scheduling can be static or dynamic.   In either case, small non-parallelizable tasks, often on the critical path, are\nscheduled on the CPU, and larger more parallelizable ones, often Level 3 BLAS\nare scheduled on accelerators.", 
            "title": "Hybrid Algorithms"
        }, 
        {
            "location": "/magma/#performance-and-energy-efficiency", 
            "text": "MAGMA solvers run close to the machine's peak performance. The LU factorization for\nexample, illustrated below, runs as fast as the GPU can run matrix-matrix multiplications (GEMM),\nas the small tasks on the critical path are offloaded to the CPU and overlapped with the GEMMs\non the GPU.", 
            "title": "Performance and Energy Efficiency"
        }, 
        {
            "location": "/magma/#magma-batched", 
            "text": "MAGMA Batched targets small linear algebra operations. Small computational tasks are difficult\nto parallelize, but applications usually require the computation of many small problems, which\ncan be grouped together (batched) and executed very efficiently. MAGMA Batched is being extended\nnow under the CEED project to support tensor data structures and tensor contractions for high-order\nmethods.", 
            "title": "MAGMA Batched"
        }, 
        {
            "location": "/magma/#magma-sparse", 
            "text": "MAGMA Sparse targets the development of high-performance sparse linear algebra operations on\naccelerators - from low-level kernels like SpMVs and SpMM, to higher-level Krylov subspace\niterative solvers, eigensolvers, and preconditioners.", 
            "title": "MAGMA Sparse"
        }, 
        {
            "location": "/magma/#magma-development", 
            "text": "MAGMA is being developed at the  Innovative Computing Laboratory \nof the  University of Tennessee .  In CEED, MAGMA is primarily involved in the efforts of the  Software ,  Hardware  and  Finite Element  thrusts.  For more information, see the MAGMA website:  http://icl.cs.utk.edu/magma .", 
            "title": "MAGMA Development"
        }, 
        {
            "location": "/occa/", 
            "text": "OCCA\n \n\n\nOCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and lets the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.\n\n\nOCCA abstracts the variety of device programming languages into one kernel\nlanguage, the OCCA kernel language (OKL). OKL minimally extends C and restricts\nthe user to write parallel code that is JIT compiled.\n\n\nOCCA is freely available under an MIT license.\n\n\nIn CEED, OCCA is primarily involved in the efforts of the \nSoftware\n and\n\nFinite Element\n thrusts.\n\n\nFor more information, see the OCCA website: \nhttp://libocca.org\n.", 
            "title": "OCCA"
        }, 
        {
            "location": "/occa/#occa", 
            "text": "OCCA is an open-source library that facilitates programming in an environment\ncontaining different types of devices. It abstracts devices and lets the user\npick at run-time, for example: CPUs, GPUs, Intel\u2019s Xeon Phi, FPGAs.  OCCA abstracts the variety of device programming languages into one kernel\nlanguage, the OCCA kernel language (OKL). OKL minimally extends C and restricts\nthe user to write parallel code that is JIT compiled.  OCCA is freely available under an MIT license.  In CEED, OCCA is primarily involved in the efforts of the  Software  and Finite Element  thrusts.  For more information, see the OCCA website:  http://libocca.org .", 
            "title": "OCCA "
        }, 
        {
            "location": "/petsc/", 
            "text": "PETSc\n\n\nPETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.\n\n\nAs part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.\n\n\nPETSc is freely available under a BSD license.\n\n\nIn CEED, PETSc is primarily involved in the efforts of the \nSoftware\n thrust.\n\n\nFor more information, see the PETSc website: \nhttps://www.mcs.anl.gov/petsc/\n.", 
            "title": "PETSc"
        }, 
        {
            "location": "/petsc/#petsc", 
            "text": "PETSc is a scalable package for solving differential and algebraic equations.\nIt supports MPI, and GPUs through CUDA or OpenCL, as well as hybrid MPI-GPU\nparallelism.  As part of CEED, the PETSc project will coordinate the development of expressive\ninterfaces for efficient and robust solution of the algebraic equations\nappearing in high-order/spectral element methods.  This will include multilevel\nsolvers that work with unassembled representations of linear operators.  PETSc is freely available under a BSD license.  In CEED, PETSc is primarily involved in the efforts of the  Software  thrust.  For more information, see the PETSc website:  https://www.mcs.anl.gov/petsc/ .", 
            "title": "PETSc"
        }, 
        {
            "location": "/pumi/", 
            "text": "PUMI\n \n\n\nPUMI\n is an unstructured, distributed mesh data management system designed for\n massively parallel computing.\n\n\nPUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:\n\n\n\n\nPCU\n for phased message passing and thread management.\n\n\nGMI\n for geometric model interface.\n\n\nMDS\n for unstructured mesh representation.\n\n\nAPF Mesh\n for partition model and distributed mesh management.\n\n\nAPF_Field\n for field management.\n\n\n\n\nPUMI is being developed at RPI's \nScientific Computation Research\nCenter\n and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.\n\n\nIn CEED, PUMI is primarily involved in the efforts of the \nFinite Element\n\nand \nSoftware\n thrusts.\n\n\nFor more information, see the \nPUMI documents\n.", 
            "title": "PUMI"
        }, 
        {
            "location": "/pumi/#pumi", 
            "text": "PUMI  is an unstructured, distributed mesh data management system designed for\n massively parallel computing.  PUMI supports a full range of operations on unstructured meshes on massively\nparallel computers consisting of five libraries:   PCU  for phased message passing and thread management.  GMI  for geometric model interface.  MDS  for unstructured mesh representation.  APF Mesh  for partition model and distributed mesh management.  APF_Field  for field management.   PUMI is being developed at RPI's  Scientific Computation Research\nCenter  and is currently being used on projects\nsponsored by the DOE, NSF, Army, NASA, IBM and several companies.  In CEED, PUMI is primarily involved in the efforts of the  Finite Element \nand  Software  thrusts.  For more information, see the  PUMI documents .", 
            "title": "PUMI "
        }, 
        {
            "location": "/thrusts/", 
            "text": "R\nD Thrusts\n\n\nCEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R\nD thrusts\nfocused on these customers and tied together by the foundational \nfinite element\nthrust\n.\n\n\n\n\nThe specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the \nOutreach page\n.\n\n\n\n\nApplications Thrust (AP)\n\n\nThe goal of CEED's Applications thrust is to impact a wide range of \nECP\napplication teams\n through focused\none-on-one interactions, facilitated by CEED application liaisons, as well as\nthrough one-to-many interactions, based on the development of easy-to-use\ndiscretization libraries for high-order finite element methods.\n\n\n\n\nHardware Thrust (HW)\n\n\nThe goal of CEED's Hardware thrust is to build a two-way (\npull-and-push\n)\ncollaboration with vendors, where the CEED team will develop hardware-aware\ntechnologies (\npull\n) to understand performance bottlenecks and take advantage\nof inevitable hardware trends, and vendor interactions to seek (\npush\n) impact\nand improve hardware designs within the ECP scope.\n\n\n\n\nSoftware Thrust (SW)\n\n\nThe goal of CEED's Software thrust is to participate in the development of\nsoftware libraries and frameworks of general interest to the scientific\ncomputing community, facilitate collaboration between CEED software packages,\nenable integration into and/or interoperability with overall ECP software\ntechnologies stack, streamline developer and user workflows, maintain testing\nand benchmarking infrastructure, and coordinate CEED software releases.\n\n\n\n\nFinite Elements Thrust (FE)\n\n\nThe goal of CEED's Finite Element thrust is to continue to improve the\nstate-of-the-art spectral element/high-order finite element algorithms and\nkernels in the CEED software targeting exascale architectures, connect and\ncontribute to the efforts of the other thrusts, and lead the development of\ndiscretization libraries, benchmarks and miniapps.", 
            "title": "R&D Thrusts"
        }, 
        {
            "location": "/thrusts/#rd-thrusts", 
            "text": "CEED scientists work closely with hardware vendors, algorithm and software\ndevelopers, and collaborate with application scientists to meet their\nneeds. Our co-design efforts are organized in four interconnected R D thrusts\nfocused on these customers and tied together by the foundational  finite element\nthrust .   The specific goals and responsibilities of each thrust are described below.\nYou can find our publications and related documents on the  Outreach page .", 
            "title": "R&amp;D Thrusts"
        }, 
        {
            "location": "/thrusts/#applications-thrust-ap", 
            "text": "The goal of CEED's Applications thrust is to impact a wide range of  ECP\napplication teams  through focused\none-on-one interactions, facilitated by CEED application liaisons, as well as\nthrough one-to-many interactions, based on the development of easy-to-use\ndiscretization libraries for high-order finite element methods.", 
            "title": "Applications Thrust (AP)"
        }, 
        {
            "location": "/thrusts/#hardware-thrust-hw", 
            "text": "The goal of CEED's Hardware thrust is to build a two-way ( pull-and-push )\ncollaboration with vendors, where the CEED team will develop hardware-aware\ntechnologies ( pull ) to understand performance bottlenecks and take advantage\nof inevitable hardware trends, and vendor interactions to seek ( push ) impact\nand improve hardware designs within the ECP scope.", 
            "title": "Hardware Thrust (HW)"
        }, 
        {
            "location": "/thrusts/#software-thrust-sw", 
            "text": "The goal of CEED's Software thrust is to participate in the development of\nsoftware libraries and frameworks of general interest to the scientific\ncomputing community, facilitate collaboration between CEED software packages,\nenable integration into and/or interoperability with overall ECP software\ntechnologies stack, streamline developer and user workflows, maintain testing\nand benchmarking infrastructure, and coordinate CEED software releases.", 
            "title": "Software Thrust (SW)"
        }, 
        {
            "location": "/thrusts/#finite-elements-thrust-fe", 
            "text": "The goal of CEED's Finite Element thrust is to continue to improve the\nstate-of-the-art spectral element/high-order finite element algorithms and\nkernels in the CEED software targeting exascale architectures, connect and\ncontribute to the efforts of the other thrusts, and lead the development of\ndiscretization libraries, benchmarks and miniapps.", 
            "title": "Finite Elements Thrust (FE)"
        }, 
        {
            "location": "/pubs/", 
            "text": "Publications and Outreach\n\n\n\n\nCEED Documents\n\n\n\n\nCEED's high-order \nBenchmarks\n and \nMiniapps\n.\n\n\nActivities in the \nApplications\n, \nHardware\n, \nSoftware\n and \nFinite Element\n thrusts.\n\n\nCEED-proposed high-order \nOperator\n and \nVisualization\n formats.\n\n\n\n\n\n\nPublications\n\n\n2018\n\n\n\n\nV. Dobrev, P. Knupp, Tz. Kolev, K. Mittal, V. Tomov, \nThe Target-Matrix Optimization Paradigm for High-Order Meshes\n, submitted, \n2018\n.\n\n\nA. Karakus, N. Chalmers, K. Swirydowicz, T. Warburton, \nGPU Acceleration of a High-Order Discontinuous Galerkin Incompressible Flow Solver\n, submitted, \n2018\n.\n\n\nA. Barker, V. Dobrev, J. Gopalakrishnan and T. Kolev, \nA scalable preconditioner for a DPG method\n, \nSIAM J. Sci. Comp.\n, accepted, \n2018\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, \nHigh-Order Multi-Material ALE Hydrodynamics\n,  \nSIAM J. Sci. Comp.\n, 40(1):B32-B58, \n2018\n.\n\n\nV. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov, \nSequential limiting in continuous and discontinuous Galerkin methods for the Euler equations\n, Journal of Computational Physics,\n356:372-390, \n2018\n.\n\n\n\n\n\n\n2017\n\n\n\n\nK. Swirydowicz, N. Chalmers, A. Karakus, T. Warburton, \nAcceleration of tensor-product operations for high-order finite element methods\n, submitted, \n2017\n\n\nJ. Solberg, E. Merzari, H. Yuan, A. Obabko and P. Fischer, S. Lee, J. Lai, M. Delgado, S. J. Lee, and Y. Hassan, \nHigh-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors\n, \nBest Paper Award\n at \nThe 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17)\n, September, \n2017\n.\n\n\nV. Dobrev, Tz. Kolev, C. Lee, V. Tomov and P. Vassilevski, \nAlgebraic Hybridization and Static Condensation with Application to Scalable H(div) Preconditioning\n, submitted, \n2017\n.\n\n\nS. Patel, P. Fisher, M. Min, and A. Tomboulides, \nA Characteristic-based, spectral element method for moving-domain problems\n, \nJ. Sci. Comp.\n, submitted, \n2017\n.\n\n\nI. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J. Dongarra, \nAlgorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices\n, \nParallel Computing (PARCO)\n, submitted, \n2017\n.\n\n\nK. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji, \nWhy is MPI so slow? Analyzing the fundamental limits in implementing MPI-3.1\n, accepted, \nSC'17\n, \n2017\n.\n\n\nP. Fischer, M. Schmitt, and A. Tomboulides, \nRecent developments in spectral element simulations of moving-domain problems\n, vol. 79, \nFields Institute Communications\n, 213\u2013244, \n2017\n.\n\n\nS. Lomperski, A. Obabko, P. Fischer, E. Merzari, and W.D. Pointer, \nJet stability and wall impingement flow field in a thermal striping experiment\n, \nInt. J. Heat Mass Transfer\n, 115A:1125\u2013 1136, \n2017\n.\n\n\nV. Makarashvilia, E. Merzari, A. Obabko, A. Siegel, and P. Fischer, \nA performance analysis of ensemble averaging for high fidelity turbulence simulations at the strong scaling limit\n, \nComp. Phys. Comm.\n, in press, \n2017\n.\n\n\nA. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra, \nFactorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures\n, \nProceedings of the 2017 International Conference on Computational Science, ICCS'17\n, Z\u00fcrich, Switzerland, June 12-14, \nProcedia Computer Science\n, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov, \nSmall Tensor Operations on Advanced Architectures for High-order Applications\n, Technical report UT-EECS-17-749, EECS Department, University of Tennessee, \n2017\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov,\n\nHigh-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation\n, Journal of Computational Physics, 334:102\u2013124, \n2017\n.\n\n\nBazilevs, Y., Kamran, K., Moutsanidis, G., Benson, D. J., \n O\u00f1ate, E, \nA new formulation for air-blast fluid\u2013structure interaction using an immersed approach. Part I: basic methodology and FEM-based simulations\n, Computational Mechanics, 1-18, \n2017\n.\n\n\nAbdelfattah, A., Haidar, A., Tomov, S., and Dongarra, J. \nNovel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs\n, International Conference on Supercomputing (ICS'17), ACM, Chicago, Illinois, pp. 1-10, June 14-16, \n2017\n.\n\n\nIbanez, D., Shephard, M.S.,  \nModifiable Array Data Structures for Mesh Topology\n, SIAM Journal on Scientific Computing, 39(2):C144-C161, \n2017\n.\n\n\nGranzow, B.N., Shephard M.S., Oberai, A.A., \nOutput-based error estimation and mesh adaptation for variational multiscale methods\n,\nComputer Methods in Applied Mechanics and Engineering, Vol. 322, pp. 441-459, \n2017\n.\n\n\n\n\n\n\n2016\n\n\n\n\nE. Merzari, A. Obabko, P. Fischer, N. Halford, J. Walker, A. Siegel, and Y. Q. Yu, \nLarge-scale large eddy simulation of nuclear reactor flows: Issues and perspectives\n, \nNuclear Engineering and Design\n, page 13, Oct., \n2016\n.\n\n\nE. Merzari, P. Fischer, H. Yuan, K. Van Tichelen, S. Keijers, J. De Ridder, J. Degroote, J. Vierendeels, H. Doolaard, V. R. Gopala, and F. Roelofs, \nBenchmark exercise for fluid flow simulations in a liquid metal fast reactor fuel assembly\n, \nNuclear Engineering and Design\n, 298(3):218\u2013228, \n2016\n.\n\n\nV. Dobrev, Tz. Kolev, R. Rieben and V. Tomov, \nMulti-material closure model for high-order finite element Lagrangian hydrodynamics\n, \nInt. J. Numer. Meth. Fluids\n, 82(10), pp. 689\u2013706, \n2016\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nHigh-performance Tensor Contractions for GPUs\n, \nProcedia Computer Science\n, Volume 80, Pages 108-118, ISSN 1877-0509, \n2016\n.\n\n\nM. B.E., Y. Peet, P. Fischer, and J. Lottes, \nA spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations\n, \nJ. Comp. Phys.\n, 307:60\u201393, \n2016\n.\n\n\nM. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min, \nAn MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication\n, \nThe International Journal of High Performance Computing Application\n, 30(3):320\u2013334, \n2016\n.\n\n\nJ. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min, \nNekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations\n, \nSpecial issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing\n, \n2016\n.\n\n\nIbanez, Daniel A., et al. \nPUMI: Parallel unstructured mesh infrastructure.\n ACM Transactions on Mathematical Software (TOMS) 42.3 \n2016\n.\n\n\nSmith, Cameron W., et al. \nIn-memory Integration of Existing Software Components for Parallel Adaptive Unstructured Mesh Workflows.\n Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM, \n2016\n.\n\n\nIbanez, D., Dunn, I., Shephard M.S., \nHybrid MPI-thread parallelization of adaptive mesh operations\n, Parallel Computing, 52:133-143, \n2016\n.\n\n\n\n\n\n\n2015 and earlier\n\n\n\n\nP. Fischer, K. Heisey, and M. Min, \nScaling limits for PDE-based simulation\n, \nIn 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation\n, AIAA 2015-3049, \n2015\n.\n\n\nA. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer, \nErosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches\n, volume 2, pp. 1448\u20131461. \nAmerican Nuclear Society\n, \n2015\n.\n\n\nE. Merzari, P. Fischer, and J. Walker, \nLarge-scale simulation of rod bundles: Coherent structure recognition and stability analysis\n, volume 1. \nAmerican Society of Mechanical Engineers\n, \n2015\n.\n\n\nM. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray, \nEntanglement of two, three and four plasmonically coupled quantum dots\n, \nPhysical Review B\n, 92:125432, \n2015\n.\n\n\nD. A. May, J. Brown, and L. Le Pourhiet. \npTatin3D: High-performance methods for long-term lithospheric dynamics\n, In Proceedings of \nSC14: International Conference for High Performance Computing, Networking, Storage and Analysis\n. ACM, \n2014\n.\n\n\nR. Anderson, V. Dobrev, Tz. Kolev and R. Rieben, \nMonotonicity in high-order curvilinear finite element ALE remap\n, \nInt. J. Numer. Meth. Fluids\n, 77(5), pp. 249\u2013273, \n2014\n.\n\n\nKamran, Kazem, et al. \nA compressible Lagrangian framework for modeling the fluid\u2013structure interaction in the underwater implosion of an aluminum cylinder.\n Mathematical Models and Methods in Applied Sciences 23.02 \n2013\n.\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG solver for H(div) problems\n,  \nSIAM J. Sci. Comp.\n, 34, pp. A3079\u2013A3098, \n2012\n.\n\n\nV. Dobrev, Tz. Kolev and R. Rieben, \nHigh-order curvilinear finite element methods for Lagrangian hydrodynamics\n, \nSIAM J. Sci. Comp.\n, 34, pp. B606\u2013B641, \n2012\n.\n\n\nJ. Brown, \nEfficient nonlinear solvers for nodal high-order finite elements in 3D\n, \nJournal of Scientific Computing\n, 45:48\u201363, \n2010\n. doi:10.1007/s10915-010-9396-8\n\n\nTz. Kolev and P. Vassilevski, \nParallel auxiliary space AMG for H(curl) problems\n, \nJ. Comput. Math.\n, 27, pp. 604-623, \n2009\n.\n\n\n\n\n\n\nPresentations\n\n\n2017\n\n\n\n\nTz. Kolev and M. Shephard, \nConforming \n Nonconforming Adaptivity for Unstructured Meshes\n, Argonne Training Program on Extreme-Scale Computing, Aug 7, \n2017\n.\n\n\nTz. Kolev and M. Shephard, \nUnstructured Mesh Technologies\n, Argonne Training Program on Extreme-Scale Computing, Aug 7, \n2017\n.\n\n\nB. Smith, \nNonlinear and Krylov Solvers\n, Argonne Training Program on Extreme-Scale Computing, Aug 7, \n2017\n.\n\n\nJ. Dongarra, \nAdaptive Linear Solvers and Eigensolvers\n, Argonne Training Program on Extreme-Scale Computing, Aug 7, \n2017\n.\n\n\nT. Warburton, \nAn Intro to GPU Architecture and Programming Models\n, Argonne Training Program on Extreme-Scale Computing, Aug 3, \n2017\n.\n\n\nS. Parker, \nArchitectures of the Argonne Cray XC40 KNL System \"Theta\"\n, Argonne Training Program on Extreme-Scale Computing, Jul 31, \n2017\n.\n\n\nS. Tomov and A. Haidar, \nMAGMA Tensors and Batched Computing for Accelerating Applications on GPUs\n, GPU Technology Conference (GTC'17), Session S7728, May 8-11, \n2017\n.\n\n\nA. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov, \nAccelerating Tensor Contractions in High-Order FEM with MAGMA Batched\n, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, Feb 26-Mar 3, \n2017\n.\n\n\nP. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, Feb 26-Mar 3, \n2017\n.\n\n\nM. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, Feb 26-Mar 3, \n2017\n.\n\n\nC. Smith, G. Diamond and M.S. Shephard, Fast Dynamic Load Balancing Tools for Extreme Scale System, \nSIAM Conference on Computer Science and Engineering (SIAM CSE'17)\n, Atlanta, GA, Feb 26-Mar 3, \n2017\n.\n\n\nS. Tendulkar, O. Klaas, M.W. Beall, and M.S. Shephard, Parallel Geometry and Meshing Adaptation with Application to Problems with Evolving Domains, \nSIAM Conf. on Computational Science and Engineering\n, Atlanta, GA, Mar 3, \n2017\n.\n\n\nP. Fischer, \nCFD, PDEs, and HPC: A 30-Year Perspective\n, Argonne Training Program on Extreme-Scale Computing, Aug 2, \n2016\n.\n\n\n\n\n\n\nHighlights\n\n\n\n\nGCN aticle: \nExascale a \"main priority\" for DOE\n, Jan 2018.\n\n\nECP article: \nCo-Design Center Develops Next-Generation Simulation Tools\n, also in \nHPCwire\n, Nov 2017.\n\n\nWork with LLNL's \nCenter for Design and Optimization\n mentioned in LLNL Newsline, Oct 2017.\n\n\nHighlight in \nCASC Newsletter #3\n, Oct 2017.\n\n\nHighlight in LLNL\u2019s \n65th Anniversary Book\n (2017 page), Oct 2017.\n\n\nGPU work highlight in LLNL's \nCOMP News page\n and Livermore_Comp's \nTwitter feed\n, May 2017.\n\n\nWork with Cardioid mentioned in LLNL's \nScience \n Technology Review\n magazine, Mar 2017.\n\n\nNews coverage of CEED announcement in \nLLNL Newsline\n\nand the \nANL press release\n, Nov 2016.\n\n\nWork with BLAST mentioned in LLNL's \nScience \n Technology Review\n magazine, Sep 2016.\n\n\n\n\n\n\nOther Resources\n\n\n\n\nCEED-tagged topics on \nECP's website\n.\n\n\nANL's \nexascale computing website\n.\n\n\nLLNL's \nexascale computing website\n.\n\n\nBlog of Virginia Tech's \nParallel Numerical Algorithms research group\n.\n\n\nU.S. Department of Energy \nExascale Initiative\n.", 
            "title": "Outreach"
        }, 
        {
            "location": "/pubs/#publications-and-outreach", 
            "text": "", 
            "title": "Publications and Outreach"
        }, 
        {
            "location": "/pubs/#ceed-documents", 
            "text": "CEED's high-order  Benchmarks  and  Miniapps .  Activities in the  Applications ,  Hardware ,  Software  and  Finite Element  thrusts.  CEED-proposed high-order  Operator  and  Visualization  formats.", 
            "title": "CEED Documents"
        }, 
        {
            "location": "/pubs/#publications", 
            "text": "", 
            "title": "Publications"
        }, 
        {
            "location": "/pubs/#2018", 
            "text": "V. Dobrev, P. Knupp, Tz. Kolev, K. Mittal, V. Tomov,  The Target-Matrix Optimization Paradigm for High-Order Meshes , submitted,  2018 .  A. Karakus, N. Chalmers, K. Swirydowicz, T. Warburton,  GPU Acceleration of a High-Order Discontinuous Galerkin Incompressible Flow Solver , submitted,  2018 .  A. Barker, V. Dobrev, J. Gopalakrishnan and T. Kolev,  A scalable preconditioner for a DPG method ,  SIAM J. Sci. Comp. , accepted,  2018 .  R. Anderson, V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov,  High-Order Multi-Material ALE Hydrodynamics ,   SIAM J. Sci. Comp. , 40(1):B32-B58,  2018 .  V. Dobrev, Tz. Kolev, D. Kuzmin, R. Rieben and V. Tomov,  Sequential limiting in continuous and discontinuous Galerkin methods for the Euler equations , Journal of Computational Physics,\n356:372-390,  2018 .", 
            "title": "2018"
        }, 
        {
            "location": "/pubs/#2017", 
            "text": "K. Swirydowicz, N. Chalmers, A. Karakus, T. Warburton,  Acceleration of tensor-product operations for high-order finite element methods , submitted,  2017  J. Solberg, E. Merzari, H. Yuan, A. Obabko and P. Fischer, S. Lee, J. Lai, M. Delgado, S. J. Lee, and Y. Hassan,  High-Fidelity Simulation of Flow Induced Vibrations in Helical Steam Generators for Small Modular Reactors ,  Best Paper Award  at  The 17th International Topical Meeting on Nuclear Reactor Thermal Hydraulics (NURETH-17) , September,  2017 .  V. Dobrev, Tz. Kolev, C. Lee, V. Tomov and P. Vassilevski,  Algebraic Hybridization and Static Condensation with Application to Scalable H(div) Preconditioning , submitted,  2017 .  S. Patel, P. Fisher, M. Min, and A. Tomboulides,  A Characteristic-based, spectral element method for moving-domain problems ,  J. Sci. Comp. , submitted,  2017 .  I. Masliah, A. Abdelfattah, A. Haidar, S. Tomov, M. Baboulin, J. Falcou, and J. Dongarra,  Algorithms and optimization techniques for high-performance matrix-matrix multiplications of very small matrices ,  Parallel Computing (PARCO) , submitted,  2017 .  K. Raffenetti et. al, P. Fischer, M. Min, and P. Balaji,  Why is MPI so slow? Analyzing the fundamental limits in implementing MPI-3.1 , accepted,  SC'17 ,  2017 .  P. Fischer, M. Schmitt, and A. Tomboulides,  Recent developments in spectral element simulations of moving-domain problems , vol. 79,  Fields Institute Communications , 213\u2013244,  2017 .  S. Lomperski, A. Obabko, P. Fischer, E. Merzari, and W.D. Pointer,  Jet stability and wall impingement flow field in a thermal striping experiment ,  Int. J. Heat Mass Transfer , 115A:1125\u2013 1136,  2017 .  V. Makarashvilia, E. Merzari, A. Obabko, A. Siegel, and P. Fischer,  A performance analysis of ensemble averaging for high fidelity turbulence simulations at the strong scaling limit ,  Comp. Phys. Comm. , in press,  2017 .  A. Abdelfattah, A. Haidar, S. Tomov, J. Dongarra,  Factorization and Inversion of a Million Matrices using GPUs: Challenges and Countermeasures ,  Proceedings of the 2017 International Conference on Computational Science, ICCS'17 , Z\u00fcrich, Switzerland, June 12-14,  Procedia Computer Science ,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, and S. Tomov,  Small Tensor Operations on Advanced Architectures for High-order Applications , Technical report UT-EECS-17-749, EECS Department, University of Tennessee,  2017 .  R. Anderson, V. Dobrev, Tz. Kolev, D. Kuzmin, M. Quezada de Luna, R. Rieben and V. Tomov, High-order local maximum principle preserving (MPP) discontinuous Galerkin finite element method for the transport equation , Journal of Computational Physics, 334:102\u2013124,  2017 .  Bazilevs, Y., Kamran, K., Moutsanidis, G., Benson, D. J.,   O\u00f1ate, E,  A new formulation for air-blast fluid\u2013structure interaction using an immersed approach. Part I: basic methodology and FEM-based simulations , Computational Mechanics, 1-18,  2017 .  Abdelfattah, A., Haidar, A., Tomov, S., and Dongarra, J.  Novel HPC Techniques to Batch Execution of Many Variable Size BLAS Computations on GPUs , International Conference on Supercomputing (ICS'17), ACM, Chicago, Illinois, pp. 1-10, June 14-16,  2017 .  Ibanez, D., Shephard, M.S.,   Modifiable Array Data Structures for Mesh Topology , SIAM Journal on Scientific Computing, 39(2):C144-C161,  2017 .  Granzow, B.N., Shephard M.S., Oberai, A.A.,  Output-based error estimation and mesh adaptation for variational multiscale methods ,\nComputer Methods in Applied Mechanics and Engineering, Vol. 322, pp. 441-459,  2017 .", 
            "title": "2017"
        }, 
        {
            "location": "/pubs/#2016", 
            "text": "E. Merzari, A. Obabko, P. Fischer, N. Halford, J. Walker, A. Siegel, and Y. Q. Yu,  Large-scale large eddy simulation of nuclear reactor flows: Issues and perspectives ,  Nuclear Engineering and Design , page 13, Oct.,  2016 .  E. Merzari, P. Fischer, H. Yuan, K. Van Tichelen, S. Keijers, J. De Ridder, J. Degroote, J. Vierendeels, H. Doolaard, V. R. Gopala, and F. Roelofs,  Benchmark exercise for fluid flow simulations in a liquid metal fast reactor fuel assembly ,  Nuclear Engineering and Design , 298(3):218\u2013228,  2016 .  V. Dobrev, Tz. Kolev, R. Rieben and V. Tomov,  Multi-material closure model for high-order finite element Lagrangian hydrodynamics ,  Int. J. Numer. Meth. Fluids , 82(10), pp. 689\u2013706,  2016 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  High-performance Tensor Contractions for GPUs ,  Procedia Computer Science , Volume 80, Pages 108-118, ISSN 1877-0509,  2016 .  M. B.E., Y. Peet, P. Fischer, and J. Lottes,  A spectrally accurate method for overlapping grid solution of incompressible Navier-Stokes equations ,  J. Comp. Phys. , 307:60\u201393,  2016 .  M. Otten, J. Gong, A. Mametjanov, A. Vose, J. Levesque, P. Fischer, and M. Min,  An MPI/OpenACC implementation of a high order electromagnetics solver with GPUDirect communication ,  The International Journal of High Performance Computing Application , 30(3):320\u2013334,  2016 .  J. Gong, S. Markidis, E. Laure, M. Otten, P. Fischer, M. Min,  Nekbone Performance on GPUs with OpenACC and CUDA Fortran Implementations ,  Special issue on Sustainability on Ultrascale Computing Systems and Applications: Journal of Supercomputing ,  2016 .  Ibanez, Daniel A., et al.  PUMI: Parallel unstructured mesh infrastructure.  ACM Transactions on Mathematical Software (TOMS) 42.3  2016 .  Smith, Cameron W., et al.  In-memory Integration of Existing Software Components for Parallel Adaptive Unstructured Mesh Workflows.  Proceedings of the XSEDE16 Conference on Diversity, Big Data, and Science at Scale. ACM,  2016 .  Ibanez, D., Dunn, I., Shephard M.S.,  Hybrid MPI-thread parallelization of adaptive mesh operations , Parallel Computing, 52:133-143,  2016 .", 
            "title": "2016"
        }, 
        {
            "location": "/pubs/#2015-and-earlier", 
            "text": "P. Fischer, K. Heisey, and M. Min,  Scaling limits for PDE-based simulation ,  In 22nd AIAA Computational Fluid Dynamics Conference, AIAA Aviation , AIAA 2015-3049,  2015 .  A. Kraus, S. Aithal, A. Obabko, E. Merzari, A. Tomboulides, and P. Fischer,  Erosion of a large-scale gaseous stratified layer by a turbulent jet - Simulations with URANS and LES approaches , volume 2, pp. 1448\u20131461.  American Nuclear Society ,  2015 .  E. Merzari, P. Fischer, and J. Walker,  Large-scale simulation of rod bundles: Coherent structure recognition and stability analysis , volume 1.  American Society of Mechanical Engineers ,  2015 .  M. Otten, R. A. Shah, N. F. Scherer, M. Min, M. Pelton, and S. K. Gray,  Entanglement of two, three and four plasmonically coupled quantum dots ,  Physical Review B , 92:125432,  2015 .  D. A. May, J. Brown, and L. Le Pourhiet.  pTatin3D: High-performance methods for long-term lithospheric dynamics , In Proceedings of  SC14: International Conference for High Performance Computing, Networking, Storage and Analysis . ACM,  2014 .  R. Anderson, V. Dobrev, Tz. Kolev and R. Rieben,  Monotonicity in high-order curvilinear finite element ALE remap ,  Int. J. Numer. Meth. Fluids , 77(5), pp. 249\u2013273,  2014 .  Kamran, Kazem, et al.  A compressible Lagrangian framework for modeling the fluid\u2013structure interaction in the underwater implosion of an aluminum cylinder.  Mathematical Models and Methods in Applied Sciences 23.02  2013 .  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG solver for H(div) problems ,   SIAM J. Sci. Comp. , 34, pp. A3079\u2013A3098,  2012 .  V. Dobrev, Tz. Kolev and R. Rieben,  High-order curvilinear finite element methods for Lagrangian hydrodynamics ,  SIAM J. Sci. Comp. , 34, pp. B606\u2013B641,  2012 .  J. Brown,  Efficient nonlinear solvers for nodal high-order finite elements in 3D ,  Journal of Scientific Computing , 45:48\u201363,  2010 . doi:10.1007/s10915-010-9396-8  Tz. Kolev and P. Vassilevski,  Parallel auxiliary space AMG for H(curl) problems ,  J. Comput. Math. , 27, pp. 604-623,  2009 .", 
            "title": "2015 and earlier"
        }, 
        {
            "location": "/pubs/#presentations", 
            "text": "", 
            "title": "Presentations"
        }, 
        {
            "location": "/pubs/#2017_1", 
            "text": "Tz. Kolev and M. Shephard,  Conforming   Nonconforming Adaptivity for Unstructured Meshes , Argonne Training Program on Extreme-Scale Computing, Aug 7,  2017 .  Tz. Kolev and M. Shephard,  Unstructured Mesh Technologies , Argonne Training Program on Extreme-Scale Computing, Aug 7,  2017 .  B. Smith,  Nonlinear and Krylov Solvers , Argonne Training Program on Extreme-Scale Computing, Aug 7,  2017 .  J. Dongarra,  Adaptive Linear Solvers and Eigensolvers , Argonne Training Program on Extreme-Scale Computing, Aug 7,  2017 .  T. Warburton,  An Intro to GPU Architecture and Programming Models , Argonne Training Program on Extreme-Scale Computing, Aug 3,  2017 .  S. Parker,  Architectures of the Argonne Cray XC40 KNL System \"Theta\" , Argonne Training Program on Extreme-Scale Computing, Jul 31,  2017 .  S. Tomov and A. Haidar,  MAGMA Tensors and Batched Computing for Accelerating Applications on GPUs , GPU Technology Conference (GTC'17), Session S7728, May 8-11,  2017 .  A. Abdelfattah, M. Baboulin, V. Dobrev, J. Dongarra, C. Earl, J. Falcou, A. Haidar, I. Karlin, Tz. Kolev, I. Masliah, S. Tomov,  Accelerating Tensor Contractions in High-Order FEM with MAGMA Batched ,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3,  2017 .  P. Fischer, Efficiency of High-Order Methods on the 2nd Generation Intel Xeon Phi Processor,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3,  2017 .  M. Min, Spectral Element Simulation for Nanowire Solar Cells on HPC Platforms,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3,  2017 .  C. Smith, G. Diamond and M.S. Shephard, Fast Dynamic Load Balancing Tools for Extreme Scale System,  SIAM Conference on Computer Science and Engineering (SIAM CSE'17) , Atlanta, GA, Feb 26-Mar 3,  2017 .  S. Tendulkar, O. Klaas, M.W. Beall, and M.S. Shephard, Parallel Geometry and Meshing Adaptation with Application to Problems with Evolving Domains,  SIAM Conf. on Computational Science and Engineering , Atlanta, GA, Mar 3,  2017 .  P. Fischer,  CFD, PDEs, and HPC: A 30-Year Perspective , Argonne Training Program on Extreme-Scale Computing, Aug 2,  2016 .", 
            "title": "2017"
        }, 
        {
            "location": "/pubs/#highlights", 
            "text": "GCN aticle:  Exascale a \"main priority\" for DOE , Jan 2018.  ECP article:  Co-Design Center Develops Next-Generation Simulation Tools , also in  HPCwire , Nov 2017.  Work with LLNL's  Center for Design and Optimization  mentioned in LLNL Newsline, Oct 2017.  Highlight in  CASC Newsletter #3 , Oct 2017.  Highlight in LLNL\u2019s  65th Anniversary Book  (2017 page), Oct 2017.  GPU work highlight in LLNL's  COMP News page  and Livermore_Comp's  Twitter feed , May 2017.  Work with Cardioid mentioned in LLNL's  Science   Technology Review  magazine, Mar 2017.  News coverage of CEED announcement in  LLNL Newsline \nand the  ANL press release , Nov 2016.  Work with BLAST mentioned in LLNL's  Science   Technology Review  magazine, Sep 2016.", 
            "title": "Highlights"
        }, 
        {
            "location": "/pubs/#other-resources", 
            "text": "CEED-tagged topics on  ECP's website .  ANL's  exascale computing website .  LLNL's  exascale computing website .  Blog of Virginia Tech's  Parallel Numerical Algorithms research group .  U.S. Department of Energy  Exascale Initiative .", 
            "title": "Other Resources"
        }, 
        {
            "location": "/fe/", 
            "text": "Finite Element Thrust \n\n\nThe goal of CEED's Finite Element (FE) thrust, led by \nVeselin Dobrev\n\nfrom \nLawrence Livermore National Laboratory\n, is to continue to improve\nthe state-of-the-art spectral element/high-order finite element algorithms and kernels in the\nCEED software targeting exascale architectures.\n\n\nWhy High-Order Finite Elements?\n\n\nEfficient exploitation of\n\nexascale architectures\n requires\na rethink of the numerical algorithms used in large-scale scientific applications.\nThese architectures favor algorithms that expose ultra-fine-grain parallelism\nand maximize the ratio of floating-point operations to energy-intensive data\nmovement.\n\n\nMany large-scale \napplications\n employ unstructured finite element\ndiscretization methods, where practical efficiency is measured by the accuracy\nachieved per unit computational time.  One of the few viable approaches to\nachieve high performance in this case is to use matrix-free high-order finite\nelement methods, since these methods can both increase the accuracy and/or lower\nthe computational time due to reduced data motion.\n\n\nTo achieve this efficiency, high-order methods use mesh elements that are mapped\nfrom canonical reference elements (hexes, wedges, pyramids, tetrahedra) and\nexploit, where possible, the tensor-product structure of the canonical mesh\nelements and finite element spaces. Through matrix-free partial assembly, the\nuse of canonical reference elements enables substantial cache efficiency and\nminimizes extraneous data movement in comparison to traditional low-order\napproaches.\n\n\nHigh-Order Benchmarks and Miniapps\n\n\nThe FE thrust works closely with the \nApplications\n and \nHardware\n\nthrusts on the development of CEED \nbenchmarks\n and\n\nminiapps\n, and coordinates the project software activities with\nthe \nSoftware\n thrust.\n\n\nEcosystem for High-Order Applications\n\n\nIn addition to performance improvements, the FE thrust is focusing on\ncommunity-wide challenges that are necessary for establishing a\nfull-fledged high-order application software ecosystem.\n\n\nThese efforts include research and development in the following areas:\n\n\n\n\nHigh-Order Meshes\n\n\nUnstructured Adaptive Mesh Refinement\n\n\nEfficient High-Order Operator Format / Representation\n\n\nBatched Dense Tensor Contractions\n\n\nScalable Matrix-Free Solvers\n\n\nGeneral Interpolation\n\n\nVisualization of High-Order Meshes and Functions", 
            "title": "Finite Elements"
        }, 
        {
            "location": "/fe/#finite-element-thrust", 
            "text": "The goal of CEED's Finite Element (FE) thrust, led by  Veselin Dobrev \nfrom  Lawrence Livermore National Laboratory , is to continue to improve\nthe state-of-the-art spectral element/high-order finite element algorithms and kernels in the\nCEED software targeting exascale architectures.", 
            "title": "Finite Element Thrust "
        }, 
        {
            "location": "/fe/#why-high-order-finite-elements", 
            "text": "Efficient exploitation of exascale architectures  requires\na rethink of the numerical algorithms used in large-scale scientific applications.\nThese architectures favor algorithms that expose ultra-fine-grain parallelism\nand maximize the ratio of floating-point operations to energy-intensive data\nmovement.  Many large-scale  applications  employ unstructured finite element\ndiscretization methods, where practical efficiency is measured by the accuracy\nachieved per unit computational time.  One of the few viable approaches to\nachieve high performance in this case is to use matrix-free high-order finite\nelement methods, since these methods can both increase the accuracy and/or lower\nthe computational time due to reduced data motion.  To achieve this efficiency, high-order methods use mesh elements that are mapped\nfrom canonical reference elements (hexes, wedges, pyramids, tetrahedra) and\nexploit, where possible, the tensor-product structure of the canonical mesh\nelements and finite element spaces. Through matrix-free partial assembly, the\nuse of canonical reference elements enables substantial cache efficiency and\nminimizes extraneous data movement in comparison to traditional low-order\napproaches.", 
            "title": "Why High-Order Finite Elements?"
        }, 
        {
            "location": "/fe/#high-order-benchmarks-and-miniapps", 
            "text": "The FE thrust works closely with the  Applications  and  Hardware \nthrusts on the development of CEED  benchmarks  and miniapps , and coordinates the project software activities with\nthe  Software  thrust.", 
            "title": "High-Order Benchmarks and Miniapps"
        }, 
        {
            "location": "/fe/#ecosystem-for-high-order-applications", 
            "text": "In addition to performance improvements, the FE thrust is focusing on\ncommunity-wide challenges that are necessary for establishing a\nfull-fledged high-order application software ecosystem.  These efforts include research and development in the following areas:   High-Order Meshes  Unstructured Adaptive Mesh Refinement  Efficient High-Order Operator Format / Representation  Batched Dense Tensor Contractions  Scalable Matrix-Free Solvers  General Interpolation  Visualization of High-Order Meshes and Functions", 
            "title": "Ecosystem for High-Order Applications"
        }, 
        {
            "location": "/meshing/", 
            "text": "High-Order Meshes \n\n\nHigh-order (curved) meshes appear in many applications, e.g. due to curved\nmaterial interfaces or due to mesh motion in simulations with moving meshes\n(for example, ALE methods).\n\n\nThe use of high-order geometry representation can lead to better feature\nresolution, improved symmetry preservation, better solution adaptivity and\nincreased robustness for problems where the mesh is being deformed in time.\n\n\nThe CEED software supports arbitrary order quadrilateral, hexahedral, triangular\nand tetrahedral meshes, and our team members are developing algorithms for\nhigh-order mesh optimization and solution transfer between deformed high-order\nmeshes (remap).\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nUnstructured AMR \n\n\nAdaptive Mesh Refinement (AMR) on unstructured grids is an important enabling\ntechnology for many applications.\n\n\nThe CEED software supports both conforming AMR on triangular and tetrahedral\nmeshes as well as general non-conforming AMR on quadrilateral and hexahedral\nmeshes. Both approaches work for low-order meshes with no performance penalty\nand support parallel dynamic load balancing.\n\n\nOne of the distinguishing characteristics of our AMR approach is that it is\nimplemented at the level of the discretization library, decoupled from the\nphysics, so it can easily be incorporated in a variety of applications.  We\nsupport spaces in the whole de Rham complex, as well as AMR on high-order\nmeshes.\n\n\nCEED scientists are working on practical error indicators and\nphysics-conservative AMR interpolations to meet application needs.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Meshing"
        }, 
        {
            "location": "/meshing/#high-order-meshes", 
            "text": "High-order (curved) meshes appear in many applications, e.g. due to curved\nmaterial interfaces or due to mesh motion in simulations with moving meshes\n(for example, ALE methods).  The use of high-order geometry representation can lead to better feature\nresolution, improved symmetry preservation, better solution adaptivity and\nincreased robustness for problems where the mesh is being deformed in time.  The CEED software supports arbitrary order quadrilateral, hexahedral, triangular\nand tetrahedral meshes, and our team members are developing algorithms for\nhigh-order mesh optimization and solution transfer between deformed high-order\nmeshes (remap).  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "High-Order Meshes "
        }, 
        {
            "location": "/meshing/#unstructured-amr", 
            "text": "Adaptive Mesh Refinement (AMR) on unstructured grids is an important enabling\ntechnology for many applications.  The CEED software supports both conforming AMR on triangular and tetrahedral\nmeshes as well as general non-conforming AMR on quadrilateral and hexahedral\nmeshes. Both approaches work for low-order meshes with no performance penalty\nand support parallel dynamic load balancing.  One of the distinguishing characteristics of our AMR approach is that it is\nimplemented at the level of the discretization library, decoupled from the\nphysics, so it can easily be incorporated in a variety of applications.  We\nsupport spaces in the whole de Rham complex, as well as AMR on high-order\nmeshes.  CEED scientists are working on practical error indicators and\nphysics-conservative AMR interpolations to meet application needs.  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Unstructured AMR "
        }, 
        {
            "location": "/linalg/", 
            "text": "Batched Dense Tensor Contractions \n\n\nThe numerical kernels of efficient high-order operator evaluation reduce to many\nsmall dense tensor contractions, one for each element of the computational mesh.\nThese contractions can be performed in parallel over the elements and can be\nimplemented as a sequence of small matrix-matrix multiplications (dgemms).\n\n\nCEED is developing efficient algorithms for these numerical kernels on a variety\nof advanced hardware, both by directly working on the specifying tensor\ncontraction found in high-order finite element kernels, as well as by recasting\nto batched interface of LAPACK-like linear algebra libraries providing efficient\ndgemms for matrices of small-to-medium size.\n\n\nCEED scientists are also involved in the standardization efforts for batched\nversions of Basic Linear Algebra Software Library (BLAS) routines, which are the\nfoundations for LAPACK-type libraries.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nScalable Matrix-Free Solvers\n\n\nCEED is developing matrix-free linear solvers and preconditioners for \npartially\nassembled\n high-order operators utilizing expertise from solver\nlibraries like \nhypre\n and\n\nPETSc\n.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Linear Algebra"
        }, 
        {
            "location": "/linalg/#batched-dense-tensor-contractions", 
            "text": "The numerical kernels of efficient high-order operator evaluation reduce to many\nsmall dense tensor contractions, one for each element of the computational mesh.\nThese contractions can be performed in parallel over the elements and can be\nimplemented as a sequence of small matrix-matrix multiplications (dgemms).  CEED is developing efficient algorithms for these numerical kernels on a variety\nof advanced hardware, both by directly working on the specifying tensor\ncontraction found in high-order finite element kernels, as well as by recasting\nto batched interface of LAPACK-like linear algebra libraries providing efficient\ndgemms for matrices of small-to-medium size.  CEED scientists are also involved in the standardization efforts for batched\nversions of Basic Linear Algebra Software Library (BLAS) routines, which are the\nfoundations for LAPACK-type libraries.  This is an active area of research for our team and we are interested in\n collaboration.", 
            "title": "Batched Dense Tensor Contractions "
        }, 
        {
            "location": "/linalg/#scalable-matrix-free-solvers", 
            "text": "CEED is developing matrix-free linear solvers and preconditioners for  partially\nassembled  high-order operators utilizing expertise from solver\nlibraries like  hypre  and PETSc .  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Scalable Matrix-Free Solvers"
        }, 
        {
            "location": "/discr/", 
            "text": "Efficient High-Order Operator Format \n\n\nWhile a global (parallel) sparse matrix is a good representation of a PDE\noperator discretized with low-order elements, a global parallel matrix is a poor\nchoice when discretizing with high-order elements, due to the large cost of both\nthe memory transfer and floating point operations.\n\n\nCEED is developing an alternative operator format, based on the CEED \nlow-level\nAPI\n, that allows efficient operator evaluation that is optimal in\nmemory and nearly-optimal in FLOPs cost.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...\n\n\nGeneral Interpolation of Solution Field Values\n\n\nParticle tracking, grid-to-grid transfer and data analysis are typical\noperations that require off-grid function evaluation.\n\n\nCEED is developing a scalable interpolation routine for arbitrary-order\nhexahedral elements that uses a hash table to rapidly identify candidate\nelements/processors that might contain the point in question, followed by a\nNewton iteration to find the point in the reference domain.\n\n\nThe iteration is based on minimization, rather than root-finding, which is\nadvantageous when the interpolation point is on or near an element boundary\nwhere high-order interpolants tend to exhibit rapid variation.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Discretization"
        }, 
        {
            "location": "/discr/#efficient-high-order-operator-format", 
            "text": "While a global (parallel) sparse matrix is a good representation of a PDE\noperator discretized with low-order elements, a global parallel matrix is a poor\nchoice when discretizing with high-order elements, due to the large cost of both\nthe memory transfer and floating point operations.  CEED is developing an alternative operator format, based on the CEED  low-level\nAPI , that allows efficient operator evaluation that is optimal in\nmemory and nearly-optimal in FLOPs cost.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Efficient High-Order Operator Format "
        }, 
        {
            "location": "/discr/#general-interpolation-of-solution-field-values", 
            "text": "Particle tracking, grid-to-grid transfer and data analysis are typical\noperations that require off-grid function evaluation.  CEED is developing a scalable interpolation routine for arbitrary-order\nhexahedral elements that uses a hash table to rapidly identify candidate\nelements/processors that might contain the point in question, followed by a\nNewton iteration to find the point in the reference domain.  The iteration is based on minimization, rather than root-finding, which is\nadvantageous when the interpolation point is on or near an element boundary\nwhere high-order interpolants tend to exhibit rapid variation.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "General Interpolation of Solution Field Values"
        }, 
        {
            "location": "/vis/", 
            "text": "Visualization of High-Order Meshes and Functions \n\n\nAccurate visualization of general finite element meshes and functions in the de\nRham complex requires finite element knowledge that may not be present in\nvisualization tools employed by applications. The visualization needs to account\nfor the orders of the mesh and solution fields, as well as the type of finite\nelement basis used for each of them.\n\n\nAn additional challenge for high-order meshes and functions is that there is no\ncommon community standard for the description of high-order data at arbitrary\nother.\n\n\nCEED is working with visualization and application teams to develop a\nstandard that not only improves visualization capabilities but also enables\nconsistent data transfer between high-order applications.\n\n\nOur work is based on the current capabilities in \nMFEM\n, illustrated in\nits native \nGLVis\n visualization tool, as well as in the\n\nVisIt\n visualization and data analysis application.\n\n\nThis is an active area of research for our team and we are interested in\n collaboration.\n\n\nStay tuned for more details...", 
            "title": "Visualization"
        }, 
        {
            "location": "/vis/#visualization-of-high-order-meshes-and-functions", 
            "text": "Accurate visualization of general finite element meshes and functions in the de\nRham complex requires finite element knowledge that may not be present in\nvisualization tools employed by applications. The visualization needs to account\nfor the orders of the mesh and solution fields, as well as the type of finite\nelement basis used for each of them.  An additional challenge for high-order meshes and functions is that there is no\ncommon community standard for the description of high-order data at arbitrary\nother.  CEED is working with visualization and application teams to develop a\nstandard that not only improves visualization capabilities but also enables\nconsistent data transfer between high-order applications.  Our work is based on the current capabilities in  MFEM , illustrated in\nits native  GLVis  visualization tool, as well as in the VisIt  visualization and data analysis application.  This is an active area of research for our team and we are interested in\n collaboration.  Stay tuned for more details...", 
            "title": "Visualization of High-Order Meshes and Functions "
        }, 
        {
            "location": "/ceed1am/", 
            "text": "CEED First Annual Meeting\n\n\nAugust 15-17, 2017\n\n\nLawrence Livermore National Laboratory\n\n\n\n\nOverview\n\n\nCEED will hold its first annual meeting in August 15-17, 2017 at the \nHPC Innovation Center\n of Lawrence Livermore National Laboratory in Livermore, California. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.\n\n\nMeeting Agenda\n\n\nLatest agenda:\n\n\n\n\nGo to the \nCVENT website\n\n\nLogin with: \nCEED2017\n\n\nClick on the \nAgenda\n tab\n\n\n\n\nCatering and Dinner\n\n\nBreakfast and Lunch will be included with a separate fee.  Dinner on 8/15 will be provided with that fee as well.\n\n\nLodging Options\n\n\nThere are many hotels in Livermore.  A few from the about 10 hotels  in Livermore are below.  Other hotels are available in Pleasanton and other nearby cities. \nIf you stay outside of Livermore we recommend staying west of the city to have a reverse commute to the lab.\n\n\n\n\nBest Western\n - 2.5 miles from LLNL\n\n\nDouble Tree\n - 5 miles from LLNL\n\n\nCourtyard by Marriot\n - 9 miles from LLNL\n\n\nHoliday Inn Express\n - 9 miles from LLNL\n\n\n\n\nAbout Livermore and LLNL\n\n\nFounded in 1869, Livermore is California\u2019s oldest wine region, framed by award-winning wineries, farm lands and ranches that mirror the valley\u2019s western heritage.  As home to renowned science and technology centers, Lawrence Livermore National Laboratory and Sandia National Laboratory, Livermore is a technological hub and an academically engaged community. It has become an integral part of the Bay Area, successfully competing in the global market powered by its wealth of research, technology and innovation.\n\n\nFor more than 60 years, the Lawrence Livermore National Laboratory has applied science and technology to make the world a safer place.  World class facilities include the National Ignition Facility, and the Terascale facility hosting Sequoia, currently the world's fifth fastest supercomputer.\n\n\nQuestions?\n\n\nPlease contact the meeting organizers at: \nllnl-ceed-meeting@listserv.llnl.gov\n.", 
            "title": "_CEED1AM"
        }, 
        {
            "location": "/ceed1am/#ceed-first-annual-meeting", 
            "text": "", 
            "title": "CEED First Annual Meeting"
        }, 
        {
            "location": "/ceed1am/#august-15-17-2017", 
            "text": "", 
            "title": "August 15-17, 2017"
        }, 
        {
            "location": "/ceed1am/#lawrence-livermore-national-laboratory", 
            "text": "", 
            "title": "Lawrence Livermore National Laboratory"
        }, 
        {
            "location": "/ceed1am/#overview", 
            "text": "CEED will hold its first annual meeting in August 15-17, 2017 at the  HPC Innovation Center  of Lawrence Livermore National Laboratory in Livermore, California. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.", 
            "title": "Overview"
        }, 
        {
            "location": "/ceed1am/#meeting-agenda", 
            "text": "Latest agenda:   Go to the  CVENT website  Login with:  CEED2017  Click on the  Agenda  tab", 
            "title": "Meeting Agenda"
        }, 
        {
            "location": "/ceed1am/#catering-and-dinner", 
            "text": "Breakfast and Lunch will be included with a separate fee.  Dinner on 8/15 will be provided with that fee as well.", 
            "title": "Catering and Dinner"
        }, 
        {
            "location": "/ceed1am/#lodging-options", 
            "text": "There are many hotels in Livermore.  A few from the about 10 hotels  in Livermore are below.  Other hotels are available in Pleasanton and other nearby cities.  If you stay outside of Livermore we recommend staying west of the city to have a reverse commute to the lab.   Best Western  - 2.5 miles from LLNL  Double Tree  - 5 miles from LLNL  Courtyard by Marriot  - 9 miles from LLNL  Holiday Inn Express  - 9 miles from LLNL", 
            "title": "Lodging Options"
        }, 
        {
            "location": "/ceed1am/#about-livermore-and-llnl", 
            "text": "Founded in 1869, Livermore is California\u2019s oldest wine region, framed by award-winning wineries, farm lands and ranches that mirror the valley\u2019s western heritage.  As home to renowned science and technology centers, Lawrence Livermore National Laboratory and Sandia National Laboratory, Livermore is a technological hub and an academically engaged community. It has become an integral part of the Bay Area, successfully competing in the global market powered by its wealth of research, technology and innovation.  For more than 60 years, the Lawrence Livermore National Laboratory has applied science and technology to make the world a safer place.  World class facilities include the National Ignition Facility, and the Terascale facility hosting Sequoia, currently the world's fifth fastest supercomputer.", 
            "title": "About Livermore and LLNL"
        }, 
        {
            "location": "/ceed1am/#questions", 
            "text": "Please contact the meeting organizers at:  llnl-ceed-meeting@listserv.llnl.gov .", 
            "title": "Questions?"
        }, 
        {
            "location": "/ceed2am/", 
            "text": "CEED Second Annual Meeting\n\n\nAugust 8-10, 2018\n\n\nUniversity of Colorado Boulder\n\n\n\n\nOverview\n\n\nCEED will hold its second annual meeting August 8-10, 2018 in \nKoelbel S127\n (\nadjacent\n to the Engineering Center) at the University of Colorado Boulder. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.\n\n\nTentative Meeting Agenda\n\n\nAugust 8 (Wednesday)\n\n\n\n\nProject overview\n\n\nCollaborative discussion\n\n\nMeeting dinner\n\n\n\n\nAugust 9 (Thursday)\n\n\n\n\nGuest speakers\n\n\nCollaborative discussion\n\n\nExcursion (hiking in the Flatirons)\n\n\n\n\nAugust 10 (Friday)\n\n\n\n\nInternal planning\n\n\nHackathon\n\n\n\n\nLodging Options\n\n\nThere are many hotels close to the university in Boulder.\n\n\n\n\nMillennium Harvest\n - 5 minute walk\n\n\nBest Western\n - 10 minute walk\n\n\nMarriott\n - 10 minute walk\n\n\nEmbassy Suites (Hilton)\n - 10 minute walk\n\n\nGarden Inn (Hilton)\n - 10 minute walk\n\n\n\n\nQuestions?\n\n\nPlease contact the meeting organizers at: \nllnl-ceed-meeting@listserv.llnl.gov\n.", 
            "title": "_CEED2AM"
        }, 
        {
            "location": "/ceed2am/#ceed-second-annual-meeting", 
            "text": "", 
            "title": "CEED Second Annual Meeting"
        }, 
        {
            "location": "/ceed2am/#august-8-10-2018", 
            "text": "", 
            "title": "August 8-10, 2018"
        }, 
        {
            "location": "/ceed2am/#university-of-colorado-boulder", 
            "text": "", 
            "title": "University of Colorado Boulder"
        }, 
        {
            "location": "/ceed2am/#overview", 
            "text": "CEED will hold its second annual meeting August 8-10, 2018 in  Koelbel S127  ( adjacent  to the Engineering Center) at the University of Colorado Boulder. The goal of the meeting is to report on the progress in the center, deepen existing and establish new connections with ECP hardware vendors, ECP software technologies projects and other collaborators, plan project activities and brainstorm/work as a group to make technical progress. In addition to gathering together many of the CEED researchers, the meeting will include representatives of the ECP management, hardware vendors, software technology and other interested projects.", 
            "title": "Overview"
        }, 
        {
            "location": "/ceed2am/#tentative-meeting-agenda", 
            "text": "", 
            "title": "Tentative Meeting Agenda"
        }, 
        {
            "location": "/ceed2am/#august-8-wednesday", 
            "text": "Project overview  Collaborative discussion  Meeting dinner", 
            "title": "August 8 (Wednesday)"
        }, 
        {
            "location": "/ceed2am/#august-9-thursday", 
            "text": "Guest speakers  Collaborative discussion  Excursion (hiking in the Flatirons)", 
            "title": "August 9 (Thursday)"
        }, 
        {
            "location": "/ceed2am/#august-10-friday", 
            "text": "Internal planning  Hackathon", 
            "title": "August 10 (Friday)"
        }, 
        {
            "location": "/ceed2am/#lodging-options", 
            "text": "There are many hotels close to the university in Boulder.   Millennium Harvest  - 5 minute walk  Best Western  - 10 minute walk  Marriott  - 10 minute walk  Embassy Suites (Hilton)  - 10 minute walk  Garden Inn (Hilton)  - 10 minute walk", 
            "title": "Lodging Options"
        }, 
        {
            "location": "/ceed2am/#questions", 
            "text": "Please contact the meeting organizers at:  llnl-ceed-meeting@listserv.llnl.gov .", 
            "title": "Questions?"
        }, 
        {
            "location": "/about/", 
            "text": "About CEED\n\n\nThe Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:\n\n\n\n\nArgonne National Laboratory\n\n\nLawrence Livermore National Laboratory\n\n\nRensselaer Polytechnic Institute\n\n\nThe University of Tennessee, Knoxville\n\n\nUniversity of Colorado Boulder\n\n\nUniversity of Illinois Urbana-Champaign\n\n\nVirginia Tech\n\n\n\n\nYou can reach us by emailing \nceed-users@llnl.gov\n or by leaving a comment in\nthe \nCEED user forum\n.\n\n\n\n\n\n\n\nThis research is supported by the \nExascale Computing Project\n (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a \ncapable exascale ecosystem\n, including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s \nexascale computing imperative\n.\n\n\n\n\nOur Team\n\n\n\n\nAhmad Abdelfattah\n\n\nAleks Obabko\n\n\nAli Karakus\n\n\nAndrew Siegel\n\n\nAzzam Haidar\n\n\nBarry Smith\n\n\nCameron Smith\n\n\nDavid Beckingsale\n\n\nDavid Medina\n\n\nIan Karlin\n\n\nJack Dongarra\n \n Lead for the \nHardware\n thrust\n\n\nJed Brown\n \n Lead for the \nSoftware\n thrust\n\n\nKatie Heisey\n\n\nKazem Kamran\n\n\nMark Shepard\n\n\nMatt Otten\n\n\nMisun Min\n \n Lead for the \nApplications\n thrust\n\n\nNoel Chalmers\n\n\nPanayot Vassilevski\n\n\nPaul Fischer\n \n Deputy Director of CEED\n\n\nPedro Bello-Maldonado\n\n\nRobert Rieben\n\n\nRon Rahaman\n\n\nScott Parker\n\n\nSom Dutta\n\n\nStanimire Tomov\n\n\nStefan Kerkemeier\n\n\nThilina Ratnayake\n\n\nTim Moon\n\n\nTim Warburton\n\n\nTzanio Kolev\n \n Director of CEED\n\n\nVeselin Dobrev\n \n Lead for the \nFinite Element\n thrust\n\n\nVladimir Tomov\n\n\n\n\n\n\nWebsite built with \nMkDocs\n, \nBootstrap\n\nand \nBootswatch\n. Hosted on \nGitHub\n.\n\n\nLLNL-WEB-732668.\n\nPrivacy \n Legal Notice\n.", 
            "title": "About"
        }, 
        {
            "location": "/about/#about-ceed", 
            "text": "The Center for Efficient Exascale Discretizations is a research partnership\nbetween two U.S. Department of Energy laboratories and five universities:   Argonne National Laboratory  Lawrence Livermore National Laboratory  Rensselaer Polytechnic Institute  The University of Tennessee, Knoxville  University of Colorado Boulder  University of Illinois Urbana-Champaign  Virginia Tech   You can reach us by emailing  ceed-users@llnl.gov  or by leaving a comment in\nthe  CEED user forum .    This research is supported by the  Exascale Computing Project  (17-SC-20-SC),\na collaborative effort of two U.S. Department of Energy organizations (Office of\nScience and the National Nuclear Security Administration) responsible for the\nplanning and preparation of a  capable exascale ecosystem , including software,\napplications, hardware, advanced system engineering and early testbed platforms,\nin support of the nation\u2019s  exascale computing imperative .", 
            "title": "About CEED"
        }, 
        {
            "location": "/about/#our-team", 
            "text": "Ahmad Abdelfattah  Aleks Obabko  Ali Karakus  Andrew Siegel  Azzam Haidar  Barry Smith  Cameron Smith  David Beckingsale  David Medina  Ian Karlin  Jack Dongarra    Lead for the  Hardware  thrust  Jed Brown    Lead for the  Software  thrust  Katie Heisey  Kazem Kamran  Mark Shepard  Matt Otten  Misun Min    Lead for the  Applications  thrust  Noel Chalmers  Panayot Vassilevski  Paul Fischer    Deputy Director of CEED  Pedro Bello-Maldonado  Robert Rieben  Ron Rahaman  Scott Parker  Som Dutta  Stanimire Tomov  Stefan Kerkemeier  Thilina Ratnayake  Tim Moon  Tim Warburton  Tzanio Kolev    Director of CEED  Veselin Dobrev    Lead for the  Finite Element  thrust  Vladimir Tomov    Website built with  MkDocs ,  Bootstrap \nand  Bootswatch . Hosted on  GitHub .  LLNL-WEB-732668. Privacy   Legal Notice .", 
            "title": "Our Team"
        }
    ]
}